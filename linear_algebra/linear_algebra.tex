% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Further Linear Algebra},
  pdfauthor={Kexing Ying},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=red,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin = 1.5in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{lipsum}
\usepackage[ruled,vlined]{algorithm2e}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\title{Further Linear Algebra}
\author{Kexing Ying}
\date{May 15, 2020}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

As we have learnt from last year, linear algebra is a important subject
regarding matrices, vector spaces, linear maps, and this year we will
also take a look at some geometrical interpretations of these concepts.

\hypertarget{matrices}{%
\subsection{Matrices}\label{matrices}}

\begin{definition} [Similar matrices]
  Let \(A, B \in \mathbb{F}^{n \times n}\) for some field \(\mathbb{F}\). 
  We say \(A\) is similar to \(B\) if and only if there exists some 
  \(P \in \mathbb{F}^{n \times n}\) such that, 
  \[B = P^{-1} A P.\]
\end{definition}

We recall that \emph{similar} is an equivalence relation and similar
matrices shares many useful properties such as

\begin{itemize}
  \item same determinant
  \item same characteristic polynomial
  \item same Eigenvalues
  \item same rank
\end{itemize}

and many more. As similar matrices share so many properties, one major
aim in linear algebra is to find a ``nice'' matrix \(B\) given any
arbitrary square matrix \(A\) such that \(A\) and \(B\) are similar. We
first saw a version of this question last year through the
\emph{diagonalisation} of matrices. However, as we have seen, not all
matrices are diagonalisable, therefore, in this course, we will take a
look at some \emph{weaker} versions that are more general.

A version of our aim is the triangular theorem which states that; given
\(A \in \mathbb{C}^{n \times n}\) (note that this theorem is not true
for arbitrary field), there exists (and not uniquely) some upper
triangular matrix \(B \in \mathbb{C}^{n \times n}\) such that \(A\) is
similar to \(B\).

Another version of this aim is the \emph{Jordan Canonical Form} theorem.
It turns out if \(A \in \mathbb{C}^{n \times n}\), then \(A\) is similar
to a \emph{unique} matrix in the Jordan canonical form. This theorem is
powerful due to the canonical nature of this theorem. One immediate
result of this theorem is that we can check whether two matrices are
similar to each other by checking where or not they have the same
\emph{JCF} (which is computationally easy to do).

However, we see that neither of the above version are theorems over
arbitrary fields. The \emph{Rational Canonical Form} attempts to solve
this.

\begin{definition} [Companion matrix]
  Given an arbitrary field \(\mathbb{F}\), \(p \in \mathbb{F}[X]\) such that 
  \(p\) is monic (i.e. the coefficient of the highest term of \(p\) is 1) and 
  \(\deg p = k\), the companion matrix of \(p\) is the the \(k \times k\) matrix 
  \[C(p) =
    \begin{bmatrix}
      0 & 0 & 0 & \cdots & 0 & -a_0 \\
      1 & 0 & 0 & \cdots & 0 & -a_1 \\
      0 & 1 & 0 & \cdots & 0 & -a_2 \\
       &  & \ddots & \ddots & \vdots & \vdots \\
      0 & 0 & 0 & \cdots & 1 & -a_{k - 1}
    \end{bmatrix}  
  \]
  where \(a_i\) is the coefficient of \(p\) of the term \(X^i\) in 
  \(\mathbb{F}\).
\end{definition}

The companion matrix is a nice matrix and it we can in fact show that
the characteristic polynomial of the companion matrix of some \(p\) is
\(p\).

\begin{theorem}
  Let \(A \in \mathbb{F}^{n \times n}\) with characteristic polynomial \(p\). 
  Then, there exists a polynomial factorisation such that 
  \(p = \prod_{i = 1}^k p_i\) and
  \[ A \sim
    \begin{bmatrix}
      C(p_1) & 0 & \cdots & 0 \\
      0 & C(p_2) & \cdots & 0 \\
      \vdots & \vdots & \ddots & \vdots \\
      0 & 0 & \cdots & C(p_k)
    \end{bmatrix}.
  \]
\end{theorem}

Furthermore, it turns out this factorisation is unique under certain
assumptions which we will take a look at in the course.

\hypertarget{geometry}{%
\subsection{Geometry}\label{geometry}}

Recall the dot product on \(\mathbb{R}^n\) where given
\(u, v \in \mathbb{R}^n\),
\(\langle u, v \rangle = \sum_{i = 1}^n u_i v_i\). Furthermore, recall
we also took a look at \emph{orthogonal} and \emph{symmetric} matrices
last year. All of these, of course, has geometric interpretations and we
will in this part of the course generalise and axiomatise these to the
theory of \emph{inner product spaces} of \(V\) over \(\mathbb{R}\). We
will also extend this theory to arbitrary fields \(\mathbb{F}\) - the
\emph{Theory of Bilinear Forms}.

\newpage

\hypertarget{algebraic-geometric-multiplicities-of-eigenvalues}{%
\section{Algebraic \& Geometric Multiplicities of
Eigenvalues}\label{algebraic-geometric-multiplicities-of-eigenvalues}}

We recall some basic definitions and properties of Eigenvectors.

\begin{definition}
  Let \(V\) be some vector space, \(T : V \to V\) a linear map and \(\lambda\) 
  an Eigenvalue of \(T\). Then the \(\lambda\)-Eigenspace of \(T\) is the 
  subspace of \(V\),
  \[E_\lambda := \{ v \in V \mid (\lambda I_V - T)v = \mathbf{0} \}.\]
  We see that this is a subspace as it is the kernel of the linear map 
  \(\lambda I_V - T\).
\end{definition}

\begin{theorem}
  Let \(V\) be some vector space, \(T : V \to V\) a linear map. Suppose that 
  \(\{v_1, \cdots, v_k\}\) are Eigenvectors corresponding to distinct 
  Eigenvalues \(\lambda_1, \cdots, \lambda_k\), then it is linearly independent.
\end{theorem}
\proof

We will prove by contrapositive. Suppose that \(\{v_1, \cdots, v_k\}\)
are Eigenvectors that are linearly independent. Then by definition,
there exists a minimal set of \(\{\mu_i \mid i \in I\}\), such that
\(\sum_{i \in I} \mu_i v_i = 0\) (we see that \(\mu_i \neq 0\) for all
\(i\) as otherwise it is not minimal). Now, let \(j \in I\), then by
rewriting, we have \(v_j = \sum_{i \neq j}\mu_i' v_i\). Thus, \[
    \lambda_j \sum_{i \neq j} \mu_i' v_i = \lambda_j v_j = T(v_j)
      = T\left(\sum_{i \neq j} \mu_i' v_i\right) 
      = \sum_{i \neq j} \mu_i' T(v_i) 
      = \sum_{i \neq j} \mu_i' \lambda_i v_i.
  \] So, by rearranging,
\(0 = \sum_{i \neq j}(\lambda_i - \lambda_j)\mu_i' v_i\). Now, if for
all \(i \neq j\), \(\lambda_i \neq \lambda_j\), we have found a smaller
subset of \(\{v_1, \cdots, v_k\}\) that is linearly dependent,
contradicting our assumption, so there must be some \(i\) such that
\(\lambda_i = \lambda_j\). \qed

\begin{corollary}
  Let \(V\) be a \(n\)-dimensional vector space. Then if the characteristic 
  polynomial of the linear map \(T : V \to V\) has \(n\) distinct roots, then 
  \(T\) is diagonalisable. 
\end{corollary}

\end{document}
