% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Further Linear Algebra},
  pdfauthor={Kexing Ying},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=red,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin = 1.5in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{lipsum}
\usepackage[ruled,vlined]{algorithm2e}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\lcm}{\mathop{\mathrm{lcm}}}
\newcommand{\End}{\mathop{\mathrm{End}}}

\title{Further Linear Algebra}
\author{Kexing Ying}
\date{May 15, 2020}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

As we have learnt from last year, linear algebra is a important subject
regarding matrices, vector spaces, linear maps, and this year we will
also take a look at some geometrical interpretations of these concepts.

\hypertarget{matrices}{%
\subsection{Matrices}\label{matrices}}

\begin{definition} [Similar matrices]
  Let \(A, B \in \mathbb{F}^{n \times n}\) for some field \(\mathbb{F}\). 
  We say \(A\) is similar to \(B\) if and only if there exists some 
  \(P \in \mathbb{F}^{n \times n}\) such that, 
  \[B = P^{-1} A P.\]
\end{definition}

We recall that \emph{similar} is an equivalence relation and similar
matrices shares many useful properties such as

\begin{itemize}
  \item same determinant
  \item same characteristic polynomial
  \item same Eigenvalues
  \item same rank
\end{itemize}

and many more. As similar matrices share so many properties, one major
aim in linear algebra is to find a ``nice'' matrix \(B\) given any
arbitrary square matrix \(A\) such that \(A\) and \(B\) are similar. We
first saw a version of this question last year through the
\emph{diagonalisation} of matrices. However, as we have seen, not all
matrices are diagonalisable, therefore, in this course, we will take a
look at some \emph{weaker} versions that are more general.

A version of our aim is the triangular theorem which states that; given
\(A \in \mathbb{C}^{n \times n}\) (note that this theorem is not true
for arbitrary field), there exists (and not uniquely) some upper
triangular matrix \(B \in \mathbb{C}^{n \times n}\) such that \(A\) is
similar to \(B\).

Another version of this aim is the \emph{Jordan Canonical Form} theorem.
It turns out if \(A \in \mathbb{C}^{n \times n}\), then \(A\) is similar
to a \emph{unique} matrix in the Jordan canonical form. This theorem is
powerful due to the canonical nature of this theorem. One immediate
result of this theorem is that we can check whether two matrices are
similar to each other by checking where or not they have the same
\emph{JCF} (which is computationally easy to do).

However, we see that neither of the above version are theorems over
arbitrary fields. The \emph{Rational Canonical Form} attempts to solve
this.

\begin{definition} [Companion matrix]
  Given an arbitrary field \(\mathbb{F}\), \(p \in \mathbb{F}[X]\) such that 
  \(p\) is monic (i.e. the coefficient of the highest term of \(p\) is 1) and 
  \(\deg p = k\), the companion matrix of \(p\) is the the \(k \times k\) matrix 
  \[C(p) =
    \begin{bmatrix}
      0 & 0 & 0 & \cdots & 0 & -a_0 \\
      1 & 0 & 0 & \cdots & 0 & -a_1 \\
      0 & 1 & 0 & \cdots & 0 & -a_2 \\
       &  & \ddots & \ddots & \vdots & \vdots \\
      0 & 0 & 0 & \cdots & 1 & -a_{k - 1}
    \end{bmatrix}  
  \]
  where \(a_i\) is the coefficient of \(p\) of the term \(X^i\) in 
  \(\mathbb{F}\).
\end{definition}

The companion matrix is a nice matrix and it we can in fact show that
the characteristic polynomial of the companion matrix of some \(p\) is
\(p\).

\begin{theorem}
  Let \(A \in \mathbb{F}^{n \times n}\) with characteristic polynomial \(p\). 
  Then, there exists a polynomial factorisation such that 
  \(p = \prod_{i = 1}^k p_i\) and
  \[ A \sim
    \begin{bmatrix}
      C(p_1) & 0 & \cdots & 0 \\
      0 & C(p_2) & \cdots & 0 \\
      \vdots & \vdots & \ddots & \vdots \\
      0 & 0 & \cdots & C(p_k)
    \end{bmatrix}.
  \]
\end{theorem}

Furthermore, it turns out this factorisation is unique under certain
assumptions which we will take a look at in the course.

\hypertarget{geometry}{%
\subsection{Geometry}\label{geometry}}

Recall the dot product on \(\mathbb{R}^n\) where given
\(u, v \in \mathbb{R}^n\),
\(\langle u, v \rangle = \sum_{i = 1}^n u_i v_i\). Furthermore, recall
we also took a look at \emph{orthogonal} and \emph{symmetric} matrices
last year. All of these, of course, has geometric interpretations and we
will in this part of the course generalise and axiomatise these to the
theory of \emph{inner product spaces} of \(V\) over \(\mathbb{R}\). We
will also extend this theory to arbitrary fields \(\mathbb{F}\) - the
\emph{Theory of Bilinear Forms}.

\newpage

\hypertarget{more-on-vector-spaces}{%
\section{More on Vector Spaces}\label{more-on-vector-spaces}}

From this point forward, we write \(\sum \mu S\) as a shorthand for
\(\sum_{s \in S} \mu_s s\) for some suitable set \(S\) and indexed value
\(\mu\). We will also write \(T \in \mathop{\mathrm{End}}(V)\) for \(T\)
is an endomorphism of \(V\), that is, a linear map \(T : V \to V\).

\hypertarget{algebraic-geometric-multiplicities-of-eigenvalues}{%
\subsection{Algebraic \& Geometric Multiplicities of
Eigenvalues}\label{algebraic-geometric-multiplicities-of-eigenvalues}}

We recall some basic definitions and properties of Eigenvectors.

\begin{definition}
  Let \(V\) be some vector space, \(T \in \mathop{\mathrm{End}}(V)\) a linear map and \(\lambda\) 
  an Eigenvalue of \(T\). Then the \(\lambda\)-Eigenspace of \(T\) is the 
  subspace of \(V\),
  \[E_\lambda := \{ v \in V \mid (\lambda I_V - T)v = \mathbf{0} \}.\]
  We see that this is a subspace as it is the kernel of the linear map 
  \(\lambda I_V - T\).
\end{definition}

\begin{theorem}\label{eigen_li}
  Let \(V\) be some vector space, \(T \in \mathop{\mathrm{End}}(V)\) a linear map. Suppose that 
  \(\{v_1, \cdots, v_k\}\) are Eigenvectors corresponding to distinct 
  Eigenvalues \(\lambda_1, \cdots, \lambda_k\), then it is linearly independent.
\end{theorem}
\proof

We will prove by contrapositive. Suppose that \(\{v_1, \cdots, v_k\}\)
are Eigenvectors that are linearly independent. Then by definition,
there exists a minimal set of \(\{\mu_i \mid i \in I\}\), such that
\(\sum_{i \in I} \mu_i v_i = 0\) (we see that \(\mu_i \neq 0\) for all
\(i\) as otherwise it is not minimal). Now, let \(j \in I\), then by
rewriting, we have \(v_j = \sum_{i \neq j}\mu_i' v_i\). Thus, \[
    \lambda_j \sum_{i \neq j} \mu_i' v_i = \lambda_j v_j = T(v_j)
      = T\left(\sum_{i \neq j} \mu_i' v_i\right) 
      = \sum_{i \neq j} \mu_i' T(v_i) 
      = \sum_{i \neq j} \mu_i' \lambda_i v_i.
  \] So, by rearranging,
\(0 = \sum_{i \neq j}(\lambda_i - \lambda_j)\mu_i' v_i\). Now, if for
all \(i \neq j\), \(\lambda_i \neq \lambda_j\), we have found a smaller
subset of \(\{v_1, \cdots, v_k\}\) that is linearly dependent,
contradicting our assumption, so there must be some \(i\) such that
\(\lambda_i = \lambda_j\). \qed

\begin{corollary}
  Let \(V\) be a \(n\)-dimensional vector space. Then if the characteristic 
  polynomial of the linear map \(T \in \mathop{\mathrm{End}}(V)\) has \(n\) distinct roots, then 
  \(T\) is diagonalisable. 
\end{corollary}

We define \emph{algebraic} and \emph{geometric} multiplicity for
Eigenvalues.

\begin{definition}[Algebraic and Geometric Multiplicity]
  Let \(T \in \mathop{\mathrm{End}}(V)\) be a linear map with characteristic polynomial \(\chi_T\),
  such that \(\chi_T(\lambda) = 0\) (i.e. \(\lambda\) is an Eigenvalue of \(T\)).

  The algebraic multiplicity of \(\lambda\) is the number \(a(\lambda)\) such 
  that
  \[\chi_T(x) = (x - \lambda)^{a(\lambda)} q(x),\]
  for some polynomial \(q(x)\) where \(q(\lambda) \neq 0\).

  The geometric multiplicity of \(\lambda\) is
  \[g(\lambda) = \dim E_{\lambda}.\]
\end{definition}

\begin{prop}
  Let \(T \in \mathop{\mathrm{End}}(V)\) be a linear map with an Eigenvalue \(\lambda\), then 
  \(g(\lambda) \le a(\lambda)\).
\end{prop}
\proof

Let \(r = g(\lambda) = \dim E_{\lambda}\), then there exits linearly
independent vectors \(v_1, \cdots, v_r\) which forms a basis of
\(E_{\lambda}\). Suppose we extend this to a basis of \(V\),
\[B = \{v_1, \cdots, v_r, w_1, \cdots, w_s\},\] then by working out
\(T(b)\) for all \(b \in B\), we find \(T(v_i) = \lambda v_i\), and
\(T(w_i) = \sum \mu_i B\) so, \[ [T]_B =
  \left[
    \begin{array}{@{}c|c@{}}
      \begin{matrix}
        \lambda & 0 & \cdots & 0 \\
        0 & \lambda & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & \lambda \\
      \end{matrix}
          & 
      \begin{matrix}
        \mu_1(v_1) & \mu_2(v_1) & \cdots & \mu_s(v_3) \\
        \mu_1(v_2) & \mu_2(v_2) & \cdots & \mu_s(v_2) \\
        \vdots & \vdots & \ddots & \vdots\\
        \mu_1(v_r) & \mu_2(v_r) & \cdots & \mu_s(v_r)\\
      \end{matrix} \\
      \cmidrule[0.4pt]{1-2}
      \textbf{\text{\huge0}} & \hspace{2mm}
      \begin{matrix}
        \mu_1(w_1) & \mu_2(w_1) & \cdots & \mu_s(w_3)\\
        \mu_1(w_2) & \mu_2(w_2) & \cdots & \mu_s(w_2)\\
        \vdots & \vdots & \ddots & \vdots &\\
        \mu_1(w_s) & \mu_2(w_s) & \cdots & \mu_s(w_s)\\
      \end{matrix} \\
    \end{array} 
  \right]. \] We will refer to the four quadrants as
\([\lambda], A, \mathbf{0}\) and \(C\) respectively.

Thus, by considering the characteristic polynomial of this, we have
\[\chi_{[T]_B} = \det(xI - [T]_B) =  (x - \lambda)^r \det(xI - C),\]
implying the algebraic multiplicity of \(\lambda\) is at least \(r\).
\qed

\begin{theorem}
  Let \(\dim V = n\) and \(T \in \mathop{\mathrm{End}}(V)\) be a linear map with distinct 
  Eigenvalues \(\lambda_1, \cdots, \lambda_r\). Suppose that the characteristic 
  polynomial of \(T\) is 
  \[\chi_T = \prod_i (x - \lambda_i)^{a(\lambda_i)},\]
  (so, \(\sum_i a(\lambda_i) = n\)). Then the following are equivalent,
  \begin{enumerate}
    \item \(T\) is diagonalisable;
    \item \(\sum_i g(\lambda_i) = n\);
    \item for all \(i\), \(g(\lambda_i) = a(\lambda_i)\).
  \end{enumerate}
\end{theorem}
\proof

\(2 \iff 3\) is trivial so let us consider the other cases.

\(1 \implies 2\). Suppose \(T\) is diagonalisable, then there exists
some \(B\), a basis of \(V\) consisting of Eigenvectors of \(T\). Then,
we can partition \(B\) into
\(F_{\lambda_i} := \{v \in B \mid T(v) = \lambda_i v\}\) for all
Eigenvalues of \(T\). By noting that the subspace induced by
\(F_{\lambda_i}\) is a subspace of the \(\lambda_i\) Eigenspace
\(E_{\lambda_i}\), we have,
\[\sum_i g(\lambda_i) = \sum_i \dim E_{\lambda_i} \ge \sum_i \dim F_{\lambda_i} = n.\]
Now, as \(\sum_i g(\lambda_i) \le \sum_i a(\lambda_i) = n\) by the
previous proposition, it follows \(\sum_i g(\lambda_i) = n\).

\(2 \implies 1\). Suppose \(\sum_i g(\lambda_i) = n\). Let \(B_i\) be
the basis of \(E_{\lambda_i}\) for all \(\lambda_i\) an Eigenvalue and
let \(B = \bigcup B_i\). We can straight away see that
\(\left| B \right| = n\) so it suffices to show that \(B\) is linearly
independent. Suppose otherwise, then there exists an index set
\(I \subseteq \{1, \cdots, r\}\), \[\sum_{i \in I} \sum \mu_i B_i = 0\]
where \(\sum \mu_i B_i \neq 0\) for all \(i\). Now as
\(\sum \mu_i B_i \in E_{\lambda_i}\), this is a sum of Eigenvectors with
distinct Eigenvalues. However, by theorem \ref{eigen_li}, these
Eigenvectors are there fore linearly independent, so they must be zero.
\# \qed

\hypertarget{direct-sums}{%
\subsection{Direct Sums}\label{direct-sums}}

Recall that we can add subspaces of a vector space together forming
another subspace, that is, given \(U_1, U_2 \le V\),
\(U_1 + U_2 = \{u_1 + u_2 \mid u_1 \in U_1, u_2 \in U_2\} \le V\).
Direct sums is a particular case of this and is closely linked to
\emph{block-diagonal} matrices.

\begin{definition}[Direct Sum of Vector Space]
  Let \(V\) be a vector space with subspaces \(U_1, \cdots, U_k\). We write 
  \[V = \bigoplus_{i = 1}^k U_i\]
  for the direct sum of subspaces if every \(v \in V\), there exists unique 
  \(u_i \in U_i\) for all \(i\) such that \(v = \sum u_i\).
\end{definition}

\begin{prop}
  Let \(V\) be a vector space with subspaces \(V_1, V_2\), then 
  \(V = V_1 \oplus V_2\) if and only if \(V_1 \cap V_2 = \{0_v\}\) and 
  \(\dim V_1 + \dim V_2 = \dim V\).
\end{prop}
\proof

Follow your nose. \qed

\begin{prop}
  Let \(V\) be a vector space with subspaces \(V_1, \cdots, V_k\), then 
  \(V = \bigoplus_{i = 1}^k V_i\) if and only if 
  \(\sum_{i = 1}^k \dim V_i = \dim V\) and if \(B_i\) is a basis of \(V_i\) then 
  \(\bigcup_{i = 1}^k B_i\) is a basis of \(V\).
\end{prop}
\proof

\((\implies)\). Suppose \(V = \bigoplus_{i = 1}^k V_i\), then for all
\(i, j\), \(V_i \cap V_j = \{0_V\}\), thus
\(B_i \cap B_j = \varnothing\) for all \(i \neq j\), implying
\(\sum_{i = 1}^k \dim V_i = \dim V\) and
\(\left| \bigcup B_i \right| = \dim V\) so it suffices to show that
\(\bigcup B_i\) is linearly independent. However, this is trivial as if
\(\bigcup B_i\) is linearly dependent, then there are two distinct ways
of writing 0 as a sum of vectors in \(V_i\). \#

\((\impliedby)\). Suppose \(\sum_{i = 1}^k \dim V_i = \dim V\) and
\(\bigcup_{i = 1}^k B_i\) is a basis of \(V\), then it follows,
\[V = V_1 + V_2 + \cdots V_k.\] Suppose for contradiction there is two
representations of \(v \in V\) where \(v = \sum v_i = \sum v_i'\). Then,
\(v = \sum_i \sum \mu_i B_i =  \sum_i \sum \mu_i' B_i\), and thus,
\(0 = \sum_i \sum \mu_i B_i -  \sum_i \sum \mu_i' B_i = \sum_i \sum (\mu_i - \mu_i') B_i\).
By rewriting, \(0 = \sum (\mu_i - \mu_i') \bigcup B_i\), implying
\(\bigcup B_i\) is linearly dependent. \# \qed

\begin{definition}[Invariant Subspace]
  Let \(V\) be a vector space with subspace \(W\) and let \(T \in \mathop{\mathrm{End}}(V)\) be 
  a linear map. We say \(W\) is \(T\)-invariant if and only if 
  \[T(W) \subseteq W.\]
  We write \(T_W : W \to W\) as the restriction of \(T\) to \(W\).
\end{definition}

A example of an invariant subspace is the Eigenspace of a linear map
since
\(T(E_\lambda) = \{ T(v) \mid v \in E_\lambda \} = \{\lambda v \} \subseteq E_\lambda\).

\begin{theorem}
  Let \(T \in \mathop{\mathrm{End}}(V)\) be a linear map and suppose \(V = \bigoplus V_i\) where 
  for all \(i\), \(V_i\) is \(T\)-invariant. Let \(B_i\) be a basis of \(V_i\),
  and \(A_i = [T_{V_i}]_{B_i}\), then 
  \[[T]_{\bigcup B_i} = \mathop{\mathrm{diag}}(A_1, A_2, \cdots, A_k).\] 
\end{theorem}
\proof

Follows directly from the \(T\)-invariant property of \(V_i\). \qed

From the proposition above, we see the close link between direct sums
and block diagonal matrices. To further highlight the fact, from this
point forward, we write
\(\bigoplus_{i = 1}^k A_i = \mathop{\mathrm{diag}}(A_!, A_2, \cdots, A_k)\)
where \(A_i\) are block matrices.

\begin{corollary}
  Let \(A = \bigoplus_{i = 1}^r A_i\) and let \(\pi \in S_r\). Then 
  \(A \sim A' := \bigoplus_{i = 1}^r A_{\pi(i)}\).
\end{corollary}
\proof

Let the vector space \(V\) in the above theorem be the span of the
columns of \(A\) and \(V_i\) the span of columns of \(A_i\) with the
missing entries filled with zero. Then it is not hard to see
\(V = \bigoplus V_i\). Now, by letting
\(T \in \mathop{\mathrm{End}}(V) : v \mapsto A v\), we see that for all
\(i\), \(V_i\) is \(T\)-invariant and \(T_{V_i} = v \mapsto A_i v\), so,
by taking the basis \(B\) to be the standard basis, we have
\(A = [T]_B\). Now, by permuting the standard basis by \(\pi\),
resulting in the basis \(B'\), we have \(A' = [T]_{B'}\), and so, by
letting \(P\) be the change of basis matrix from \(B \to B'\), we have
shown \(A \sim A'\). \qed

\hypertarget{quotient-spaces}{%
\subsection{Quotient Spaces}\label{quotient-spaces}}

Just like other algebraic graphs we can construct a quotient structure
on vector spaces.

Let \(V\) be a vector space and \(W \le V\), then let
\(\sim_W : V \to V \to \text{Prop}\) be the binary relation such that
\[v_1 \sim_W v_2 \iff v_1 + W = v_2 + W,\] where
\(v + W = \{v + w \mid w \in W\}\) for all \(v \in V\).

By manually checking, we find this is an equivalence relation and the
set \(V / \sim_W\) equipped with the natural addition and scalar
multiplication form a vector space. We will write \(V / W\) for this
quotient space.

\begin{definition}
  Given a quotient space \(V / W\), there exists a linear map
  \[q_W : V \to V / W : v \mapsto v + W.\]
\end{definition}

\begin{prop}\label{dim_sub}
  Let \(V\) be a finite dimensional vector space with the subspace \(W\), 
  then \(\dim V / W = \dim V - \dim W\).
\end{prop}
\proof

Let \(B_W\) be a basis of \(W\) and \(B_V\) the extension basis of \(V\)
from \(B_W\). Then we easily see that
\(V / W \subseteq \text{sp}(q_W(B_V \setminus B_W))\) as for all
\(v \in V\), \(v = \sum \mu B_V\), so
\(v + W = q_W(v) = q_W(\sum \mu B_V) =  \sum \mu q_W(B_W) + \sum \mu q_W(B_V \setminus B_W) = \sum 0_{V / W} +  \sum \mu q_W(B_V \setminus B_W) \in \text{sp}(q_W(B_V \setminus B_W))\).

Now suppose \(q_W(B_V \setminus B_W)\) is not linearly independent in
\(V / W\), then, there exists \(\mu\),
\(0 = \sum \mu q_W(B_V \setminus B_W) =  q_W (\sum \mu (B_V \setminus B_W))\),
so \(\sum \mu (B_V \setminus B_W) \in  \ker q_W = W\). If
\(\sum \mu (B_V \setminus B_W) = 0_V\), then \(B_V \setminus B_W\) is
not linearly dependent, a contradiction so,
\(\sum \mu (B_V \setminus B_W) \neq 0_V\). Now, as
\(\sum \mu (B_V \setminus B_W) \in W\), there is some \(\lambda\),
\(\sum \mu (B_V \setminus B_W) = \sum \lambda B_W\), so
\(\sum \mu (B_V \setminus B_W) - \sum \lambda B_W = 0\) implying \(B_V\)
is not linearly independent. \# \qed

With the above proposition, we have found a method to find a basis of a
quotient space \(V / W\) by extending the basis of \(W\).

Let us now consider quotient spaces' relation with linear maps.

\begin{definition}[Quotient Map]
  Let \(V\) be a vector space and \(W\) a subspace of \(V\). Suppose 
  \(T \in \mathop{\mathrm{End}}(V)\) is a linear map and \(W\) is \(T\)-invariant. Then there 
  is an induced quotient map 
  \[\bar{T} : V / W \to V / W : q_W(v) \mapsto q_W(T(v)).\]
\end{definition}

To see that this is well defined, let \(u, v \in V\),
\(q_W(u) = q_W(v)\), then \(u - v \in W\) implying \(T(u - v) \in W\) as
\(W\) is \(T\)-invariant. Thus,
\(0_{V / W} = q_W(T(u - v)) = q_W(T(u) - T(v)) = q_W(T(u)) - q_W(T(v))\)
implying \(\bar{T}(u) = \bar{T}(v)\).

\begin{theorem}\label{blocks_quotient}
  Let \(V\) be a vector space \(W\) a subspace that is \(T\)-invariant for some 
  \(T \in \mathop{\mathrm{End}}(V)\) a linear map. Let \(B_W\) be a basis of \(W\), \(B\) the 
  extended basis of \(V\) from \(B_W\), and \(\bar{B}\) the basis of \(V / W\) 
  as constructed by proposition \ref{dim_sub}. Then 
  \[[T]_B = 
    \left[
    \begin{array}{c|c}
      [T_W]_{B_W} & A \\
      \midrule
      \mathbf{0} & [\bar{T}]_{\bar{B}}
    \end{array}
    \right],
  \]
  where \(A\) is some matrix.
\end{theorem}
\proof

Consider where \(T(v)\) lands whenever \(u \in B_w \subseteq W\), and
where \(\bar{T}(v)\) lands for the rest of the basis vectors. \qed

\begin{corollary}\label{prod_char}
  Let \(T \in \mathop{\mathrm{End}}(V)\) be a linear map and \(W \le V\) is \(T\)-invariant, 
  then \(\chi_T = \chi_{T_W} \chi_{\bar{T}}\) where \(\chi_f\) denotes the 
  characteristic polynomial of the linear map \(f\). 
\end{corollary}

\hypertarget{triangularisation-theorem}{%
\subsection{Triangularisation Theorem}\label{triangularisation-theorem}}

We have now arrived at the first major theorem of this course, that
under certain conditions we can always triangularise matrices. We will
in general work with upper triangular matrices when referring to
triangular matrices.

\begin{prop}
  Let \(A = [a_{i, j}], B = [b_{i, j}] \in M_n(\mathbb{F})\) be triangular, then 
  \begin{itemize}
    \item \(\chi_A(x) = \prod_{i = 1}^n (x - a_{i, i})\);
    \item \(\det A = \prod_{i = 1}^n a_{i, i}\);
    \item \(AB\) is also triangular with diagonal \(a_{i, i}b_{i, i}\).
  \end{itemize}
\end{prop}

The Triangularisation theorem states:

\begin{theorem}
  Let \(V\) be a finite dimensional vector space over some field \(\mathbb{F}\), 
  and let \(T \in \mathop{\mathrm{End}}(V)\) be a linear map. Suppose the the characteristic 
  polynomial of \(T\), \(\chi_T\) factorises into a product of linear factors, 
  i.e. there exists \(\lambda_i \in \mathbb{F}\), 
  \[\chi_T(x) = \prod (x - \lambda_i),\]
  then, there exists a basis \(B\) of \(V\) such that \([T]_B\) is upper 
  triangular.
\end{theorem}

Straight away, we see a version of this in terms of matrices instead of
linear maps in which the matrix is \emph{similar} to a triangular
matrix. We also note that, for some fields, such as the complex numbers
\(\mathbb{C}\), we can always triangularise any matrix (by \emph{FTA}).
This might not be the case for other fields such as the real numbers.

\proof

We induct on the dimension of \(V\). The theorem is trivial when
\(\dim V = 1\), so let us consider the case when \(\dim V = k + 1\)
under the inductive hypothesis.

As \(\chi_T\) factorises, \(T\) has an Eigenvalue \(\lambda\) and some
Eigenvector \(v \in V\). Let \(W = \text{sp}(v)\) be a \(T\)-invariant
subspace of \(V\). Then, by proposition \ref{dim_sub}, \(V / W\) has
dimension \(k\) and we have the induced quotient map
\(\bar{T} : V / W \to V / W\). Now, by corollary \ref{prod_char},
\(\prod (x - \lambda_i) = \chi_T(x) =  \chi_{\bar{T}}(x) \chi_{T_W}(x) = \chi_{T_W}(x)(x - \lambda)\).
So, \(\chi_{T_W}(x)\) is a polynomial of degree \(k\) which factorises.
Then by our inductive hypothesis, there exists a basis \(\bar{B}\) such
that \([\bar{T}]_{\bar{B}}\) is triangular. Then by theorem
\ref{blocks_quotient}, we have found a basis \(B\), \([T]_B\) is
triangular. \qed

\begin{corollary}
  Let \(A \in M_n(\mathbb{C})\) with Eigenvalues \(\lambda_i\). Then 
  \(\sum g(\lambda_i)\lambda_i = \mathop{\mathrm{tr}}(A)\).
\end{corollary}
\proof

By the triangularisation theorem, \(A = PQP^{-1}\) where \(Q\) is
triangular. As \(A\) and \(Q\) have the same Eigenvalues, it suffices to
show that \(\mathop{\mathrm{tr}}(A) = \mathop{\mathrm{tr}}(Q)\). But
this follows as
\(\mathop{\mathrm{tr}}(A) = \mathop{\mathrm{tr}}(PQP^{-1}) =  \mathop{\mathrm{tr}}(P^{-1}PQ) = \mathop{\mathrm{tr}}(Q)\).
\qed

\hypertarget{polynomials}{%
\section{Polynomials}\label{polynomials}}

\hypertarget{cayley-hamilton-theorem}{%
\subsection{Cayley-Hamilton Theorem}\label{cayley-hamilton-theorem}}

Recall that given a polynomial \(p(x) = \sum_{i = 0}^n a_i x^i\), we
write \(p(T)\) as the linear map \(\sum_{i = 0}^n a_i T^i\) for the
linear map \(T \in \mathop{\mathrm{End}}(V)\) and similarly for
matrices. Then, the Cayley-Hamilton theorem states the famous result
that, given a linear map \(T \in \mathop{\mathrm{End}}(V)\), if
\(\chi_T\) is the characteristic polynomial of \(T\), then
\(\chi_T(T) = 0\). We will prove this theorem within this chapter.

Straight away, we see that the result is trivial if the matrix in
question is diagonal (or thus, similar to a diagonal matrix) since if
\(A = \mathop{\mathrm{diag}}(\lambda_i)\),
\(p(A) = \mathop{\mathrm{diag}}(p(\lambda_i))\) for any polynomial
\(p\). In fact, by similar argument, we find the theorem is also true
for triangular matrices, and thus, by the triangularisation theorem, the
Cayley-Hamilton theorem is true for vector spaces over the complex
numbers (see problem sheet 3). However, this is less trivial for general
matrices over arbitrary fields which we shall provide a proof here.

\begin{lemma}\label{no_proper}
  Let \(T \in \mathop{\mathrm{End}}(V)\) be a linear map such that there does not exists a proper 
  non-trivial \(T\)-invariant subspace of \(V\). Suppose \(\dim V = n\), then 
  the set \(B := \{v, T(v), T^2(v), \cdots, T^{n - 1}(v)\}\) forms a basis of 
  \(V\) for any non-zero \(v \in V\). 
\end{lemma}
\proof

As \(\left| B \right| = n\), it suffices to show that it is linearly
independent. Suppose otherwise, then there exists some \(\mu\), such
that \(\sum \mu B = 0\). Then, we can choose \(i\) such that \(i\) is
the largest number in which \(T^i(v) = \sum_{i \neq j} \mu_j T^j(v)\).
But then, for all \(u \in \text{sp}(B), u = \sum \lambda T^i(v)\), so
\(T(u) =  T(\sum \lambda T^i(v)) = \sum \lambda T^{i + 1}(v)\). Now, as
\(T^{n + 1}(v)  = T^{n + 1 - k}(T^k(v)) = T^{n + 1 - k}(\sum_{i \neq j} \mu_j T^j(v))  = \sum_{i \neq j} \mu_j T^{n + 1 - k + j}(v) \in \text{sp}(B)\)
as \(n + 1 - k + j \le n\) for all \(j\) as \(k\) is the largest. Thus,
\(\text{sp}(B)\) is a proper and non-trivial \(T\)-invariant subspace.
\# \qed

\proof (Cayley-Hamilton Theorem). Let \(T \in \mathop{\mathrm{End}}(V)\)
and \(\chi_T\) be the characteristic polynomial of \(T\). We will induct
on the dimension of \(V\), \(n\).

The \(n = 1\) case is trivial, so let us suppose the inductive
hypothesis for dimensions \(\le k\) and we will prove this theorem for
\(n = k\).

Suppose first that there exists a proper and non-trivial \(T\)-invariant
subspace \(W\) of \(V\) and suppose it has basis \(B_W\). We can then
extend this basis to a basis \(B\) of \(V\) such that \[[T]_B = 
    \left[
      \begin{array}{c|c}
        [T_W]_{B_W} & A \\
        \midrule
        \mathbf{0} & [\bar{T}]_{\bar{B}}
      \end{array}
      \right]. \] Now, we recall that
\(\chi_T = \chi_{T_W} \chi_{\bar{T}}\), so, \begin{align*}
    \chi_T([T]_B) & = \chi_{T_W}([T]_B) \chi_{\bar{T}}([T]_B)\\
    & = 
    \left[
      \begin{array}{c|c}
        \chi_{T_W}([T_W]_{B_W}) & A \\
        \midrule
        \mathbf{0} & \chi_{T_W}([\bar{T}]_{\bar{B}})
      \end{array}
    \right]
    \left[
      \begin{array}{c|c}
        \chi_{\bar{T}}([T_W]_{B_W}) & A \\
        \midrule
        \mathbf{0} & \chi_{\bar{T}}([\bar{T}]_{\bar{B}})
      \end{array}
    \right]\\
    & = 
    \left[
      \begin{array}{c|c}
        \mathbf{0} & A \\
        \midrule
        \mathbf{0} & \chi_{T_W}([\bar{T}]_{\bar{B}})
      \end{array}
    \right]
    \left[
      \begin{array}{c|c}
        \chi_{\bar{T}}([T_W]_{B_W}) & A \\
        \midrule
        \mathbf{0} & \mathbf{0}
      \end{array}
    \right]\\
    & = \mathbf{0},
  \end{align*} where we write \(A\) for arbitrary block matrix and the
second to last equality is due to the inductive hypothesis.

Suppose now that there does not exists a non-trivial proper
\(T\)-invariant subspace of \(V\). Then by lemma \ref{no_proper}, the
set \(B := \{v, T(v), \cdots, T^n(v)\}\) forms a basis of \(V\). Now, we
see that \([T]_B\) is a companion matrix resulting in
\(\chi_{[T]_B}(x) = \sum a_i x^i\), where \(a_i\) are chosen such that
\(T^{n + 1} = \sum -a_i T^i(v)\). But
\(T^{n + 1} = \sum -a_i T^i(v) \iff \sum_{i = 0}^{n + 1} -a_i T^i(v) = 0\)
where we let \(a_{n + 1} = 1\), so we have
\(\chi_{[T]_B}([T]_B(v)) =  \sum_{i = 0}^{n + 1} -a_i T^i(v) = 0\).

Thus, by the law of excluded middle, we have Cayley-Hamilton. \qed

\hypertarget{some-theories-on-polynomials}{%
\subsection{Some Theories on
Polynomials}\label{some-theories-on-polynomials}}

Let \(\mathbb{F}\) be a field, then we denote the ring formed by the
polynomials over \(\mathbb{F}\) as \(\mathbb{F}[X]\). And we will
develop the theories of greatest common divisor, least common multiple,
and polynomial prime factorisation for this ring.

\begin{theorem}[Euclidean Algorithm]
  Let \(f, g \in \mathbb{F}[X]\) such that \(\deg g \ge 1\). Then there 
  exists \(q, r \in \mathbb{F}[X]\), \(f = q g + r\) where \(r = 0\) or 
  \(\deg r < \deg g\).
\end{theorem}
\proof

We induct on the degree of \(f\), \(n\). For \(n = 0\), we can choose
\(q = 0\) and \(r = f\) and we are done. Let's now assume \(n = k + 1\)
alongside the inductive hypothesis.

Let's write \(f(x) = a_{k + 1}x^{k + 1} + \cdots\) and
\(g(x) = a_m x^m + \cdots\). Then, we can write
\(f_1 = f - a_{k + 1} b_m^{-1} x^{n - m} g\), where
\(\deg f_1 \le \deg f\). So by the inductive hypothesis, there exists
some \(q, r \in \mathbb{F}[X]\) such that \(f_1 = q g + r\) and
\(\deg r \le \deg g\). Then,
\(f = f_1 + a_{k + 1} b_m^{-1} x^{n - m} g  = (q + a_{k + 1} b_m^{-1} x^{n - m}) g + r\).
\qed

\begin{definition}[Greatest Common Divisor]
  Let \(f, g \in \mathbb{F}[X] \setminus \{0\}\). Then we say \(d \in \mathbb{F}[X]\) 
  is the greatest common divisor of \(f\) and \(g\), \(\gcd(f, g)\) if and only 
  if \(d \mid f, d \mid g\) and for all \(e \mid f\) and \(e \mid g\), \(e \mid d\). 
\end{definition}

Straight away, we see that, unlike the integers, the greatest common
divisor is of two polynomials is not unique as we can simply multiply
the gcd by any scalar and receive another gcd. However, if we quotient
out by this relation
(\(\sim : \mathbb{F}[X] \to \mathbb{F}[X] \to \text{Prop} : f, g \mapsto \exists \lambda \in \mathbb{F} \setminus \{0\}, f = \lambda g\)),
the gcd turns out to be unique (see problem sheet 3) and exists.

\begin{theorem}
  If \(f, g \in \mathbb{F}[X] \setminus \{0\}\), then the \(\gcd(f, g)\) exists.
\end{theorem}
\proof

Same argument as the integers by repeatedly applying the Euclidean
algorithm. \qed

\begin{definition}[Coprime]
  We call two polynomials \(f, g \in \mathbb{F}[X]\) to be coprime if and only if
  \(\gcd(f, g) = 1\).
\end{definition}

\begin{theorem}[Bezout's]
  If \(f, g, d \in \mathbb{F}[X]\) such that \(d = \gcd(f, g)\), then there 
  exists \(r, s \in \mathbb{F}[X]\) such that \(d = r f + s g\).
\end{theorem}

Now that we have established some basic properties about polynomials, we
would like to consider what it might mean to be a prime polynomial.

\begin{definition}[Irreducible]
  A polynomial \(f \in \mathbb{F}[X]\) is irreducible over \(\mathbb{F}\) if 
  and only if \(\deg f \ge 1\) and there does not exists 
  \(g, h \in \mathbb{F}[X]\), \(\deg g, \deg h < \deg f\) such that \(f = gh\). 
\end{definition}
\begin{remark}
  We see that this definition of irreducibility is consistent with the one we 
  have defined in ring theory since, \(\langle f \rangle\) is not a maximal 
  ideal if and only if there exists \(g \in \mathbb{F}[X]\), 
  \(\langle f \rangle \subset \langle g \rangle \subset \mathbb{F}[X]\) and 
  hence, \(f \in \langle g \rangle\) implying there exists 
  \(h \in \mathbb{F}[X], f = gh\).
\end{remark}

Given \(p \in \mathbb{Q}[X]\), it is usually difficult to decide whether
or not it is irreducible. However, there is some tools that can help us
determine the irreducibility of some rational polynomials.

\begin{theorem}
  Let \(p \in \mathbb{Q}[X]\) be a monic polynomial with integer coefficients.
  Then, 
  \begin{itemize}
    \item if \(\alpha \in \mathbb{Q}\) is a root of \(p\), then 
      \(\alpha \in \mathbb{Z}\);
    \item if \(p\) is irreducible over \(\mathbb{Q}\), then it has a monic
      factorisation \(q, r\), where \(q, r\) also have integer coefficients. 
  \end{itemize}
\end{theorem}
\proof

The first part follows easily while the other is Gauss' lemma. \qed

\begin{theorem}
  Let \(p \in \mathbb{F}[X]\) be irreducible, and \(a, b \in \mathbb{F}[X]\), 
  if \(p \mid ab\), then \(p \mid a\) or \(p \mid b\).
\end{theorem}
\proof

Suppose \(p \nmid a\), then \(\gcd(p, q) = 1\) and by Bezout's, there
exists \(r, s \in \mathbb{F}[X]\) such that \(rp + sa = 1\) so
\(b = rpb + sab\). Now, as \(p \mid ab\), there exists
\(q \in \mathbb{F}[X]\) such that \(ab = pq\) and so \(b = (rb + sq)p\)
and thus \(p \mid b\). \qed

\begin{theorem}[Unique factorisation Theorem for Polynomials]
  Let \(f \in \mathbb{F}[X]\) with \(\deg f \ge 1\), then there exists a unique 
  sequence of polynomials \((p_i)_{i = 1}^r \subset \mathbb{F}[X]\), such that 
  \(f = \prod p_i\).
\end{theorem}
\proof

Let us first prove existence. We induct on the degree of \(f\). For
\(\deg f = 1\), the result is trivial so let use consider the theorem
with \(\deg f = k + 1\) with the inductive hypothesis. Now, if \(f\) is
irreducible, the result follows so suppose otherwise. Then \(f\) can be
factorised into two polynomials with degree less than that of \(f\).
But, by the inductive hypothesis, these two polynomial can be
factorised, so, by multiplying their factors, can \(f\) be factorised.

Let us now prove uniqueness. Suppose now
\(f = \prod_{i = 1}^r p_i =  \prod_{j = 1}^s q_j\). Then, by considering
that for all \(i\) \(p_i \mid q_j\) for some \(j\), the result follows.
\qed

Lastly, we conclude on defining the least common multiple for
polynomials.

\begin{definition}[Least Common Mulitple]
  Let \(f, g \in \mathbb{F}[X]\), then the least common multiple of \(f\) and 
  \(g\), \(\mathop{\mathrm{lcm}}(f, g)\) is the polynomial \(h\) such that \(f \mid h, g \mid h\) 
  and for all \(k \in \mathbb{F}[X]\), if \(f \mid k\) and \(g \mid k\), then 
  \(h \mid k\).
\end{definition}

Similarly to the greatest common multiple, we find the least common
multiple exists and is unique (up to scalar multiplication).

\hypertarget{minimal-polynomial}{%
\subsection{Minimal Polynomial}\label{minimal-polynomial}}

We will in this section take a look at the minimal polynomial and some
of its applications.

\begin{definition}[Minimal Polynomial]
  The minimal polynomial of \(T \in \mathop{\mathrm{End}}(V)\) is the non-zero monic polynomial 
  \(m_T(x)\) of least degree such that \(m_T(T) = x \mapsto 0\).
\end{definition}

Straight away, we see \(m_T \mid \chi_T\) as, by the Euclidean
algorithm, there exists \(q, r \in \mathbb{F}[X]\) such that
\(\chi_T = q m_T + r\) where \(\deg r < \deg m_T\) if \(r \neq 0\). By
evaluating with \(T\) on both sides, we have \(0 = r(T)\), so \(r\)
annihilates \(T\). But this contradicts the minimality of \(m_T\), so
\(r = 0\). This can be generalised to any annihilators of \(T\).

\begin{lemma}
  Let \(T \in \mathop{\mathrm{End}}(V)\) and let \(p \in \mathbb{F}[X]\) such that \(p(T) = 0\). 
  Then, a minimal polynomial of \(T\) divides \(p\). 
\end{lemma}
\proof

By the Euclidean algorithm, there exists, \(q, r \in \mathbb{F}[X]\),
where \(r = 0\) or \(\deg r < \deg m_T\), such that \(p = qm_T + r\).
Thus, \(0 = p(T) = q(T) m_T(T) + r(T) = r(T)\). \# (Minimality of
\(m_T\).) \qed

Furthermore, we can deduce that the minimal polynomial is unique.

\begin{theorem}
  Let \(T \in \mathop{\mathrm{End}}(V)\) and let \(m_T\) and \(m'_T\) be minimal polynomials of 
  \(T\), then \(m_T = m'_T\).
\end{theorem}
\proof

By the previous lemma, we have \(m_T \mid m'_T\) so there exists
\(p \in \mathbb{F}[X]\), such that \(m_T = p m'_T\). Now, as both
\(m_T\) and \(m'_T\) are the minimal polynomials, they must have the
same degree, thus \(\deg p = 0\) and hence, is a constant. But, since
both \(m_T\) and \(m'_T\) are monic, \(p = 1\), so \(m_T = m'_T\). \qed

As one can imagine, the minimal polynomial for matrices is defined
similarly and alike many other properties, is shared for similar
matrices.

\begin{theorem}
  Let \(A, B \in M_n{\mathbb{F}}\) and \(A \sim B\). Then the minimal polynomial 
  of \(A\), \(m_A\) is the same as the minimal polynomial of \(B\), \(m_B\).
\end{theorem}
\proof

Suppose \(A = P^{-1} B P\). Then,
\(0 = m_A(A) = m_A(P^{-1}BP) = P^{-1} m_A(B) P\), so \(m_A(B) = 0\) as
\(P\) is invertible. By symmetry, \(m_B(A) = 0\) and thus, the result
follows by divisibility. \qed

As the minimal polynomial is pretty powerful, it will be helpful to be
able to compute the minimal polynomial. We will now develop some tools
to help us find this minimal polynomial.

\begin{theorem}
  Let \(T \in \mathop{\mathrm{End}}(V)\), then \(\lambda\) is an Eigenvalue of \(T\) if and only 
  if \(m_T(\lambda) = 0\).
\end{theorem}
\proof

\((\implies)\) If \(\lambda\) is an Eigenvalue of \(T\) then there
exists non-zero \(v \in V\), \(T(v) = \lambda v\), then
\(0 = m_T(T)(v) = \sum a_i T^i(v) = \sum a_i \lambda^i v = m_T(\lambda)v\).
Now, as \(v\) is non-zero, \(m_T(\lambda) = 0\).

\((\impliedby)\) Backwards direction follows straight away as if
\(m_T(\lambda) = 0\), as \(m_T \mid \chi_T\) there exists some
polynomial \(p\) such that \(\chi_T = m_T p\) and thus,
\(\chi_T(\lambda)  = m_T(\lambda)p(\lambda) = 0\), and hence, is an
Eigenvalue. \qed

With that, in order to find the minimal polynomial, it will be
insightful to find the characteristic polynomial as the minimal
polynomial shares roots and divides it (and then you can check all the
cases).

\begin{prop}\label{comp_min}
  The minimal polynomial of the companion matrix of some polynomial \(p\) is \(p\).
\end{prop}

Before we can prove proposition \ref{comp_min}, let us prove another
useful lemma.

\begin{lemma}
  Let \(A \in M_n(\mathbb{F})\) and suppose there exists some \(v \in \mathbb{F}^n\),
  such that the set \(S := \{A^i v \mid i \le k \}\) is linearly independent for 
  some \(k < n\). Then all polynomials that annihilates \(A\) has degree at 
  least \(k + 1\).
\end{lemma}
\proof

Suppose there exists \(p \in \mathbb{F}[X]\) with degree \(l \le k\)
such that \(p(A) = 0\). Then \(0 = p(A)v = \sum A^i v\). \# \qed

\proof

(Proposition \ref{comp_min}). By the previous lemma, it suffices to find
some \(v\) such that \(\{C(p)^i v \mid i \le n - 1\}\) is linearly
independent. Straight away, we see \(v = e_1\) suffices so we are done.
\qed

While, we proved that the minimal polynomial share linear factors with
the characteristic polynomial, we hope the same is true for all
irreducible factors. This turns out to be true.

\begin{theorem}
  Let \(T \in \mathop{\mathrm{End}}(V)\), then, for all \(p \in \mathbb{F}[X]\) such that 
  \(p\) is a irreducible factor of \(\chi_T\), \(p \mid m_T\).
\end{theorem}
\proof

We will prove this theorem by cases on whether there exists a proper
non-trivial \(T\)-invariant subspace of \(V\), so first, let us suppose
such a subspace \(W\) exists.

Let us induct on the dimension of \(V\), n.~The theorem is trivial for
\(n = 1\), so let us consider the case for \(n = k + 1\) with the
inductive hypothesis holding for all \(n = m \le k\). Again, as \(W\) is
\(T\)-invariant, we have \[[T]_B = 
    \left[
      \begin{array}{c|c}
        [T_W]_{B_W} & A \\
        \midrule
        \mathbf{0} & [\bar{T}]_{\bar{B}}
      \end{array}
      \right]. \] Now, suppose we write \(m_T(x) = \sum a_i x^i\), then,
\begin{align*}
    0 & = m_T([T]_B) = \sum a_i [T]_B^i \\
      & = \left[
          \begin{array}{c|c}
            \sum a_i [T_W]_{B_W}^i & A' \\
            \midrule
            \mathbf{0} & \sum a_i [\bar{T}]_{\bar{B}}^i
          \end{array}
          \right] \\
      & = \left[
          \begin{array}{c|c}
            m_T([T_W]_{B_W}) & A' \\
            \midrule
            \mathbf{0} & m_T([\bar{T}]_{\bar{B}})
          \end{array}
          \right],
  \end{align*} so \(m_T\) annihilates both \([T_W]_{B_W}\) and
\([\bar{T}]_{\bar{B}}\). Now, by considering
\(p \mid \chi_T = \chi_{[T_W]_{B_W}} \chi_{[\bar{T}]_{\bar{B}}}\), where
\(p\) is irreducible, \(p \mid \chi_{[T_W]_{B_W}}\) or
\(p \mid \chi_{[\bar{T}]_{\bar{B}}}\). Either way, by the inductive
hypothesis, \(p \mid m_{[T_W]_{B_W}}\) or
\(p \mid m_{[\bar{T}]_{\bar{B}}}\), both of which divides \(m_T\) as
\(m_T\) annihilates their respecctive matrices.

Let us now consider the case in which there does not exists a proper
non-trivial \(T\)-invariant subspace of \(V\). But, then, similar to the
proof of the Cayley-Hamilton theorem, we can construct some basis \(B\)
such that \([T]_B\) is the companion matrix of \(\chi_{[T]_B}\). Now as
the companion matrix has the same minimal polynomial as the
characteristic polynomial, we are done! \qed

Before we end this section, I would like to prove a powerful result
regarding diagonalisability.

\begin{theorem}\label{diag_distinct}
  Let \(T \in \mathop{\mathrm{End}}(V)\), and suppose there exists a non-zero polynomial \(p\) of 
  degree greater than 0 such that \(p\) annihilates \(T\) and \(p\) can be 
  factored into distinct roots. Then \(T\) is diagonalisable. 
\end{theorem}
\proof

Suppose we write \(p(x) = \prod(x - \lambda_i)\), then, it suffices to
show that
\(\dim \ker (\prod(T - \lambda_i)) \le \sum \dim \ker(T - \lambda_i)\)
as the kernel of \(\prod(T - \lambda_i)\) is \(V\) while the latter is
the sum of the dimensions of the Eigenspaces. This can be proved by
showing \(\dim \ker T_1 T_2 \le \dim \ker T_1 + \dim \ker T_2\) and
applying induction. \qed

\begin{prop}
  Let \(T_1, T_2 \in \mathop{\mathrm{End}}(V)\), then 
  \(\dim \ker T_1 T_2 \le \dim \ker T_1 + \dim \ker T_2\).
\end{prop}
\proof

It is easy to see that \(\ker T_2 \le \ker T_1 T_2\) so it suffices to
show \(\dim \ker T_1 T_2 / \ker T_2 \le \dim \ker T_1\) since for
\(W \le V\), \(\dim V / W = \dim V - \dim W\).

Consider the map
\(\phi : \ker T_1 T_2 / \ker T_2 \to \dim \ker T_1 :  v + \ker T_2 \mapsto T_2(v)\).
\(\phi\) has well-defined range as for all \(v \in \ker T_1 T_2\),
\(T_2(v) \in \ker T_1\). We will now show \(\phi\) is well-defined
overall and injective all at once. Let \(v_1, v_2 \in \ker T_1 T_2\),
then,
\[v_1 + \ker T_2 = v_2 + \ker T_2 \iff v_1 - v_2 \in \ker T_2 \iff T_2 (v_1 - v_2) = 0
  \iff T_2(v_1) = T_2(v_2).\] Furthermore, it is easy to see that
\(\phi\) is a linear map since \begin{align*}
    \phi((v_1 + \ker T_2) + (v_2 + \ker T_2)) & = \phi((v_1 + v_2) + \ker T_2) \\
      & = T_2(v_1 + v_2) = T_2(v_1) + T_2(v_2) \\
      & = \phi(v_1 + \ker T_2) + \phi(v_2 + \ker T_2).
  \end{align*} so \(\phi\) is a injective linear map. Now, as the image
of an linearly independent set under a injective linear map is also
linearly independent, we can construct an linearly independent set in
\(\ker T_1\) with cardinality \(\dim \ker T_1 T_2 / \ker T_2\) by taking
the image of its basis over \(\phi\). Thus,
\(\dim \ker T_1 T_2 / \ker T_2 \le \dim \ker T_2\) and the result
follows from proposition \ref{dim_sub}. \qed

\hypertarget{canonical-forms-of-vector-spaces}{%
\section{Canonical Forms of Vector
Spaces}\label{canonical-forms-of-vector-spaces}}

As mentioned in the introduction, the highlight of this course are the
canonical form theorems on general vector spaces (that is we would like
to show any matrix over some field is similar to a particularly ``nice''
block diagonal matrix). This will allow us to deduce properties about
general matrices and correspondingly, linear maps.

In order to achieve this, we will first develop some theories on how to
decompose a general vector space, that is, given vector space \(V\), we
would like to find \(V_i\) such that \(V = \bigoplus V_i\).

\hypertarget{primary-decomposition}{%
\subsection{Primary Decomposition}\label{primary-decomposition}}

\begin{theorem}[Primary Decomposition Theorem]
  Let \(V\) be a finite dimensional vector space of the field \(\mathbb{F}\), and 
  let \(T \in \mathop{\mathrm{End}}(V)\) with minimal polynomial \(m_T\). Suppose \(m_T\) has 
  the irreducible factors \(p_i\) such that,
  \[m_T = \prod_{i = 1}^k p_i^{n_i},\]
  where \(p_i \neq p_j\) for all \(i \neq j\). Then, 
  \begin{itemize}
    \item V = \(\bigoplus_{i = 1}^k \ker(p_i(T)^{n_i})\);
    \item \(\ker(p_i(T)^{n_i})\) is \(T\)-invariant;
    \item each restriction of \(T\) on \(\ker(p_i(T)^{n_i})\) has minimal polynomial 
      \(p_i^{n_i}\).
  \end{itemize}
\end{theorem}

Straight away, we see that this decomposition is unique by the unique
factorisation theorem, so, we can call it a canonical decomposition.
Furthermore, if all factors of \(m_T\) are linear, the factorisation
becomes, \[m_T = \prod_{i = 1}^k (x - \lambda_i)^{n_i},\] for distinct
\(\lambda_i\). In this cases, the individual decomposition becomes
\(\ker (T - \lambda I)^{n_i}\) which are the \emph{generalised
Eigenspace} of \(T\). Lastly, we notice a direct corollary of this
theorem is theorem \ref{diag_distinct}, so we have alternatively an
(arguably) easier proof of this.

Before we can prove the primary decomposition theorem, let us prove the
following lemma.

\begin{lemma}
  Let \(T \in \mathop{\mathrm{End}}(V)\) and suppose \(p_1, p_2 \in \mathbb{F}[X]\) are coprime 
  such that \(p_1(T)p_2(T) = 0\), then \(V = \ker p_1(T) \oplus \ker p_2(T)\). 
  Furthermore, if \(m_T = p_1 p_2\), then the restriction of \(T\) to 
  \(\ker p_i(T)\) has minimal polynomial \(p_i\) for \(i = 1, 2\).
\end{lemma}
\proof

By Bezout's, there exists \(s, t \in \mathbb{F}[X]\) such that
\(1 = sp_1 + tp_2\). So, by evaluating at \(T\), we have
\(I = s(T)p_1(T) + t(T)p_2(T)\). Then for all \(v \in V\),
\(v = Iv = s(T)p_1(T)(v) + t(T)p_2(T)(v) = v_1 + v_2\). Now, as
\(p_1 p_2 = 0\), we have
\(p_2(v_1) = p_2(s(T)p_1(T)(v)) = p_2 s(T)p_2 p_1(T)(v) = 0\), we have
\(v_1 \in \ker p_2(T)\) and similarly, \(v_2 \in \ker p_1(T)\), and so,
\(V = \ker p_1(T) + \ker p_2(T)\). Also, suppose \(v \in V_1 \cap V_2\),
then \(v = Iv = s(T)p_1(T)(v) + t(T)p_2(T)(v) = 0\), so
\(V = \ker p_1(T) \oplus \ker p_2(T)\).

Now, suppose \(m_T = p_1 p_2\) and let us denote the minimal polynomial
of \(T\) restricted on \(\ker p_i(T)\) as \(m_i\) for \(i = 1, 2\). By
definition \(p_i(T_i) := p_i(T \mid_{\ker p_i(T)}) = 0\), so
\(m_i \mid p_i\). So, as \(p_1\) and \(p_2\) are coprime, so are \(m_1\)
and \(m_2\), thus, \(m_T = \mathop{\mathrm{lcm}}(m_1, m_2) = m_1 m_2\)
and the result follows. \qed

We can see straight away how this lemma can help us prove the theorem.

\proof (Primary Decomposition Theorem). Let us write
\(m_T = \prod_{i = 1}^k p_i^{n_i}\) and we will induct on \(k\). The
theorem is trivial for for the base case so we begin by letting
\(k = n + 1\). Then, by considering the previous lemma on \(p_{n + 1}\)
and \(\prod_{i = 1}^n p_i\), the theorem follows. \qed

\hypertarget{jordan-canonical-form}{%
\subsection{Jordan Canonical Form}\label{jordan-canonical-form}}

Recall that the triangularisation theorem is limited in many ways. Not
only does it not apply for matrices who's characteristic polynomial
cannot be factored, for those matrices the theorem does apply, the
triangularisation is not unique. We attempt to improve upon this with
the Jordan canonical form theorem.

\begin{definition}[Jordan Blocks]
  Let \(\mathbb{F}\) be a field and let \(\lambda \in \mathbb{F}\). Then 
  \(J_n(\lambda) \in M_n(\mathbb{F})\) is a Jordan block if and only if 
  \[J_n(\lambda) = \begin{bmatrix}
      \lambda & 1 & 0 & \cdots & 0\\
      0 & \lambda & 1 & \cdots & 0\\
      0 & 0 & \lambda & \cdots & 0\\
      \vdots & \vdots &  \vdots & \ddots & \vdots \\
      0 & 0 & 0 & \cdots & \lambda\\
    \end{bmatrix}.\]
\end{definition}

Jordan blocks are a nice form of matrices and we can derive several
properties straight away.

\begin{prop}
  Let \(J = J_n(\lambda)\) be some Jordan block, then 
  \begin{itemize}
    \item \(\chi_J(x) = (x - \lambda)^n = m_J\);
    \item \(\lambda\) is the only Eigenvalue and \(a(\lambda) = n, g(\lambda) = 1\);
    \item \((J - \lambda I)e_{i + 1} = e_i\) for \(i = 1, \cdots, n - 1\) 
      and \((J - \lambda I)e_1 = 0\).
  \end{itemize}
\end{prop}
\proof

Obvious. \qed

Before we state the Jordan canonical form theorem, let us recall some
properties of block diagonal matrices. Let \(A_i\) have characteristic
polynomial \(\chi_i\) and let \(A = \bigoplus A_i\), then

\begin{itemize}
  \item \(\chi_A = \prod \chi_i\);
  \item \(m_A = \mathop{\mathrm{lcm}}(m_i)\);
  \item for all \(\lambda\) and Eigenvalue of \(A\), 
    \(\dim E_\lambda(A) = \sum \dim E_\lambda(A_i)\);
  \item for all \(q \in \mathbb{F}[X]\), \(q(A) = \bigoplus q(A_i)\).
\end{itemize}

With that, we can state the Jordan canonical form theorem.

\begin{theorem}[Jordan canonical form theorem]
  Let \(A \in M_n(\mathbb{F})\) and the characteristic polynomial is a product 
  of linear factors over \(\mathbb{F}\). Then, 
  there exists some \(J\), \(A \sim J = \bigoplus J_{n_i}(\lambda_i)\) and 
  this decomposition is unique up to the ordering of the Jordan blocks.
\end{theorem}

We call this unique decomposition the Jordan canonical form of \(A\).

\begin{remark}
  The condition on which the characteristic polynomial must be factorisable is 
  necessary as otherwise it would not even be triangulisable. Also, we note that 
  the \(\lambda_i\) is not necessarily distinct.
\end{remark}

Of course, there is an equivalent theorem for linear maps. In fact, we
shall prove the linear map version now.

\begin{theorem}
  Let \(V\) be a \(n\)-dimensional vector space over \(\mathbb{F}\) and 
  \(T \in \mathop{\mathrm{End}}(V)\) such that \(\chi_T\) is a product of linear factors over 
  \(\mathbb{F}\). Then, there exists a basis \(B\) of \(V\) such that 
  \([T]_B = J = \bigoplus J_{n_i}(\lambda_i)\), and furthermore, \(J\) is 
  uniquely determined by \(T\) (up to ordering).
\end{theorem}
\proof

We shall provide a proof for uniqueness first.

Suppose that \(T\) has only one Eigenvalue \(\lambda\) and
\[[T]_B \sim J = \bigoplus J_{n_i}(\lambda)\] for some basis of \(V\),
\(B\). Let us write \(J = \bigoplus_{i = 1}^r J_i(\lambda)^{a_i}\) where
\(a_i\) is the number of \(\lambda\)-blocks with size \(i\). Then, let
us define \[m_i := \dim \text{Im}(J - \lambda I)^i.\] Now, by
considering \(J_r(0)^i\), for \(i = 1, \cdots, r\) we have
\begin{align*}
    m_{r - 1} & = a_r,\\ 
    m_{r - 2} & = a_{r - 1} + 2a_r,\\
    m_{r - 3} & = a_{r - 2} + 2a_{r - 1} + 3a_r,\\
  \end{align*} and so on. In general, we see
\[m_i = \sum_{j = 1}^{r - i} j a_{i + j}\] for \(i = 0, \cdots, r - 1\).
Thus, \(m_i\) uniquely determines \(a_i\); and as \(m_i\) is computable
and is uniquely determined by \((J - \lambda I)^i\), we have \(a_i\) is
unique.

Now, let us relax the number of Eigenvalues condition. Suppose \(T\) has
at least two Eigenvalues with one of them being \(\lambda\), then we can
write \[[T]_B \sim J = J_\lambda \oplus L,\] where \(L\) is the direct
sum of all the other Jordan blocks. Now, as \(L - \lambda I\) is
invertible (easy to see as the determinant is non-zero), it has full
rank \(l\). Thus, by defining \[r_i = \dim \text{Im}(J - \lambda I)^i,\]
we have \(r_i = \dim \text{Im}(J_\lambda - \lambda I)^i + l\), allowing
us to compute
\(m_i = \dim \text{Im}(J_\lambda - \lambda I)^i = r_i - l\). \qed

In most cases, it might not be necessary to go through the steps as
described above to find the Jordan canonical form of a matrix.

\begin{prop}
  Let \(A \in M_n(\mathbb{F}) \sim J\) where \(J\) is in Jordan Canonical Form. 
  Suppose we write 
  \[J = (J_{n_1}(\lambda) \oplus \cdots \oplus J_{n_\alpha}(\lambda)) \oplus 
    (J_{m_1}(\mu) \oplus \cdots \oplus J_{m_\beta}(\mu)) \oplus \cdots,\]
  where \(\lambda, \mu, \cdots\) are Eigenvalues of \(A\), then,
  \begin{itemize}
    \item the sum of the size of the \(\lambda\)-blocks equals the algebraic 
    multiplicity of \(\lambda\), i.e. \(n_1 + \cdots + n_\alpha = a(\lambda)\);
    \item the number of \(\lambda\)-blocks equals the geometric multiplicity of 
    \(\lambda\), i.e. \(\alpha = g(\lambda)\);
    \item \(\max \{n_1, \cdots, n_\alpha\} = r\) where \((x - \lambda)^r\) is the 
    highest power of \(x - \lambda\) dividing \(m_A\).
  \end{itemize}
\end{prop}
\proof

The proofs are rather straight forward.

\begin{itemize}
    \item Obvious by considering the characteristic polynomial of \(J_k(\lambda)\).
    \item True by considering each \(\lambda\)-block has geometric multiplicity one, 
      and that \(J\) and \(A\) share the same \(\lambda\)-Eigenspace.
    \item Since for all \(n\), \(J_n(\lambda)\) has minimal polynomial 
    \((x - \lambda)^n\), we have the power dividing \(m_J\) is 
    \(\mathop{\mathrm{lcm}}((x - \lambda)^{n_i}, \cdots, (x - \lambda)^{n_\alpha})\) which equals 
    \((x - \lambda)^{\max \{n_1, \cdots, n_\alpha\}}\).
  \end{itemize}
\qed

Now, let us finish the proof of the JCF theorem by proving that a linear
map is similar to its JCF (we can say this since we had proved
uniqueness). \proof First we reduce the proof to the case where
\(T \in \mathop{\mathrm{End}}(V)\) has only one Eigenvalue. Suppose
\[m_T(x) = \prod_{i = 1}^n(x - \lambda_i)^{n_i},\] where \(\lambda_i\)
are distinct. By the primary decomposition theorem
\[V = \bigoplus_{i = 1}^n \ker (T - \lambda_i I).\] Then, if \(B_i\) is
a basis of \(\ker (T - \lambda_i I)\), then \(\bigcup_i B_i\) is a basis
of \(V\). Furthermore, as \(\ker (T - \lambda_i I)\) are
\(T\)-invariant, we can write
\[[T]_B = \bigoplus_{i = 1}^n [T_{V_i}]_{B_i},\] and for all \(i\),
\([T_{V_i}]_{B_i}\) has minimal polynomial \((x - \lambda_i)^{n_i}\),
that is \([T_{V_i}]_{B_i}\) has only one Eigenvalue. Hence, it suffices
to show the theorem whenever \(T\) has only one Eigenvalue \(\lambda\).

If \(T\) only has one Eigenvalue \(\lambda\), then
\(\chi_T(x) = (x - \lambda)^n\) where \(n = \dim V\). Suppose we define
\(S := T - \lambda I\), then straight away, we see \(S^n = 0\) by
Cayley-Hamilton so \(S\) is nilpotent, and hence, it suffices to show
that \(S\) is similar to a JCF with 0-Jordan blocks.

By considering the 0-Jordan blocks are cyclic matrices, that is, we see
\([S]_B = J_{1}(0) \oplus \cdots\) where \(B := \{v_1, \cdots, v_n\}\)
if and only if \(S(v_i) = S(v_{i + 1}\) for all \(i = 0, \cdots n - 1\)
and \(S(v_{n_1}) = 0\), we can write the basis \(B\) as the union
\[\bigcup_{i = 1}^k \{v_1, S(v_1), \cdots, S^{n_i - 1}(v)\}.\] So, it
suffices to find such \(\{v_1, \cdots, v_k\}\) (we call basis generated
using \(S\) by this set a Jordan basis of \(V\)).

To prove this, we shall induct on \(n = \dim V\). The base case is
trivial so assume the existence of \(\{v_1, \cdots, v_k\}\) for all
\(\dim V < n\). Consider \(\text{Im}(S) = S(V)\) is a proper subspace of
\(V\) (as \(S\) is nilpotent). So, by the inductive hypothesis, there
exists a Jordan basis of \(S(V)\) generated by
\(U = \{u_1, \cdots, u_r\}\). Now as, for all \(u_i \in U\),
\(u_i \in S(V)\), there exists \(v_i \in V\), such that
\(S(v_i) = u_i\). Furthermore, as \(S^{m_i}(u_i) = 0\), we see that
\(S^{m_i - 1}(u_i) \in \ker S\), and thus, we can extend this to a basis
of \(\ker S\) by adding \(w_1, \cdots, w_s\).

Finally, by letting \(B = \{v_1, \cdots, v_r, w_1 \cdots, w_s\}\), we
see that \(B\) generates a Jordan basis if and only if \(S^i(B)\)
linearly independent (since \(\dim \ker S + \dim \text{Im} S = n\)).
Suppose then there exists \(\mu\) such that \(\sum \mu S^i(B) = 0\).
Then, by applying \(S\) on both sides, we receive
\(\sum \mu'S^i(U) = 0\) implying the non-vanishing terms has zero
coefficient. Furthermore, we see the vanishing vectors is a basis of
\(\ker S\), so they also have zero coefficient, and hence \(\mu = 0\)
and \(S^i(B)\) is linearly independent. Thus, we have found a Jordan
basis of \(S\), so we are done! \qed

Now that we have proved that the JCF theorem, we would like to compute
the Jordan basis of \(V\) such that \([T]_B\) is in JCF for some
appropriate \(T \in \mathop{\mathrm{End}}(V)\).

Let \(S \in \mathop{\mathrm{End}}(V)\) be nilpotent,

\begin{enumerate}
  \item Compute the chain of subspaces
    \[V \ge S(V) \ge S^2(V) \ge \cdots \ge S^r(V) \ge 0\]
    where \(S^{r + 1}(V) = 0\).
  \item Find a basis of \(S^r(V)\) and for each \(i\), add \(v_i\) to \(U\) where 
    \(S(v_i) = u_i\). Furthermore, if needed, extend \(U\) such that it has span 
    greater than \(\ker S^{r - 1}\). This results in \(U\) to be a Jordan basis of \(S^{r - 1}(V)\).
  \item Repeat the above step until we get a Jordan basis of \(V\).
\end{enumerate}

\hypertarget{cyclic-subspaces}{%
\subsection{Cyclic Subspaces}\label{cyclic-subspaces}}

So far we have seen a satisfactory canonical form for matrices who's
characteristic polynomial factors into linear roots. However, this is
often not the case. So, in the next few sections, we shall develop
theories for the \emph{rational canonical form} theorem.

\begin{definition}
  Let \(V\) be a finite dimensional vector space over some field \(\mathbb{F}\), 
  \(T \in \mathop{\mathrm{End}}(V)\), \(0 \neq v \in V\) and define the subspace
  \begin{align*}
    Z(v, T) := & \{p(T)(v)) \mid p \in \mathbb{F}[X]\} \\
      = & \text{sp}(v, T(v), T^2(v), \cdots),
  \end{align*}
  and we call it the  \(T\)-cyclic subspace of \(V\) generated by \(v\).
\end{definition}

Straight away, we see that \(Z(v, T)\) is \(T\)-invariant and thus, we
can restrict \(T\) by \(Z(v, T)\) (denoted by \(T_v\)). Also, if \(v\)
is an Eigenvector of \(T\), then \(Z(v, T) = \text{sp}(v)\).

\begin{definition}[\(T\)-annihilator]
  The \(T\)-annihilator of \(v\) is the smallest degree, monic polynomial \(m_v(x)\) 
  such that \(m_v(T)(v) = 0\).
\end{definition}

Consider the sequence \(v, T(v), T^2(v), \cdots\). As \(V\) is finite
dimensional there exists some \(k\), where \(k\) is the smallest natural
number such that \(T^k(v) \in \text{sp}(v, \cdots, T^{k - 1}(v))\).
Then, there exists \(\mu\), such that \(T^k(v) = - \sum \mu T^i(v)\),
and so, \((\sum^n \mu T^i)v = 0\). Hence, by the choice of \(k\),
\(m_v(x) = x^k \sum \mu x^i\).

\begin{prop}
  Given \(k\) as defined above, \(B = \{v, \cdots, T^{k - 1}v\}\) is a basis 
  of \(Z(v, T)\). Furthermore, \([T_v]_B\) is the companion matrix of \(m_v\) and 
  the minimal polynomial of \(T_v\) is \(m_v\).
\end{prop}
\proof

Obvious except perhaps for the last part. However, we have proved that
the minimal polynomial of the companion matrix is the polynomial it is
associated with, that is \(m_v\). \qed

\end{document}
