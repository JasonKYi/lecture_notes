% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Network Science},
  pdfauthor={Kexing Ying},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=red,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin = 1.5in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{lipsum}
\usepackage[ruled,vlined]{algorithm2e}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}[theorem]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\title{Network Science}
\author{Kexing Ying}
\date{May 15, 2020}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{fundamental-graph-theory}{%
\section{Fundamental Graph Theory}\label{fundamental-graph-theory}}

\hypertarget{basic-concepts}{%
\subsection{Basic Concepts}\label{basic-concepts}}

We begin with a few basic definitions.

\begin{definition}[Graph]
  A graph \(G\) is a tuple \((V(G), E(G))\) equipped with a function 
  \(\sim : E(G) \to V(G) \to V(G) \to \text{Prop}\) where \(V(G)\) is the 
  \textit{vertex set}, \(E(G)\) is the \textit{edge set} and for all 
  \(e \in E(G)\) there exists a unique pair \(v_1, v_2 \in V(G)\) such that 
  \(v_1 \sim_e v_2\). We write \(v_1 \sim_e v_2\) as a short hand for 
  \(\sim(e, v_1, v_2) = \text{true}\).
\end{definition}

Note that this definition works for both directed and undirected graphs
as by this definition, a undirected graph is a directed graph with the
condition that for all \(e \in E(G)\), \(\sim_e\) is symmetric.

\begin{definition}[Subgraph]
  Let \(G\) be a graph, then \(H\) is a subgraph of \(G\) if and only if \(H\) 
  is a graph such that \(V(H) \subseteq V(G)\), \(E(H) \subseteq E(G)\) and the 
  restriction \(\sim^G\mid_H = \sim^H\). We will write \(H \le G\) for \(H\) is 
  a subgraph of \(G\).
\end{definition}

\begin{definition}[Loop]
  Let \(G := (V(G), E(G))\) be a graph and \(e \in E(G)\) be an edge. We say 
  \(e\) is a loop at some \(v \in V(G)\) if and only if \(v \sim_e v\).
\end{definition}

\begin{definition}[Multiple Edges] 
  Let \(G := (V(G), E(G))\) be a graph and \(e, f \in E(G)\) be edges. We call 
  \(e, f\) be multiple edges if and only if there exists \(v_1, v_2 \in V(G)\) 
  such that \(v_1 \sim_e v_2\) and \(v_1 \sim_f v_2\).
\end{definition}

\begin{definition}[Simple]
  We call a graph simple if it contains no loops nor multiple edges.
\end{definition}

If a graph is simple we can then model the edge set of the graph
\(E(G)\) by a set of unordered tuples where each edge \(e\) with end
points \(v_1, v_2\) can be uniquely represented by \(e = v_1 v_2\)
(commutative if and only if \(G\) is undirected).

\begin{definition}[Complete Graph]
  A graph \(G\) is complete if and only if \(G\) is simple and every vertex is 
  adjacent to every other vertices, i.e. 
  \(E(G) = \{v_i v_j \mid v_i, v_j \in V(G), i \neq j \}\).
\end{definition}

\begin{definition}[Adjacent]
  Let \(G := (V(G), E(G))\) be a graph and \(v_1, v_2 \in V(G)\), then \(v_1\) 
  and \(v_2\) are adjacent (or are neighbours) if and only if there exists some 
  edge \(e \in E(G)\) such that \(v_1 \sim_e v_2\).
\end{definition}

\begin{definition}[Path]
  Let \(G\) be a graph, then a path in \(G\) is a simple subgraph \(P\) of \(G\) 
  such that \(V(P)\) can be ordered in a list such that consecutive vertices are 
  adjacent. On top of this, if this ordering resulted in the first element to be 
  adjacent to the last, then we say \(P\) is a \textit{cycle}.
\end{definition}

Note that we said \emph{ordered}, so each element of \(V(P)\) can only
appear once in the arrangement (so no infinite loops back and forth).
This can be somewhat limiting as sometimes lists with duplicate
naturally arises, so let us consider the path induced by such a list.

\begin{lemma}\label{mk_path}
  Let \(G\) be a graph, \(v_1, v_2 \in V(G)\), and \(L\) a finite list of 
  vertices of \(G\) (possibly with repeats) such that consecutive vertices are 
  adjacent. Then \(v_1, v_2\) are connected.
\end{lemma}
\proof

We present an algorithm to find such a path. While \(L\) contains
duplicates, find the first pair of duplicates and remove every vertex in
the sequence between the two duplicates including the last duplicate.
This algorithm will always terminate as \(L\) is finite. \qed

\begin{definition}[Connected]
  A graph \(G\) is connected if and only if for all \(u, v \in V(G)\), there 
  exists a path \(P\) in \(G\) such that the rearrangement of \(P\) begins in 
  \(u\) and ends in \(v\).
\end{definition}

\hypertarget{graph-as-models}{%
\subsection{Graph as Models}\label{graph-as-models}}

\begin{definition}[Complement]
  Let \(G\) be a simple graph, then the complement of \(G\), \(\bar{G}\) is the 
  simple graph \((V(G), E(\bar{G}))\) where for all \(u, v \in V(G)\), 
  \(uv \in E(\bar{G})\) if and only if \(uv \notin E(G)\).
\end{definition}

Note that this complement graph is unique only if we restrict it to be
simple. Suppose \(G\) is simple and let \(v \in V(G)\), then
\(vv \notin E(G)\) by the no loop condition. Thus, if we do not restrict
\(\bar{G}\) to be simple, then we can add how many loops as we want at
\(v\), making the complement not unique.

\begin{prop}
  Let \(G\) be a simple graph, then the complement of \(G\) is unique.
\end{prop}
\proof

Let \(G_1, G_2\) be complements of \(G\). By definition
\(V(G_1) = V(G) = V(G_2)\) so \(G_1 = G_2\) if and only if
\(E(G_1) = E(G_2)\). Wlog. it suffices to show that
\(E(G_1) \subseteq E(G_2)\). Let \(uv \in E(G_1)\), then
\(uv \notin E(G)\) and thus \(uv \in E(G_2)\). \qed

Let us consider a real world problem. Suppose we have \(n\) job openings
and \(k\) applicants but not all applicants are qualified for all jobs.
We can easily model this problem by connecting each applicants to their
respective qualified jobs and ask whether we can find a subgraph that
consist of \(n\) pairwise disjoint edges.

Upon examining this question, we find that this particular model has an
interesting graph structure in which none of the jobs are adjacent to
each other (similarly for the applicants). This type of graphs are
called \emph{bipartite} and the set vertices representing people and
jobs respectively are called independent.

\begin{definition}[Independent]
  Let \(G\) be a graph and \(S \subseteq V(G)\). \(S\) is called an independent 
  set in \(G\) if and only if for all \(u, v \in S\), \(uv \notin E(G)\).
\end{definition}

\begin{definition}[Bipartite]
  A graph \(G\) is called bipartite if and only if \(V(G)\) is the disjoint 
  union of two independent sets in \(G\). We call these two independent sets the 
  \textit{partite} sets of \(G\).
\end{definition}

\hypertarget{graph-isomorphism}{%
\subsection{Graph Isomorphism}\label{graph-isomorphism}}

Similar to all mathematical structures, there exists a notion of
isomorphisms between two graphs that are essentially the
same\footnote{By essentially the 
same we mean that almost all properties of a graph commutes via an isomorphism.}

\begin{definition}[Isomorphism]
  An isomorphism from a simple graph \(G\) to a simple graph \(H\) is a 
  bijection \(f : V(G) \to V(H)\) such that for all \(uv \in E(G)\), 
  \(f(u)f(v) \in E(H)\). We write \(G \cong H\) as usual.
\end{definition}

\begin{theorem}
  Graph isomorphisms is a equivalence relation.
\end{theorem}

As usual, theorems of the above kind is simply by checking each of the
properties so we will omit it here. As an equivalence relation induces a
partition, a set of graphs can be quotiented out by the isomorphism
relation and we call each element of this quotient an \emph{isomorphism
class}.

As all graphs in a isomorphism class are pairwise isomorphic to one
another, they all share graph structures. Therefore, when discussing
graph structures, it makes sense to talk about a isomorphism class
rather than a particular graph. When we do this, we will informally call
it an \emph{unlabeled} graph.

We will now introduce some notations. The unlabeled path and cycle of
\(n\) vertices is denoted by \(P_n\) and \(C_n\) respectively while the
complete graph of \(n\) vertices is denoted by \(K_n\). If \(G\) is a
bipartite graph with \(n\) vertices such that two vertices are adjacent
if and only if they are in different partite sets, then we call \(G\) a
complete bipartite graph and denote it by \(K_{r, s}\) where \(r, s\)
are the sizes of the partite sets.

\hypertarget{trees}{%
\subsection{Trees}\label{trees}}

\begin{definition}[Minimally Connected]
  We say a graph \(G\) is minimally connected if and only if it is connected 
  and for all \(e \in E(G)\), the graph \((V(G), E(G) \setminus \{e\})\) is not 
  connected.
\end{definition}

\begin{definition}[Tree]
  A tree is a minimally connected graph.
\end{definition}

\begin{lemma}
  A tree does not have any cycles.
\end{lemma}
\proof

Let \(G\) be a tree and for contradiction suppose \(C \le G\) is a
cycle. Then by the definition of a cycle, there exists \(v \in V(C)\),
such that elements of \(V(C)\) can be arranged into a circle of
consecutively adjacent vertices. Let \(u, v\) be the first and last
element of \(V(C)\) in the above arrangement, and \(x, y \in V(G')\)
where \((G' := V(G), E(G) \setminus \{uv\})\). As the there path
arranged above remains, it follows that \(u\) and \(v\) are connected.
So, as \(x, y \in V(G') = V(G)\), there exists some path \(P\) from
\(x\) to \(y\) in \(G\). Now, if \(uv \in E(P)\), we can simply replace
that with path from \(u\) to \(v\) implying \(G'\) is also connected. \#
\qed

\begin{lemma}\label{tree_acyclic}
  The graph \(G\) is a tree if and only if it is connected and acyclic.
\end{lemma}
\proof

The forward direction follows directly from the above lemma so it
suffices to show that \(G\) is minimally connected if it it connected
and acyclic.

Suppose that \(G\) is not minimally connected, then there exists
\(uv \in E(G)\) such that \(G' := (V(G), E(G) \setminus \{uv\})\) is
also connected, so there exists a path \(P\) in \(G'\) from \(u\) to
\(v\). Now, as this sequence \(P\) by definition begins with \(u\) and
end with \(v\), and \(u\), \(v\) are adjacent by \(uv\) in \(G\), this
sequence is therefore a cycle in \(G\). \# \qed

\begin{lemma}
  Any two vertices on a tree is joined uniquely by a path.
\end{lemma}
\proof

Existence is true by the connectedness of a tree so all that remains is
to prove that the path is unique.

Let \(T\) be a tree, \(x, y \in V(T)\), and \(P_1, P_2\) paths from
\(x\) to \(y\) in \(T\). Suppose \(P_1 \neq P_2\) and let us denote
\(p_1, p_2\) the path arrangement. Let
\(S := \{ n \in \mathbb{N} \mid p_1[n] \neq p_2[n]\}\), then, as
\(P_1 \neq P_2\), \(S\) is not empty; and the fact that paths are
finite, \(S\) has a minimum and a maximum value \(i, j\) by the
well-ordering principle. Now, by connecting \(p_1[i - 1 : j + 1]\) to
\(p_2[i - 1 : j + 1]\) (both of which index makes sense as \(i > 1\) as
\(p_1[1] = x = p_2[1]\) and similarly for \(j\)), and by considering
lemma \ref{mk_path}, we have created a cycle in \(T\). \# \qed

In nature, many trees have \emph{leaves} and therefore, so do our
graph-theoretic trees (in fact our trees are stronger as \emph{all}
trees with two or more vertices have at least two leaves).

\begin{definition}[Leaf]
  A vertex in a tree is a leaf if and only if it has degree one, i.e. it is only 
  connected to one other vertex.
\end{definition}

\begin{lemma}\label{two_leaves}
  A tree with two or more vertices has at least two leaves.
\end{lemma}
\proof

Consider the longest path in this tree. It must have a leaf at either
end of the path as otherwise it is not the longest path in this tree.
\qed

\begin{lemma}\label{sub_leaf_is_tree}
  Let \(T\) be a tree and \(v \in V(T)\) a leaf. Then \(T - v\) is also a 
  tree\footnote{We write \(T - v\) for the graph 
  \((V(T)\setminus\{v\}, E(T)\setminus S)\) where \(S\) is the set of edges 
  with \(v\) as an endpoint.}.
\end{lemma}
\proof

\(T - v\) is connected as for all \(x, y \in V(T - v)\), \(x, y\) is
connected in \(T\) by some path \(P\). Now, as \(v \notin V(P)\) since
if otherwise \(v\) must be on the either end of the path implying either
\(x\) or \(y\) equals \(v\). Also, as \(v \notin V(P)\), \(e_v\), the
unique edge with endpoint \(v\) is also not in \(P\), so \(P \le T - v\)
connecting \(x\), and \(y\).

Thus, by lemma \ref{tree_acyclic}, it suffices to show that \(T - v\) is
acyclic. This is trivial as if \(C \le T - v\) is a cycle, then it is
also a cycle in \(T\), so we are done! \qed

\begin{lemma}\label{tree_edges}
  A tree of \(n\) vertices has \(n - 1\) edges.
\end{lemma}
\proof

For convenience let us denote the trees of \(n\) vertices by \(T_n\).

We apply natural number's induction on \(n\). If \(n = 1\) the result is
trivial. Suppose for \(n = k\), for all \(T_k\),
\(\left| E(T_k) \right| = k - 1\), and let us consider the case for the
tree \(T\) with \(n = k + 1\) vertices.

By lemma \ref{two_leaves}, \(T\) has at least two leaves and let \(l\)
be one of the leaves. Then by lemma \ref{sub_leaf_is_tree}, \(T - l\) is
a tree of \(n\) vertices, so it has \(k - 1\) number of edges. However,
as \(l\) is a leaf, there is only one edge with endpoint \(l\), so
\(\left| E(T) \right| = \left| E(T - v) \right| + 1 = k - 1 + 1 = k\).
\qed

\begin{lemma}
  A connected graph \(G\) of \(n\) vertices has at least \(n - 1\) edges.
\end{lemma}
\proof

We induct on the number of vertices. As the number of edges of a graph
is a natural number, it is greater or equal to \(0 = 1 - 1\), so true
for \(n = 1\).

Suppose \(G\) has \(k + 1\) vertices, then by excluded middle, either
\(G\) is minimally connected, or it is not. If it is, then it has
\(n - 1\) edges by the previous lemma so suppose otherwise. Then there
exists \(v \in V(G)\) such that \(G - v\) is still connected. As
\(G - v\) has \(k\) vertices, it has at least \(k - 1\) number of edges
by the inductive hypothesis. Now, as \(G\) is connected, \(v\) is at
least connected to another vertex, thus, \(G\) has at least \(k\) edges.
\qed

\begin{lemma}
  A connected graph \(G\) of \(n\) vertices is a tree if and only if it has 
  \(n - 1\) edges.
\end{lemma}
\proof

The forward direction of the proof is exact lemma \ref{tree_edges} so
let us consider the reverse direction.

As \(G\) is connected, by lemma \ref{tree_acyclic}, it suffices to show
that \(G\) is not cyclic. In order to show this, we apply induction on
the number of vertices of \(G\).

If \(G\) has one vertex, then it is trivially acyclic. Now suppose for
connected graphs \(G_k\) of \(k\) vertices and \(k - 1\) edges are
trees, let us consider the connected graph of \(G\) with \(k + 1\)
vertices and \(k\) edges. If \(G\) is minimally connected then \(G\) is
acyclic by lemma \ref{tree_acyclic} so suppose that there exists
\(v \in V(G)\) such that \(G - v\) is connected. Now as \(G\) is
connected, there exists at least one edge who has endpoint \(v\), so
\(G - v\) has at most \(k - 1\) edges. However, as \(G - v\) is
connected, it has at least \(k - 1\) edges by the above lemma so it has
exactly \(k - 1\) edges and \(v\) is a leaf. Therefore, as \(G - v\) is
a tree by the inductive hypothesis, we have \(G\) is also a tree. \qed

\hypertarget{working-with-networks}{%
\section{Working with Networks}\label{working-with-networks}}

In this section we will be less rigorous and focus more on the methods
used to analysis graphs (networks) especially really large ones.

\hypertarget{degree-distribution}{%
\subsection{Degree Distribution}\label{degree-distribution}}

Network systems vary in size but they are normally very large (that is
they are large enough such that we can't draw them by hand), so, in
order to analyse large networks, it is often useful to take a
probabilistic approach.

\begin{definition}[Degree of a Vertex of a Undirected Graph]
  Let \((V(G), E(G))\) be a undirected graph and \(v \in V(G)\), if \(v\) is the 
  end point of some \(e \in E(G)\), then we say \(v\) and \(e\) are 
  \textit{incident}. Then the degree of \(v\) is the number of incident edges.
\end{definition}

Suppose we denote the degree of some vertex \(v\) by \(d(v)\), then we
find the total number of edges is simply
\[ \left| E(G) \right| = \frac{1}{2} \sum_{v \in V(G)} d(v).\] Note that
the \(1 / 2\) factor is because each edge is incident to two vertices.

\begin{definition}[Average Degree of a Undirected Graph]
  Let \(G = (V(G), E(G))\) be a undirected graph, then the average degree of 
  \(G\) is \(\frac{1}{\left| V(G) \right|} \sum_{v \in V(G)} d(v) = 
  2 \left| E(G) \right| / \left| V(G) \right|\).
\end{definition}

The above, however, does not simply transfer to directed graphs since we
would loss the information of ``directedness'' of the graph. Therefore,
the degree is defined slightly differently for directed graphs.

\begin{definition}[Degree of a Vertex of a Directed Graph]
  Let \((V(G), E(G))\) be a directed graph and \(v \in V(G)\), then the degree 
  of \(v\) is simply the difference between number incoming edges and the number 
  of out going edges. 
\end{definition}

With the definition above, we see straight away the sum of the degrees
of all vertices in a directed graph is zero so the definition for
average degree does not apply for
digraphs\footnote{Digraph is an alternative word to directed graph.}
either. Therefore, instead defining the average degree by averaging the
sum of degrees, we use the average of the sum of either the incoming or
outgoing degrees (both of which are equal).

Let us now consider the \emph{degree distribution}, \(p_k\), a
characterisation of a graph that provides the probability that a
randomly selected vertex has \(k\) degree.

Straight away, given some graph \((V(G), E(G))\), let
\(S := \{ v \in V(G) \mid d(v) = k \}\), then
\(p_k = \left| S \right| / \left| V(G) \right|\).

\hypertarget{clustering-coefficient}{%
\subsubsection{Clustering Coefficient}\label{clustering-coefficient}}

The clustering coefficient of a graph attempts to capture the degree to
which the adjacent nodes of a given node is connected to each other.

\begin{definition}[Clustering Coefficent]
  Let \(G\) be a graph and \(u \in V(G)\) is a node with degree \(d(u)\), then 
  the clustering coefficient of \(u\) is 
  \[
    C_u = \frac{2 L(u)}{d(u)(d(u) - 1)},
  \]
  where \(L(u)\) is the number of edges in \(E(G)\) connecting two neighbours of 
  \(u\).
\end{definition}

We note that the clustering coefficient \(C_u\) is a number between 0
and 1 with \(C_u = 0\) representing none of the adjacent nodes of \(u\)
is adjacent with each other which \(C_u = 1\) means every adjacent node
of \(u\) is adjacent to every other adjacent node of \(u\).

If the graph in question is \emph{simple}, then we see that the
clustering coefficient of some node \(u\) represents the probability
that two (unique) adjacent nodes of \(u\) are
adjacent\footnote{This is because \(d(u)(d(u) - 1)\) 
is the number of ordered combinations of choosing two nodes while for each 
edge counted by \(L(u)\), it connects two unique parings \((u, v)\) and 
\((v, u)\).}.

\begin{theorem}
  An acyclic graph has zero clustering coefficients everywhere.
\end{theorem}
\proof

This follows directly from that if there exists some node with
clustering coefficient greater than 0, it has at least two neighbouring
nodes which are adjacent to each other. Then we can easily create a path
starting and ending at this node creating a cycle. \# \qed

\begin{corollary}
  A tree has zero clustering coefficients everywhere.
\end{corollary}

By taking the average of the clustering coefficient of every node, we
have the \emph{average clustering coefficient} which represents the
degree of clustering over the whole network.

\begin{definition}[Average Clustering Coefficien]
  Let \(G\) be a graph and for all \(u \in V(G)\) we denoted the clustering 
  coefficient of \(u\) by \(C_u\), then the average clustering coefficient of 
  \(G\) is
  \[\langle C \rangle = \frac{1}{\left| V(G) \right|} \sum_{u \in V(G)} C_u.\]
\end{definition}

\hypertarget{path-distance}{%
\subsection{Path \& Distance}\label{path-distance}}

Given a graph \(G\) and two nodes \(u, v \in V(G)\), we would often like
to quantitatively construct an ordering on these paths. This can be
achieved through creating a distance function on the set of paths.

\begin{definition}[Distance of Unweighed Paths]
  Let \(G\) be a graph with unweighed edges and \(u, v \in V(G)\). Suppose that 
  \(P\) is a path between \(u\) and \(v\) in \(G\), then the distance of \(P\) is
  \[\text{dist} P = \left| P \right| - 1, \] 
  where \(\left| P \right|\) denoted the length of the sequence of nodes induced 
  by \(P\).
\end{definition}

It is not hard to imagine why such an distance function is useful, and
why often we might want to look for the shortest path between to nodes
in a network.

\begin{definition}[Shortest Path]
  Let \(G\) be a graph and \(u, v \in V(G)\). Suppose we denote \(S\) as the set 
  of paths between \(u\) and \(v\). Then, we call \(P \in S\) a shortest path 
  if and only if 
  \[ \text{dist} P = \min_{Q \in S} \text{dist} Q.\]
  We denote this distance by \(d_{u, v}\).
\end{definition}

This definition makes sense as we are work with finite networks, and
thus, the number of paths between two nodes is finite. We note that the
shortest path between two nodes is not unique as many paths can have the
same distance.

We quickly introduce some more definitions. Given a graph \(G\), and a
path \(P\) in \(G\),

\begin{itemize}
  \item the \textit{diameter} of \(G\) is \(\max \{d_{u, v} \mid u, v \in V(G)\}\);
  \item the \textit{average path length} of \(G\) is 
    \(\frac{1}{\left| S \right|} \sum_{d \in S} d\) 
    where \(S = \{d_{u, v} \mid u, v \in V(G)\}\);
  \item \(P\) is an \textit{Eulerian path} if and only if \(E(G) \subseteq P\);
  \item \(P\) is a \textit{Hamiltonian path} if and only if for all 
    \(u \in V(G)\), there exists \(e \in P\), \(u\) is an endpoint of \(e\).
\end{itemize}

\hypertarget{breadth-first-search-bfs-algorithm}{%
\subsubsection{Breadth-First Search (BFS)
Algorithm}\label{breadth-first-search-bfs-algorithm}}

The BFS algorithm is an algorithm to determine the shortest distance
between two nodes of a network.

Let \(G\) be a network and \(u, v \in V(G)\), then

\begin{enumerate}
  \item For each adjacent nodes \(w\) of \(u\), construct a tuple 
    \((w, d(w) = 1)\) and put them in a queue.
  \item While the first element \(w\) of the queue is not equal to \(v\), 
    for each adjacent node \(x\) of \(w\), construct a tuple 
    \((x, d(x) = d(w) + 1)\) and remove \(w\) from the queue.
  \item If the first element of the queue is \(v\), then the distance is simply 
    \(d(v)\). However, if there is no element remain in the queue, we know that 
    the two nodes are not connected.
\end{enumerate}

Note that we can optimize the algorithm by making sure that we do not go
back to previously visited nodes but that's simply an implementation
issue on how one can achieve this.

\hypertarget{analysing-complexity}{%
\subsection{Analysing Complexity}\label{analysing-complexity}}

We recall from year 1 computing the definitions of big-\(O\) and
big-\(\Omega\) notations.

\begin{definition}[Big-\(O\) notation]
  Given a cost function \(C : \mathbb{N} \to \mathbb{R}\), we say 
  \(C(N) = O(f(N))\) for some \(f : \mathbb{N} \to \mathbb{R}\) if and only 
  if there exists some \(N_0 \in \mathbb{R}, r \in \mathbb{R}\) such that 
  for all \(n \ge N_0\), \(C(n) \le rf(n)\).
\end{definition}

\begin{definition}[Big-\(\Omega\) notation]
  Given a cost function \(C : \mathbb{N} \to \mathbb{R}\), we say 
  \(C(N) = \Omega(f(N))\) for some \(f : \mathbb{N} \to \mathbb{R}\) if and only 
  if there exists some \(N_0 \in \mathbb{R}, r \in \mathbb{R}\) such that 
  for all \(n \ge N_0\), \(C(n) \ge rf(n)\).
\end{definition}

We can interpret the big-\(O\) and big-\(\Omega\) notations as providing
some information about the upper and lower bounds of the cost function.

With that, we see that the complexity of this algorithm is
\(O(\left|V\right|)\) since every node will be explored in the worst
case.

\hypertarget{random-networks}{%
\section{Random Networks}\label{random-networks}}

While sometimes we do have a concrete network on hand to work this, this
is not often the case, thus, it is sometimes useful to model the
behaviours of networks with random networks. We will first take a look
at the classical random network -- the \(G(N, p)\) model and move on to
some modern approaches.

\hypertarget{some-properties-of-the-model}{%
\subsection{Some Properties of the
Model}\label{some-properties-of-the-model}}

In the section we will take a look at the \(G(N, p)\) model. As the
number of edges in \(G\) is characterised solely by the probability
\(p\), \(G\) can any number of edges between 0 and
\(\begin{pmatrix} n \\ k \end{pmatrix}\), so it is useful to consider
the probability that \(G\) has \(n\) edges.

We see straight away that the probability that \(G\) has \(n\) edges
equals the sum of the probabilities that \(G\) equals to some graph
\(G_i\) with \(\left| E(G_i) \right| = n\), so
\[\mathbb{P}(\left| E(G) \right| = n) = \sum_{\left| E(G_i) \right|}\mathbb{P}(G = G_i)
  = \sum p^n (1 - p)^{N (N - 1) / 2 - n},\] so it suffices to find the
number of possible graphs with \(N\) nodes and \(n\) edges. This is
simply \(\begin{pmatrix} N (N - 1) / 2 \\ n \end{pmatrix}\), thus, the
probability of \(G\) having \(n\) edges is
\[\begin{pmatrix} N (N - 1) / 2 \\ n \end{pmatrix}p^n (1 - p)^{N (N - 1) / 2 - n}.\]
In fact, this is the binomial distribution with parameters
\(N (N - 1) / 2\) and \(n\).

We might also find the degree distribution of \(G\) to be useful. Let
\(v \in V(G)\), we recall the degree distribution is the probability
that \(v\) is connected to \(k\) other nodes. As there are in total
\(N\) nodes, \(v\) can at most connect to \(N - 1\) other nodes each
with a success possibility of \(p\). Thus, the degree distribution of
\(G\) is
\[p_k = \begin{pmatrix} N - 1 \\ k \end{pmatrix} p^k (1 - p)^{N - 1 - k}, \]
that is, a binomial distribution with parameters \(N - 1\) and \(p\).

We recall that for large \(n\) and small \(p\) such that
\(n \sim p^{-1}\), so, in fact, at the limit, the degree distribution of
a random network can be approximated by a Poisson distribution with the
parameter \(\lambda = np\).

The clustering coefficient of a random Erdos-Renyi graph is given by
\(p\). This is because given \(n\) neighbouring nodes of a node, there
are \(n (n - 1) / 2\) possible edges connecting them each with
probability \(p\), resulting in the local clustering coefficient for a
node being \(p n (n - 1) / 2\). But now, by putting this into the
clustering coefficient formulae, we have
\[C_i = \frac{2 p n (n - 1) / 2}{n (n - 1)} = p,\] as required.

\hypertarget{properties-at-the-limit}{%
\subsection{Properties at the Limit}\label{properties-at-the-limit}}

Before now, we have considered the random graph mostly heuristically
without much rigour. While we shall not change this approach to proving
everything from first principle, we will in this section consider random
graphs from a more probabilistically point of view.

\hypertarget{basic-notions}{%
\subsubsection{Basic Notions}\label{basic-notions}}

Formally, a random Erdos-Renyi graph can be formulated as follows.

\begin{definition}[Random Graph]
  A random graph \(G(N, p)\) is a probability space \((\Omega, \mathcal{F}, \mathbb{P})\) 
  where the sample space \(\Omega\) is all possible simple graphs with \(N\) 
  nodes (that is all \(\left| \Omega \right| = 2^{N'}\)\footnote{From this point 
  forward, we write \(\begin{pmatrix} n \\ 2 \end{pmatrix}\) as \(N'\).} of them), 
  \(\mathcal{F}\) some \(\sigma\)-algebra and \(\mathbb{P} : \mathcal{F} \to [0, 1]\) 
  the probability measure. 
\end{definition}

\begin{remark}
  We will begin by looking at the probability measure \(\mathbb{P}\) such that 
  for all \(G \in \Omega\), \(\mathbb{P}(G) = p^m (1 - p)^{N - m}\), where 
  \(m = \left| E(G) \right|\).
\end{remark}

We will also be using small \(o\) notation in which we refer a function
\(f\) as \(f(n) = o(g(n))\) if \(f(n) / g(n) \to 0\) as
\(n \to \infty\).

A statement that is commonplace within network science is: ``For
\(p > 0\), a random graph in \(G(N, p)\) is connected.'' This means
mathematically,
\(\mathbb{P}(G \in G(N, p), G \text{ is connected}) \to 1\) as
\(N \to \infty\). In short, one often says: ``for \(p > 0\), \(G\) is
connected w.h.p\footnote{With high probability.}''.

Consider a random Erdos-Renyi graph \(G(N, p)\). Suppose, naively, we
write \(k_i\) for the degree of the \(i\)-th node, then, it might make
sense to say that, the average \(k_i\) tends to the expected value of
the nodes \(\langle k \rangle = p(N - 1)\), i.e.
\[\lim_{N \to \infty} \frac{1}{N} \sum_{i = 0}^N k_i = \langle k \rangle.\]
But upon further inspection, this equation is less clear. As \(G(N, p)\)
is simply a probability space, it doesn't make sense to take the
\(i\)-th node, so \(k_i\) is in fact a function on the sample space
\(\Omega\) to \(\mathbb{R}\). Furthermore, this sequence of functions
does not converge to some number, that is, for all \(N \in \mathbb{N}\),
we can find a graph \(G_N\) with \(0\) edges so the limit is also \(0\)
for this sequence. Indeed, what we would actually like to say is
\[\lim_{N \to \infty} \mathbb{P} \left( \left| \frac{1}{N}\sum_{i = 0}^N k_i - 
  \langle k \rangle \right| \ge 0 \right) = 0, \] for all
\(\epsilon > 0\). We often write this as
\[\lim_{N \to \infty} \frac{1}{N} \sum_{i = 0}^N k_i = \langle k \rangle 
  \hspace{2mm} \text{w.h.p}.\] This is true since
\(k_i \sim B(N - 1, p)\) identically with mean \(\langle k \rangle\).
So, using a version of the
LLN\footnote{Be careful as they are not independent 
since if \(k_0 = N - 1\), then \(k_i \ge 1\) for all other \(i\).}, the
result follows.

\hypertarget{existence-of-large-clusters}{%
\subsubsection{Existence of Large
Clusters}\label{existence-of-large-clusters}}

We will now prove some theorems that will provide us with some insights
into when will large clusters arise.

\begin{theorem}
  \(\langle k \rangle > 1 \iff\) there exists w.h.p. a connected component which 
  contains a proportion \(S > 0\) of the total number of nodes, where \(S\) 
  satisfies \(S = 1 - e^{-\langle k \rangle S}\).
\end{theorem}

\begin{theorem}
  Given \(p > 0\), the diameter of \(G \in G(N, p)\) is w.h.p. is at most two, 
  so \(G\) is connected w.h.p.
\end{theorem}
\proof

Let \(X_N(G)\) be the number of pairs \((i, j)\) in \(G\) such that
\(i, j\) has no common neighbours. So, by considering the probability
that \(i, j\) do not have \(k\) as a common neighbour is
\[ p(1 - p) + (1 - p)p + (1- p)(1 - p) = 1 - p^2,\] (where we simply add
up the probabilities of all valid combinations), we have the probability
that \(i ,j\) has no common neighbours is \((1 - p^2)^{N - 2}\). So,
\[E(X_N) = N'(1 - p^2)^{N - 2} \to 0,\] as \(N \to \infty\). And, hence,
by Markov's inequality, \(\mathbb{P}(X_N \ge 1) \le E(X_N) \to 0\) so
\(\mathbb{P}(X_N = 0) \to 1\) as \(N \to \infty\). \qed

\begin{theorem}
  If \(p(N) = o(N) / N\) then, w.h.p. \(G \in G(N, p)\) has no triangles.
\end{theorem}
\proof

Let \(T_N\) be the number of triplets \((i, j ,k)\) such that they are
pairwise connected. Of course, then probability of a randomly chosen
triplet being a triangle is \(p^3\), so we have
\[E(T_N) = \begin{pmatrix} n \\ 3 \end{pmatrix} p^3 \to 0,\] as \(p\)
tends to zero faster than \(N\). Hence, again by Markov's inequality we
have \(\mathbb{P}(T_N \ge 1) \to 0\), resulting in
\(\mathbb{P}(T_N = 0) \to 1\). \qed

\hypertarget{other-models-of-random-networks}{%
\subsection{Other Models of Random
Networks}\label{other-models-of-random-networks}}

By comparing the \(G(N, p)\) model to to real networks, we find that
there is a big difference between the prediction and the actual
networks. The could be due to numerous reasons such as the fact that the
probability of the existence of a particular link in the \(G(N, p)\)
model is independent from the number of links a node already have. In
this section we shall take a look at some more modern models of random
graphs with (perhaps) better predictions in certain situations.

We will first take a look at the \emph{configuration model}.

\begin{definition}[Configuration Model]
  The configuration model with \(n\) nodes a random network with the
  sequence \(S = \{x_1 \cdots x_n\} \subseteq (\mathbb{N}^*)^n\) such that the 
  \(i\)-th node has degree \(x_i\).
\end{definition}

\begin{remark}
  We see that \(\sum S\) must be even as each edge has two endpoints.
\end{remark}

While the configuration model have some benefits in that it better model
a network given its degree distribution, and that it retains the small
world property, we find that it is still lacking. Not only is does the
configuration model require we to provide it with the degree
distribution, it has a fixed number of nodes, so it cannot model the
growth of a network over time. We attempts to achieve this with the
\emph{Barabasi-Albert} model.

Consider an evolving network in which the number of nodes increases over
time. By considering real world examples, we notices some pattern
regarding how new links are formed -- the nodes with higher degree form
new links more easily. This property is called \emph{preferential
attachment} and we would like to model this with our random graph.

To achieve this, at each iteration, we assign each nodes some value
proportional to their degree to indicate how likely they are to form new
links. The easiest way to do this is linear preferential attachment.

\begin{definition}[Linear Preferential Attachment]
  The probability of a new link attaching to an existing node following 
  linear preferential attachment is given by 
  \[\pi_i (t + 1, G(t)) = \frac{k_i}{\sum_{i = 1}^{N(t)}k_i(t)},\]
  where \(\pi_i(t + 1, G(t))\) is the probability of a link connecting to node 
  \(i\) in graph \(G(t)\) with \(t = 1, 2, \cdots\).
\end{definition}

So, at each time \(t\), we add in a new node and a new edge with
probabilities according to the linear preferential attachment. We call
this an element of the family of the Barabasi-Albert models.

Without needing to consider the preferential attachment model, we see
that at time \(t \in \mathbb{N}\), there are \((t + 1)!\) number of
possible graphs.

We would also like to consider the expected properties of this model. By
definition, given some property of a graph at time \(t\), \(f(t)\), we
can write the expected value of \(f\) as
\[\langle f(t) \rangle = \sum_{i = 1}^{(t + 1)}\mathbb{P}(G_i(t))f(t \mid G_i(t)).\]
However, this is very vague and does not provide us with much useful
information, so let us look at some concrete examples.

Let us take a look at the degree distribution of a Barabasi-Albert
model. Consider the expected number of nodes of degree \(k\) at time
\(t + 1\), \[
\langle N_k(t + 1) \rangle = \sum_{i = 1}^{(t + 1)!} P(G_i(t)) N_k(t + 1 \mid G_i(t)).
\] By considering each cases we find, in general, \[
N(t + 1)p_k(t + 1) = N(t)p_k(t) - \frac{p_k(t)N(t)k}{2L(t)} + \frac{p_k(t)N(t)(k - 1)}{2L(t)},
\] and for \(k = 1\), \[
N(t + 1)p_1(t + 1) = N(t)p_1(t) - \frac{p_1(t)N(t)}{2L(t)} + 1.
\] Thus, by taking the limit of the two equations as \(t \to \infty\),
we have \[p_{1, \infty} = \frac{2}{3};\] and
\[p_{k, \infty} = \frac{k - 1}{k + 2}p_{k - 1, \infty},\] where
\(p_{k, \infty}\) is the limit of the probability that a node has degree
\(k\) as \(t \to \infty\). By rearranging, we have
\(p_{k + 1, \infty} = \frac{k}{k + 3}p_{k, \infty}\), and by induction,
we find \[p_{k, \infty} = \frac{4}{(k + 2)(k + 1)k} \sim k^{-3}.\] Thus,
we can conclude that the Barabasi-Albert model has the scale free
property.

Lastly, let us look at how the degree of a node evolve over time.
Luckily, this is very simple. If we let \(k_i(t)\) be the random
variable describing the degree of node \(i\), we see straight away the
relation, \[k_i(t + 1) = k_i(t) + \delta_i(t + 1),\] where
\(\delta_i(t + 1)\) is the Bernoulli random variable with parameter
given by \(k_i(t)\), that is \(\pi_i(t + 1)\). So, (assuming linear
preferential attachment), we have
\[\langle k_i(t + 1) \rangle = \langle k_i(t) \rangle\left(1 + \frac{1}{2 + 2t}\right).\]
So, by induction, \[\langle k_i(t_0 + j) \rangle = 
  \prod_{n = 0}^{t_0 + j - 1}\left( 1 + \frac{1}{2 + 2(t_0 + n)} \right)
  \sim \sqrt{\frac{t_0 + 1 + j}{t_0 + 1}},\] where \(t_0\) is defined to
be the smallest \(t\) such that \(k_i(t = t_0) = 1\). We define such an
\(t_0\) since for all \(t < t_0\), \(i\)-th node does not exist.

So far, we have looked at an element of the family of the
Barabasi-Albert model, that is adding one edge and one node at each
iteration. It is not hard to see that this will result in zero
clustering everywhere. Suppose instead, we add one node and \(m\) new
edges every iteration. Then, the degree distribution is
\[p_k \approx \frac{2m(m + 1)}{(k + 2)(k + 1)k},\] and the clustering is
\[\langle C \rangle \sim \frac{(\log N)^2}{N}.\]

\begin{remark}
  We see that the degree distribution is consistent with our previous result 
  where \(m = 1\).
\end{remark}

With the general Barabasi-Albert model, we observe an ``ultra
small-world'' behaviour, that is the distance grows slower than
\(\log N\). Furthermore, by once again observing our degree
distribution, we see that they all follow a \(k^{-3}\) trend. However,
this is not necessary the case for real networks. By observing real
networks, it was found that the linear preferential attachment model can
be generalised such that \[\pi_i = c(k_i^\alpha + \beta),\] where
\(\alpha, \beta\) are parameters and \(c\) is the normalising constant.

With \(\alpha = 1\),
\[p_k \sim k^{- \left(3 + \frac{\beta}{m}\right)}.\] With \(\beta = 0\)
and \(\alpha < 1\), we find
\[p_k \sim k^{-\alpha}\exp\left(\frac{-2 \mu(\alpha)}
{\langle k\rangle (1 - \alpha)k^{1 - \alpha}}\right).\] Lastly, with
\(\alpha > 1\), we find a unrealistic hub structure is generated.

\hypertarget{spreading-processes}{%
\section{Spreading Processes}\label{spreading-processes}}

\hypertarget{diffusion-in-the-first-dimension}{%
\subsection{Diffusion in the First
Dimension}\label{diffusion-in-the-first-dimension}}

We will first consider random walks and diffusion in the first
dimension.

\begin{definition}[Random Walk in 1-D]
  A random walk in the first dimension is a real valued function 
  \(X_i : \mathbb{R} \to \mathbb{R}\) such that 
  \[X_i(t + \delta) = X_i(t) + l_i(t),\]
  where \(l_i\) is the random variable such that 
  \(\mathbb{P}(l_i(t) = h) = \mathbb{P}(l_i(t) = -h) = 1 / 2\) for 
  some \(t, \delta, h \in \mathbb{R}\).
\end{definition}

Given some particles following some a particular random walk, we would
like to find the expected number of particles as some position \(x_0\)
at time \(t\). Consider that, during some time \(\Delta t\), a particle
will move some distance \(\pm \Delta x\) that is, the flux at
\(x_0 \pm \Delta x/ 2\). We see that
\[\langle j(x_0 + \Delta x / 2, t) \rangle = \frac{1}{2}(N(x_0, t) - N(x_0 + \Delta x, t)),\]
\[\langle j(x_0 - \Delta x / 2, t) \rangle = \frac{1}{2}(N(x_0 + \Delta x, t) - N(x_0, t)),\]
where \(j(x, t)\) denotes the flux at position \(x\) time \(t\) (note
that we have define movement to the right to be positive). These flux'
tells us how the expected number of particles changes over a particular
time step with
\[\langle N(x_0, t + \Delta t) \rangle = \langle N(x_0, t) \rangle + 
  \langle j(x_0 - \Delta x / 2, t) \rangle -
  \langle j(x_0 + \Delta x / 2, t) \rangle,\] and so,
\[\langle N(x_0, t + \Delta t) \rangle - \langle N(x_0, t) \rangle = 
  \frac{1}{2}(\langle N(x_0, t + \Delta t) \rangle - 
   2\langle N(x_0 + \Delta x / 2, t) \rangle) +
  \langle N(x_0 - \Delta x / 2, t) \rangle.\] However, we are not
interested in this values as it is dependent on a particular pair of
\(\Delta x\) and \(\Delta t\), and so, by defining
\(n(x, t) := \langle N(x, t) \rangle / \Delta x\), we can take the limit
as \(\Delta x, \Delta t\) tends to 0, resulting in,
\begin{equation}\label{diffusion1d}
\frac{\partial{n}}{\partial{t}} = \alpha \frac{\partial^2{n}}{\partial{x}^2}.  
\end{equation} To obtain this, we assumed
\(\Delta x \sim 2\alpha \sqrt{\Delta t}\) which turns out to be
empirically supported. Equation \ref{diffusion1d} is refereed to as the
one dimensional diffusion equation and can be easily extended in to
higher dimensions to obtain \begin{equation}
  \frac{\partial{n}}{\partial{t}} = \alpha \nabla ^2 n.
\end{equation}

\hypertarget{diffusion-in-networks}{%
\subsection{Diffusion in Networks}\label{diffusion-in-networks}}

With networks, we no longer have spatial derivatives, however, we can
employ a similar idea. Instead of just considering the two fluxes, we
now consider the flux' of all edges connected to some node.

Suppose we denote \(n_i(t)\) to be the number of particles at node \(i\)
at time \(t\) and \(J_{i,j}(t)\) to be the flux per unit time
\(\Delta t\) from \(j\) to \(i\) at time \(t\), we see that
\[J_{i, j}(t) = -\alpha(n_i(t) - n_j(t)),\] and so, following the 1
dimensional case,
\[n_i(t + \Delta t) - n_i(t) = \Delta t \sum_{j \in N_i} J_{i, j}(t),\]
where \(N_i\) is the set of neighbours of \(i\). Thus, by taking the
limit as \(\Delta t \to 0\), we have, \begin{equation}
  \frac{dn_i}{dt} = \alpha \sum_{j = 1}^N A_{i, j} J_{i, j} = 
  \alpha \sum_{j = 1}^N A_{i, j}(n_j - n_i)
\end{equation} where \(A\) is the adjacency matrix of the network. By
recognizing that
\(\sum_{j = 1}^N A_{i, j}n_i = n_i \sum_{j = 1}^N A_{i, j}\), and that
\(\sum_{j = 1}^N A_{i, j} = k_i\), the degree of \(i\), we see
\begin{equation}
  \frac{dn_i}{dt} = \alpha (\sum_{j = 1}^N A_{i, j} n_j - n_i k_i).
\end{equation} Lastly, by defining
\(Q := \text{diag}(k_1, \cdots, k_n)\) the degree matrix, and by writing
\(c = (n_1, \cdots, n_N)^T\), we find, \begin{equation}
  \frac{d\mathbf{n}}{dt} = \alpha (A - Q)\mathbf{n}.
\end{equation} Interestingly, we recall from last year that we have seen
\(A - Q = -L\) where \(L\) is the graph Laplacian, so we can write this
equation as \(\frac{d\mathbf{n}}{dt} = -\alpha L \mathbf{n}\) which is
easily solvable using the methods from last year's calculus.

\end{document}
