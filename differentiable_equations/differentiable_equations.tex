% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Differentiable Equations},
  pdfauthor={Kexing Ying},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=red,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin = 1.5in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{lipsum}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{physics}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\Arg}{\mathop{\mathrm{Arg}}}
\newcommand{\hess}{\mathop{\mathrm{Hess}}}

\title{Differentiable Equations}
\author{Kexing Ying}
\date{January 11, 2021}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\newpage

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

While we have seen differential equations in year one, we have mostly
focused on the different methods of solving specific differential
equations. This cannot be expected for general differential equations
and in this year, we will focus on existence and uniqueness of solutions
to differential equations and develop qualitative tools to help us
understand these solutions.

We recall that an algebraic equation is an equation of the form
\(f(x) = 0\) while a differential equation is an equation of the form
\(\dot x = f(x)\) for some function \(f : \mathbb{R} \to \mathbb{R}\).
That is, an algebraic equation has real numbers as solutions while an
differential equation has functions as its solution.

As an example, let us consider the simple differentiable equation
\begin{equation}\label{simple}
  \dot x = a x,
\end{equation} for some \(a \in \mathbb{R}\). Then, a function
\(\lambda : I \to \mathbb{R}\) solves \ref{simple} if
\(\dot \lambda = a \lambda\) for all \(t \in I\) where
\(I \subseteq \mathbb{R}\) is a interval. These types of differentiable
equations occurs often in relation in growth and decay and one can
easily see that the family of functions
\[\lambda_b : \mathbb{R} \to \mathbb{R} = t \mapsto b e^{at}, \hspace{2mm} b \in \mathbb{R},\]
are solutions to \ref{simple}. Of course, we know this already, so an
more interesting question would be whether or not this family contains
all the solutions to \ref{simple}. It turns out to be true, and to show
this we will assume \(\mu : I \to \mathbb{R}\) is a solution to
\(\dot x = a x\). Then,
\[\dv{t}\left(\mu e^{-at}\right) = \dot \mu e^{-at} - a\mu e^{-at} = 0,\]
since \(\dot \mu = a \mu\) and so, \(\mu e^{-at}\) is constant,
i.e.~there exists \(b \in \mathbb{R}\) such that \(\mu e^{-at} = b\) and
hence, \[\mu = b e^{at}.\] This demonstrates that all solutions to
\ref{simple} are members of the aforementioned solution family and
hence, we have found \textbf{all} of the solutions to \ref{simple}.

With the above example, we see that rather than working with solutions
that are in finite-dimensional vector spaces, our solution are in
function spaces which are typically infinite-dimensional. This is
studied in more detail in the next year's \emph{functional analysis}
course, and in general, infinite-dimensional spaces are more difficult
to grasp. However, for the vast majority of materials in this course, a
finite-dimensional thinking suffices while we will also cover some
material from functional analysis to understand the differentiable
equations as well.

\hypertarget{ordinary-differential-equations-and-initial-value-problems}{%
\subsection{Ordinary Differential Equations and Initial Value
Problems}\label{ordinary-differential-equations-and-initial-value-problems}}

There are two types of differential equations -- \emph{autonomous
differential equations} and \emph{nonautonomous differentiable
equations}. Autonomous differential equations are differentiable
equations of the form \(\dot x = f(x)\) such as equation \ref{simple}
while nonautonomous differential equations are equations of the form
\(\dot x = f(t, x)\).

We note that this does not cover higher-order differential equations,
but from last year, we recall that one may reduce a higher-order
differential equations into a first-order differential equation in
vector form and thus, the theories we develop within this course will
also apply to higher-order differential equations.

\begin{definition}[Ordinary Differential Equation]
  Let \(d \in \mathbb{N}\), \(D \subseteq \mathbb{R} \times \mathbb{R}^d\) be 
  open, and a function \(f \to D \to \mathbb{R}^d\). Then, an equation of the form 
  \[\dot x = f(t, x)\]
  is called a \(d\)-dimensional (first-order) ordinary differential equation. 

  A differentiable function \(\lambda : I \to \mathbb{R}^d\) on some interval 
  \(I \subseteq \mathbb{R}\) is called a solution of the differential equation 
  if and only if for all \((t, \lambda(t) \in D\), if \(t \in I\) then, 
  \[\dot \lambda(t) = f(t, \lambda(t)).\]
  We say that an ordinary differential equation is autonomous if \(f\) is 
  independent of \(t\) and nonautonomous otherwise.
\end{definition}

We will only consider ordinary differential equations (ODE) in this
course while partial differential equations, that is differential
equations which solutions are functions which depends on multiple
variables are covered in the second year course \textbf{Partial
Differential Equations in Action}.

\begin{prop}[Constant solutions to autonomous differential equations]\label{const_sol}
  Let \(D \subseteq \mathbb{R}^d\) be an open set and \(f : D \to \mathbb{R}^d\) 
  be a function where \(d \in \mathbb{N}\). Then, there exists a constant 
  solution \(\lambda : \mathbb{R} \to \mathbb{R}^d : x \mapsto a\) to the 
  autonomous differential
  \[\dot x = f(x)\]
  for some \(a \in \mathbb{R}^d\) if and only if \(f(a) = 0\).
\end{prop}
\proof

(\(\implies\)) Suppose that
\(\lambda : I \to \mathbb{R}^d : x \mapsto a\) is a solution the
\(\dot x = f(x)\). Then
\[0 = \dot \lambda (t) = f(\lambda (t)) = f(a).\] (\(\impliedby\))
Suppose there exists some \(a \in \mathbb{R}^d\) such that \(f(a) = 0\),
then verifying, we find
\(\lambda : \mathbb{R} \to \mathbb{R}^d : x \mapsto a\) is a solution to
the differential equation. \qed

This proposition allows us to find solutions to many autonomous ODEs as,
indeed, if \(f : D \to \mathbb{R}^d\) has a root \(a \in \mathbb{R}^d\),
the above proposition guarantees that
\(\lambda : \mathbb{R} \to \mathbb{R}^d : x \mapsto a\) is a solution to
\(\dot x = f(x)\).

\begin{definition}[Initial Value Problem]
  Let \(d \in \mathbb{N}\), \(D \subseteq \mathbb{R} \times \mathbb{R}^d\) be an 
  open set, and \(f : D \to \mathbb{R}^d\) be a function. The system of equations 
  from combining the differential equation 
  \[\dot x = f(t, x),\]
  with the initial condition
  \[x(t_0) = x_0\] 
  where \((t_0, x_0) \in D\) is called an initial value problem.

  A solution to the above initial value problem is a function 
  \(\lambda : I \to \mathbb{R}^d\) that is a solution to the differential equation 
  \(\dot x = f(t, x)\) and \(\lambda(t_0) = x_0\).
\end{definition}

While, previously, we have seen a differential equation which always has
a solution. This, however, is not always the case.

\begin{example}
  Consider the differential equation \(\dot x = f(x)\), where 
  \[f(x) = \begin{cases}
    1, & x < 0;\\
    -1, & x \ge 0,
  \end{cases}\]
  with the boundary condition \(x(0) = 0\). As we will see on the problem sheet, 
  this differential equation indeed does not have a solution.
\end{example}

\begin{example}
  Consider the initial value problem \(\dot x = f(x) = \sqrt{|x|}\) with the 
  boundary condition \(x(0) = 0\).

  Since \(f(0) = 0\) we have \(x(t) = 0\) is a constant solution by 
  proposition \ref{const_sol}. Furthermore, by consider the function 
  \[\lambda_b(t) = 
  \begin{cases}
    0, & t \le b;\\
    \frac{1}{4}(t - b)^2, & t > b,
  \end{cases}\]
  we find \(\lambda_b\) to also be a solution for any \(b \in \mathbb{R}^+_0\).
  However, as for all \(t \le b\), \(\lambda_b(t) = 0\), we see that \(x\) is 
  not unique given values of \(x\) at \(t\).
\end{example}

Before moving on, let us quickly recall the \emph{separation of
variables} procedure for solving differential equations. Suppose we are
to solve a differential equation of the form, \[\dot x = g(t) h(x),\]
with the boundary condition \(x(t_0) = x_0\) where
\(g : I \to \mathbb{R}\) and \(h : J \to \mathbb{R}\) are continuous
functions. Then, we have
\[\int_{x_0} ^x\frac{\dd y}{h(y)} = \int_{t_0}^t g(s) \dd s.\] Thus, by
evaluating the integral, we can express \(x\) in \(t\). We see that this
procedure does indeed provide us with a correct solution by simply
applying FTC on both sides of the equation.

\begin{example}
  Consider the initial value problem \(\dot x = t x^2\) with the boundary condition 
  \(x(t_0) = x_0\) where \(x_0 \neq 0\). By the separation of variables, we find 
  \[x = \frac{2x_0}{2 + x_0(t_0^2 - t^2)}.\]
  However, we see that for this solution, \(x\) does not necessarily exist for 
  all \(t\). Indeed, if \(t_0 = 0, x_0 = 1\), we find 
  \[x = \frac{2}{2 - t^2},\]
  which does not have a solution for \(t = \pm\sqrt{2}\), and so, the solution 
  does not exist globally.
\end{example}

\hypertarget{visualisations}{%
\subsection{Visualisations}\label{visualisations}}

Visualisations are very important for differential equations as it
provides us a mental image of how to think about differential equations.
In general, there are two main methods to visualise differential
equations:

\begin{itemize}
  \item nonautonomous differential equations \(\dot x = f(t, x)\) via the 
    \textit{solution portrait} in the extended phase space;
  \item autonomous differential equations \(\dot x = f(x)\) via the 
    \textit{phase portrait} in the phase space.
\end{itemize}

Suppose we have the nonautonomous differential equations
\(\dot x = f(t, x)\) where
\(f : D \subseteq \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d\),
have a solution \(\lambda : I \to \mathbb{R}^d\), i.e.~for all
\(t \in I\), \(\dot \lambda(t) = f(t, \lambda(t))\). Thus, we see that
the vector \((1, f(t_0, \lambda(t_0))\) for some \(t_0 \in I\) is
tangential to the solution which passes through \((t_0, \lambda(t_0))\).
This can be done for all \(p \in \mathbb{R} \times \mathbb{R}^d\) and
so, by drawing these vectors at a sufficient number of points, we can
have a mental image of what the solution looks like. A plot of these
vectors is refereed to as a \textbf{vector field}.

A \textbf{solution portrait} is given by a visualisation of several
solution curves in the \((t, x)\)-space, the so called \emph{extended
phase space}. This is called as such since the \(x\)-space is normally
refereed as the phase space, and so, we are extending it by the time
axis.

On the other hand, suppose we have the autonomous differential equation
\(\dot x = f(x)\) where
\(f : D \subseteq \mathbb{R}^d \to \mathbb{R}^d\) has a solution
\(\lambda : I \to \mathbb{R}^d\). By considering that \(f\) is
independent of \(t\), we see that translating \(\lambda\) along \(t\) is
also a valid solution. Indeed, at \(x = \lambda(t_0)\), if \(\lambda'\)
is another solution such that \(\lambda'(t_1) = \lambda(t_0)\), then
\(\dot \lambda'(t_1) = \dot \lambda(t_0)\). This property is refereed to
as \emph{translation invariance} and all solutions of autonomous
differential equations are translation invariant.

\begin{prop}
  let \(\dot x = f(x)\) be an autonomous differential equation. Then, if 
  \(\lambda : I \to \mathbb{R}^d\) is a solution to this differential equation, 
  so is
  \[\mu : \bar{I} \to \mathbb{R}^d : t \mapsto \lambda(t + \tau),\]
  where \(\tau \in \mathbb{R}\), \(\bar{I} := I + \tau\).
\end{prop}
\proof

Follows straight away by chain rule. \qed

\begin{example}
  Consider the harmonic oscillator \(\ddot x = -x\). By converting it into 
  a first order differential equation, we have 
  \[\dot{\mathbf{x}} = 
  \begin{bmatrix}
    \dot x\\ \dot y
  \end{bmatrix} = 
  \begin{bmatrix}
    0 & 1 \\
    -1 & 0
  \end{bmatrix}\mathbf{x}.\]
  By solving the system, we find 
  \[\lambda(t) = \begin{bmatrix}
    \cos(t)\sin(t)\\ -\sin(t)\cos(t)
  \end{bmatrix}
  \begin{bmatrix}
    x_0\\ y_0
  \end{bmatrix}\]
  is a solution. Indeed, this describes a oscillatory motion in the phase space 
  where the position is circular.
\end{example}

With consideration with the example above, we see that we may project a
solution portrait \((t, x) \mapsto (1, f(x))\) onto \(x \mapsto f(x)\)
resulting in a \textbf{phase portrait}.

\hypertarget{existence-and-uniqueness}{%
\section{Existence and Uniqueness}\label{existence-and-uniqueness}}

As we have seen, differential equations need not have unique solutions
given an initial value. Indeed, we have also seen that it is not
guaranteed to have a solution at all. We will in this chapter resolve
the question on whether or not a solution exists by presenting a theory
that guarantees existence and uniqueness for solutions to initial value
problems.

\hypertarget{picard-iterates}{%
\subsection{Picard iterates}\label{picard-iterates}}

We observe that often times, to solve an differential equation, we need
to reformulate it as an integral equation.

\begin{prop}\label{alt_repr}
  Consider the initial value problem 
  \[\dot x = f(t, x); \hspace{2mm} x(t_0) = x_0,\]
  where \(f : D \subseteq \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d\) is 
  continuous. Let \(\lambda : I \to \mathbb{R}^d\), \(\lambda\) solves the IVP 
  if and only if \(\lambda\) is continuous and solves the integral 
  equation\footnote{We recall that the integral of a vector valued function is 
  simply the integral of the components.} 
  \[\lambda(t) = x_0 + \int_{t_0}^t f(s, \lambda(s)) \dd s.\]
\end{prop}
\proof

Follows from FTC. \qed

However, as \(f\) depends on \(\lambda\), we in general cannot compute
the integral on the right hand side. Nonetheless, this brings us closer
to the formulation of the Picard iterates.

\begin{prop}
  Let \(f\) be a continuous function. Then by defining \(a_0\) for some value and 
  \(a_{n + 1} = f(a_n)\), if \((a_n)_{n = 1}^\infty\) converges to some value 
  \(a\), then \(a = f(a)\).
\end{prop}
\proof

We see that if \((a_n)_{n = 1}^\infty\) converges to some value \(a\),
\(f(a_n) \to f(a)\) by sequential continuity. However, as
\(\lim_{n \to \infty} f(a_n) = \lim_{n \to \infty} a_{n + 1} = a\), by
uniqueness of limits on Hausdorff spaces, we have \(a = f(a)\). \qed

With this proposition in mind, we may apply a similar iteration on the
integral equation resulting in the Picard iterates.

\begin{definition}[Picard Iterates]
  Given an IVP, the Picard iterates is the sequence of functions 
  \((\lambda_n : J \to \mathbb{R}^d)_{n = 1}^\infty\) be defined such that 
  \(\lambda_0(t) = x_0\), and 
  \[\lambda_{n + 1}(t) = x_0 + \int_{t_0}^t f(s, \lambda_n(s)) \dd s.\]
\end{definition}

With connotation to the motivating proposition, we hope that
\((\lambda_n)\) converges to the solution \(\lambda_\infty\) in some
notion. As we shall see, the Picard iterates needs to converge to
\(\lambda_\infty\) uniformly in order for \(\lambda_\infty\) to be a
solution.

\begin{prop}
  Given an IVP, if the Picard iterates \((\lambda_n)_{n = 0}^\infty\) converges 
  uniformly to \(\lambda_\infty\), then \(\lambda_\infty\) is a solution to the 
  IVP.
\end{prop}
\proof

Consider the following chain of equalities, \[\begin{split}
    \lambda_\infty(t) = \lim_{n \to \infty} \lambda_{n + 1}(t) & = 
    \lim_{n \to \infty} x_0 + \int_{t_0}^t f(s, \lambda_n(s)) \dd s\\ &= 
    x_0 + \int_{t_0}^t f(s, \lim_{n \to \infty} \lambda_n(s)) \dd s = 
    x_0 + \int_{t_0}^t f(s, \lambda_\infty(s)) \dd s,
  \end{split}\] where the third equality is true as
\(\lambda_n \to \lambda_\infty\) uniformly (see first year analysis for
proof). Thus, by proposition \ref{alt_repr}, \(\lambda_\infty\) solves
the IVP. \qed

With this proposition in mind, it is \emph{sometimes} possible to show
that a particular Picard iterates converges uniformly towards some
function resulting in a solution to the corresponding IVP.

\hypertarget{lipschitz-continuity}{%
\subsection{Lipschitz Continuity}\label{lipschitz-continuity}}

We recall from first year analysis the definition of Lipschitz
continuity. We are interested in Lipschitz continuity since it is very
helpful when showing the existence and uniqueness of solutions of IVPs.
Indeed, by reformulating the Picard iterates as an operator \(P\) on the
Banach space \(\mathcal{C}^\circ(J, \mathbb{R}^d)\), we find that \(P\)
is a contraction (i.e.~\(P\) has Lipschitz constant \(< 1\)) if \(f\)
satisfies certain Lipschitz condition, and hence, by Banach's fixed
point theorem, has a fixed point.

We recall that a normed vector space is a vector space \(V\) equipped
with a norm \(\| \cdot \|\) such that the norm satisfies positive
definiteness, absolute homogeneity and the triangle inequality. Indeed,
we recall the chain of induced structure: Inner product space
\(\implies\) Normed vector space \(\implies\) (Metric space \(\implies\)
Topological space) \(\vee\) Vector space.

\begin{definition}[Lipschitz Continuity]
  Let \((V, \| \cdot \|_V)\), \((W, \| \cdot \|_W)\) be two normed vector spaces 
  and suppose \(X \subset V\) and \(Y \subseteq W\), then \(f : X \to Y\) is 
  Lipschitz continuous if there exists some \(K > 0\) (a Lipschitz constant) 
  such that for all \(x, y \in X\),
  \[\| f(x) - f(y) \|_W \le K \|x - y\|_V.\]
\end{definition}

We recall the sufficient conditions for which a function is Lipschitz
continuous.

\begin{prop}
  Let \(f : I \to \mathbb{R}\) be differentiable with bounded derivative on some 
  interval \(I\), then \(f\) is Lipschitz continuous.
\end{prop}
\proof

By MVT, for all \(x, y \in I\), there exists some \(c \in (x, y)\) such
that \[f(x) - f(y) = f'(c) (x - y).\] So by taking absolute value on
both sides, and by choosing the Lipschitz constant as the supremum of
\(|f'|\) we have found some \(K := \sup_{c \in I} \|f'(c)\|\), so
\[|f(x) - f(y)| \le K \|x - y\|.\] \qed

We see that, if \(f\) is continuously differentiable, \(f'\) is
continuous on the compact set \(I\), and so \(f'\) is uniformly
continuous, and so is bounded. Thus, if \(f : I \to \mathbb{R}\) is
continuously differentiable, then it is Lipschitz continuous.

For higher dimensions, we require the mean value inequality for higher
dimensions and so an similar result is achieved. To look at the mean
value theorem for higher dimensions, let us first introduce the operator
norm for linear maps.

\begin{definition}[Operator Norm]
  Let \(f : \mathbb{R}^n \to \mathbb{R}^m\) be a linear map, then the operator 
  norm of \(f\) is 
  \[\| f\|_\text{op} := \sup_{x \in \mathbb{R}^n \setminus \{0\}} \frac{\| f(x) \|}{\|x\|}
  \equiv \sup_{x \in \mathbb{R}^n, \|x\| = 1} \|f(x)\|,\]
  where \(\| \cdot \|\) are the Euclidean norms.
\end{definition}

As we saw in linear algebra last term, the operator norm form norms on
the space of linear maps as the name suggests. By considering that the
set \(S_1 := \{x \in \mathbb{R}^n, \|x\| = 1\}\) is closed and bounded,
and hence compact by Heine-Borel, as linear maps are continuous, \(f\)
attains its maximum as on \(S_1\), so we may in fact write the operator
norm as
\[\| f\|_\text{op} := \max_{x \in \mathbb{R}^n, \|x\| = 1} \|f(x)\| < \infty.\]

\begin{prop}
  If \(f : \mathbb{R}^n \to \mathbb{R}^m\) is a linear map and \(x \in \mathbb{R}^n\),
  then, 
  \[\|f(x)\|  \le \|f\|_\text{op} \|x\|.\]
\end{prop}
\proof

If \(x = 0\) then both sides are zero so suppose otherwise. If
\(x \neq 0\), then \(\|x\| > 0\) and so it suffices to show
\[\frac{\|f(x)\|}{\|x\|} \le \|f\|_\text{op}.\] But, this follows
directly from the definition so we are done. \qed

\begin{prop}
  If \(f : \mathbb{R}^n \to \mathbb{R}^m\) be a linear map, then \(f\) is Lipschitz 
  continuous with the Lipschitz constant \(\|f\|_\text{op}\).
\end{prop}
\proof

This follows as, for all \(x, y \in \mathbb{R}^n\)
\[\|f(x) - f(y)\| = \| f(x - y) \| \le \|f\|_\text{op} \|x - y\|.\] \qed

\begin{definition}[Closed Line Segment]
  For convenience, for all \(x, y \in \mathbb{R}^n\), we define the closed line 
  segment 
  \[[x, y] := \{\alpha x + (1 - \alpha)y \mid \alpha \in [0, 1]\}.\]
\end{definition}

\begin{theorem}[Mean Value Inequality]
  Let \(D \subseteq \mathbb{R}^n\) be open and \(f : D \to \mathbb{R}^m\) be a 
  function which is continuously differentiable. Then, for all \(x, y \in D\) 
  with \([x, y] \subseteq D\), there exists some \(\xi \in [x, y]\) such that 
  \[\| f(x) - f(y)\| \le \|f'(\xi)\|_\text{op} \|x - y\|.\]
\end{theorem}
\proof

Let
\(g : [0, 1] \to \mathbb{R}^m : \alpha \mapsto f(\alpha x + (1 - \alpha) y)\)
and we see that \(g\) is continuously differentiable as it is the
composition of two continuous differentiable functions. Consider,
\[\| f(x) - f(y)\| = \|g(1) - g(0)\| = \left\| \int_0^1 g'(\alpha) \dd \alpha\right\|.\]
By the chain rule, we have
\(g'(\alpha) = (x - y) f'(\alpha x +(1 - \alpha)y)\) and so,
\[\begin{split}
    \left\| \int_0^1 g'(\alpha) \dd \alpha\right\| 
      & = \left\| \int_0^1 f'(\alpha x +(1 - \alpha)y) (x - y) \dd \alpha \right\|\\
      & = \int_0^1 \| f'(\alpha x +(1 - \alpha)y) (x - y) \| \dd \alpha\\
      & \le \|x - y\| \int_0^1 \| f'(\alpha x +(1 - \alpha)y) \|_\text{op} \dd \alpha\\
      & \le \|x - y\| \max_{\alpha \in [0, 1]} \| f'(\alpha x +(1 - \alpha)y) \|_\text{op}.
  \end{split}\] So, by defining \(\xi = \alpha x +(1 - \alpha)y\) where
\(\alpha\) maximises \(\| f'(\alpha x +(1 - \alpha)y) \|_\text{op}\), we
have \[\| f(x) - f(y)\| \le \|f'(\xi)\|_\text{op} \|x - y\|.\] \qed

We note that in the proof above, we assumed
\(\|\int f\| \le \int \|f\|\). We shall prove this claim now.

\begin{prop}
  Let \(I \subseteq \mathbb{R}\) be a interval and let \(f : I \to \mathbb{R}^m\) 
  be continuous. Then, 
  \[\left\|\int_{t_0}^{t_1} f(s) \dd s\right\| \le \left|\int_{t_0}^{t_1} \|f(s)\| \dd s\right|,\]
  for all \(t_0, t_1 \in I\).
\end{prop}
\proof

We use Riemann rather than Darboux sums for this occasion. Since, as we
have shown in first year analysis that Riemann and Darboux sums are
equivalent, this does not matter.

Wlog. assume \(t_0 < t_1\), and let us consider the \(n\)-th Riemann sum
of \(\left\|\int_{t_0}^{t_1} f(s) \dd s\right\|\),
\[\left\| \frac{t_1 - t_0}{n} 
    \sum_{i = 0}^{n - 1}f\left(t_0 + \frac{i}{n}(t_1 - t_0)\right)\right\| 
    \le \frac{t_1 - t_0}{n} 
    \left\|\sum_{i = 0}^{n - 1}f\left(t_0 + \frac{i}{n}(t_1 - t_0)\right)\right\|.\]
Thus, by taking \(n \to \infty\), the inequality is achieved. \qed

Now, with the mean value inequality under our belt, we may generalise
the result about Lipschitz continuity for higher dimensions.

\begin{corollary}
  Let \(U \subseteq \mathbb{R}^n\) be open, \(f : U \to \mathbb{R}^m\) be 
  continuously differentiable and \(C \subseteq U\) be compact and convex. 
  Then, \(f \mid_C : C \to \mathbb{R}^m\) is Lipschitz continuous.
\end{corollary}
\proof

Follows straight away from the mean value inequality. \qed

We note the restriction to a convex subset \(C\) since, if otherwise,
for all \(x, y \in \mathbb{R}^n\), it is not necessarily true that
\([x, y] \subseteq U\), and so, the mean value inequality does not
apply.

\hypertarget{picard-lindeluxf6f-theorem}{%
\subsection{Picard-Lindelöf Theorem}\label{picard-lindeluxf6f-theorem}}

We have now come to a very important theorem in this course -- the
Picard-Lindelöf theorem. The Picard-Lindelöf theorem is a strong
statement providing us the existence and uniqueness of solutions to
particular IVPs.

As we have seen previously, we shall approach this using the Banach's
fixed point theorem. By considering the Banach space
\(C^\circ(J, \mathbb{R}^d\)) equipped with the supremum norm, and by
showing a particular mapping is a contraction, we may apply Banach's
fixed point theorem resulting in the existence of a solution to the IVP.

\begin{theorem}[Picard-Lindelöf Theorem (global version)]
  Consider a nonautonomous differential equation 
  \[\dot x = f(t, x),\]
  where \(f : \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d\) is continuous 
  and satisfy the global Lipschitz condition, that is, there exists some 
  \(K \in \mathbb{R}^+\), such that,
  \[\| f(t, x) - f(t, y)\| \le K \|x - y\|,\]
  for all \(t \in \mathbb{R}\), \(x, y \in \mathbb{R}^d\). Then, the IVP 
  \[\dot x = f(t, x); \hspace{2mm} x(t_0) = x_0,\]
  has a unique solution on \([t_0 - h, t_0 + h]\), given by 
  \(\lambda : [t_0 - h, t_0 + h] \to \mathbb{R}^d\), where \(h := 1 / 2K\).
\end{theorem}

To prove this theorem, let us first prove the following lemma in which
we shall simply use the same notations as established for convenience.

\begin{lemma}
  Let \(X\) be the Banach space \(C^\circ([t_0 - h, t_0 + h], \mathbb{R}^d)\) 
  equipped with the supremum norm \(\|\cdot\|_\infty\). Then the function,
  \[P : X \to X : \lambda \mapsto \left(t \mapsto x_0 + \int_{t_0}^t f(s, \lambda(s)) \dd s\right)\]
  is a contraction on \(X\).
\end{lemma}
\proof

To show \(P\) is a contraction, it suffices to show that \(P\) is
Lipschitz continuous with some Lipschitz constant
\(K \in \mathbb{R}_{< 1}^+\); in this case, we shall show \(P\) has
Lipschitz constant \(1 / 2\).

Let \(u_1, u_2 \in X\), by the definition of the suprmum norm, it
suffices to show that for all \(t \in [t_0 - h, t_0 + h]\),
\[\|P(u_1)(t) - P(u_2)(t)\| \le \frac{1}{2} \|u_1 - u_2\|_\infty.\] This
follows as, \[\begin{split}
    \|P(u_1)(t) - P(u_2)(t)\| 
    & = \left\|\int_{t_0}^t f(s, u_1(s)) \dd s - \int_{t_0}^t f(s, u_2(s)) \dd s\right\|\\
    & = \left\|\int_{t_0}^t f(s, u_1(s)) - f(s, u_2(s)) \dd s \right\|\\
    & \le \left|\int_{t_0}^t \| f(s, u_1(s)) - f(s, u_2(s)) \| \dd s\right|
  \end{split}\] Since \(f\) satisfy the Lipschitz condition,
\(\|f(s, u_1(s)) - f(s, u_2(s))\| \le K \|u_1(s) - u_2(s)\| \le K\|u_1 - u_2\|_\infty\)
and so, \[\begin{split}
     \left|\int_{t_0}^t \| f(s, u_1(s)) - f(s, u_2(s)) \| \dd s\right|
      & \le K\|u_1 - u_2\|_\infty \left|\int_{t_0}^t \dd s\right|\\
      & \le K h \|u_1 - u_2\|_\infty = \frac{1}{2} \|u_1 - u_2\|_\infty.
  \end{split}\] \qed

With that, we can apply the Banach fixed point theorem.

\proof (Picard-Lindelöf theorem). Let \(P\) to be the function as
before, then by the Banach fixed point theorem, there exists a unique
\(\lambda \in X\) such that \(P(\lambda) = \lambda\). So, \(\lambda\) is
the unique local solution to our IVP. \qed

While the theorem only provides us with a local solution, the solution
can be easily extended on the whole space by reapplying the theorem at
the end points of the local unique solution (see problem sheet).

We remark that we have proven the global version of the Picard-Lindelöf
theorem in contrast to the local version. The global version requires
\(f\) is globally defined and is also the Lipschitz condition is held
globally; these conditions will be relaxed for the local version.
Indeed, it is not difficult to see that the global Lipschitz condition
is too strong by considering the differential equations such as
\(\dot x = tx^2\).

Let us now differentiate between the functions that are globally and
locally Lipschitz continuous.

\begin{definition}[Globally Lipschitz Continous]
  \(f : D \subseteq \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d\) is said 
  to be globally Lipschitz continuous with respect to \(x\) if there exists some 
  \(K > 0\) such that for all \(t \in \mathbb{R}\), \(x, y \in \mathbb{R}^d\),
  \[\|f(t, x) - f(t, y)\| \le K\|x - y\|.\]
\end{definition}

\begin{definition}[Locally Lipschitz continuous]
  \(f : D \subseteq \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d\) is said 
  to be locally Lipschitz continuous with respect to \(x\) if for all 
  \((t_0, x_0) \in D\), there exists some neighbourhood \(U\) of \((t_0, x_0)\)
  and some \(K > 0\) such that 
  \[\|f(t, x) - f(t, y)\| \le K\|x - y\|,\]
  for all \((t, x), (t, y) \in U\).
\end{definition}

\begin{theorem}[Picard-Lindelöf Theorem (local version)]
  Let \(D \subseteq \mathbb{R} \times \mathbb{R}^d\) be open and 
  \(f : D \to \mathbb{R}^d\) be continuous and locally Lipschitz continuous. 
  Then, for all \((t_0, x_0) \in D\), the IVP 
  \[\dot x = f(t, x); \hspace{2mm} x(t_0) = x_0\]
  has a unique solution on an interval of the form \([t_0 - h, t_0 + h]\) where 
  \(h = h(t_0, x_0)\) (Qualitative version). Furthermore, if we denote \(U\) for 
  the neighbourhood of \((t_0, x_0)\) on which the Lipschitz condition is held 
  with the Lipschitz constant \(K > 0\), by defining 
  \[W^{\tau, \delta}(t_0, x_0) := [t_0 - \tau, t_0 + \tau] \times 
    \overline{B_\delta(x_0)} \subseteq U,\]
  for some sufficent \(\tau, \delta > 0\), and \(M :\ge \|f(t, x)\|\) on 
  \(W^{\tau, \delta}(t_0, x_0)\), there exists a unique solution on the 
  interval \([t_0 - h, t_0 + h]\), where \(h := \min\{\tau, 1 / 2K, \delta / M\}\) 
  (Quantitative version). 
\end{theorem}
\proof

See extra meterials. \qed

\begin{prop}
  Let \(\Omega \subseteq \mathbb{R}^d\) be open and \(f : \Omega \to \mathbb{R}^d\) 
  be continuously differentiable. Then \(f\) is locally Lipschitz continuous.
\end{prop}
\proof

For al \(x \in \Omega\), let \(U \subseteq \Omega\) be a compact set
containing \(x\) (\(U\) exists as \(\Omega\) is open, there exists some
\(\delta > 0\) such that \(B_\delta(x) \subseteq \Omega\), and so, we
can simply take \(U = \overline{B_{\delta / 2}(x)}\)). Then, since
\(f'\) is continuous, it is bounded by some \(M > 0\). Thus, for all
\(x, y \in U\), by the mean value inequality, there exists some
\(\xi \in [x, y]\) such that
\[\|f(x) -f(y)\| \le \|f(\xi)\|_{\text{op}}\|x - y\| \le M \|x - y\|.\]
Thus, \(f\) is locally Lipschitz at \(x\) on \(U\) with the Lipschitz
constant \(M\). \qed

\begin{lemma}
  Let \(f : D \subseteq \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d\) be continuous 
  and locally Lipschitz where \(D\) is open. Then, given two solutions 
  \(\lambda : I \to \mathbb{R}^d\) and \(\mu : J \to \mathbb{R}^d\) of 
  the differential equation \(\dot x = f(t, x)\), either \(\lambda(t) = \mu(t)\) 
  or \(\lambda(t) \neq \mu(t)\) for all \(t \in I \cap J\).
\end{lemma}
\proof

Suppose there exists some \(t_0, t_1 \in I \cap J\) such that
\(\lambda(t_0) = \mu(t_0)\) and \(\lambda(t_1) \neq \mu(t_1)\), and
furthermore, Wlog. assume \(t_1 > t_0\). Then, by defining
\[\tilde t := \sup \{t > t_0 \mid \lambda(t') = \mu(t'), 
    \hspace{1mm} \forall t' \in [t_0, t]\},\] by the continuity of
\(\lambda\) and \(\mu\), \(\lambda - \mu\) is continuous, and so, by
considering
\(0 = \lim_{n \to \infty} (\lambda - \mu)(\tilde t - 1 / n) =  (\lambda - \mu)(\tilde t)\),
we have \(\lambda(\tilde t) = \mu(\tilde t)\). Then, by applying the
local version of the Picard-Lindelöf theorem with the initial value
\((\tilde t, \lambda(\tilde t) = \mu(\tilde t))\), we see that there
exists two different solutions \#. \qed

\hypertarget{maximal-solutions}{%
\subsection{Maximal Solutions}\label{maximal-solutions}}

As we have seen, the local version of Picard-Lindelöf provides us with a
unique solution on some interval around the initial value. We would now
like to maximise the size of this interval around the initial value and
such solutions are called maximal solutions.

\begin{definition}[Maximal Existence Interval]
  Given a initial value problem \(\dot x = f(t, x)\), \(x(t_0) = x_0\), 
  we define 
  \[I_+(t_0, x_0) := \sup \{t \ge t_0 \mid \exists \text{ solution on } [t_0, t]\},\]
  \[I_-(t_0, x_0) := \inf \{t \le t_0 \mid \exists \text{ solution on } [t, t_0]\},\]
  and the maximal existence interval,
  \[I_{\max} (t_0, x_0) := (I_-(t_0, x_0), I_+(t_0, x_0)).\]
\end{definition}

\begin{theorem}
  There exists a maximal solution\footnote{Any solution is defined on a subset of
  \(I_{\max}(t_0, x_0)\).} 
  \[\lambda_{\max} : I_{\max} (t_0, x_0) \to \mathbb{R}^d\]
  of the IVP \(\dot x = f(t, x)\), \(x(t_0) = x_0\) where \(f\) is continuous 
  and locally Lipschitz. Furthermore, if \(I_+(t_0, x_0) < \infty\), then 
  either the maximal solution is unbounded for \(t \supseteq t_0\), that is,
  \[\sup_{t \ge t_0}\|\lambda_{\max}(t)\| = \infty,\]
  or \(\partial D \neq \varnothing\), and 
  \[\lim_{t \uparrow I_+(t_0, x_0)} \text{dist}((t, \lambda_{\max}(t)), \partial D) = 0,\]
  where \(\text{dist}(y, A) := \inf\{\|x - y\| \mid x \in A\}\).
  Analogous case for \(I_- (t_0, x_0) > -\infty\).
\end{theorem}
\proof

Let \(\bar{t} \in I_{\max}(t_0, x_0)\), then, by the definition of the
maximal existence interval, there exists some solution
\(\mu : I \to \mathbb{R}^d\) such that \(\bar{t}, t_0 \in I\). Now, by
considering that the solutions cannot cross each other, we see that all
solutions of the IVP on \([t_0, \bar{t}]\) must coincide as they crosses
at \((t_0, x_0)\). Thus, the definition
\[\lambda_{\max}(\bar{t}) := \mu(\bar{t})\] is well-defined. Indeed,
\(\lambda_{\max}\) is a solution at \(\bar{t}\) since
\[\dot \lambda_{\max}(\bar{t}) = \dot \mu(\bar{t}) = f(\bar{t}, \mu(\bar{t})) = 
    f(\bar{t}, \lambda_{\max}(\bar{t})).\] Now, by the construction of
\(I_{\max}\), the only possible case on which the the theorem does not
hold is that \(I_+(t_0, x_0) \in I_{\max}(t_0, x_0)\). So, for
contradiction, assume that there exists some solution
\(\mu : [t, I_{\max}(t_0, x_0)] \to \mathbb{R}^d\) to the IVP. However,
by the local version of Picard-Lindelöf, there exists some open \(U\)
containing \(I_{\max}(t_0, x_0)\) such that there exists a unique
solution \(\eta\) of the IVP on the interval
\((I_{\max}(t_0, x_0) - h, I_{\max}(t_0, x_0) + h)\), and hence, we may
extend \(\mu\) to be a solution on \([t, I_{\max}(t_0, x_0) + h]\)
contradicting the maximum condition \#.

For the second part of the theorem, assume \(I_+(t_0, x_0) < \infty\).
Then, we may find some sequence \((t_n)_{n = 1}^\infty\subseteq D\) that
converges to \(I_-(t_0, x_0)\) and assume, for contradiction, there
exists some \(M > 0\), such that \(\|\lambda_{\max}(t_n)\| \le M\) and
\(\text{dist}((t_n, \lambda_{\max}(t_n)),  \partial D) \ge 1 / M\) for
all \(n \in \mathbb{N}\).

By assumption, we have \((t_n, \lambda_{\max}(t_n))\) is a bounded
sequence, and so, by Bolzano-Weierstrass, it has a convergent
subsequence \((t_{n_i}, \lambda_{\max}(t_{n_i}))\) which converges to
\((t^*, x^*) \in D\)\ldots{} \qed

\end{document}
