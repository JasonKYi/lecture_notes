% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Differentiable Equations},
  pdfauthor={Kexing Ying},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=red,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin = 1.5in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{lipsum}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{physics}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\Arg}{\mathop{\mathrm{Arg}}}
\newcommand{\hess}{\mathop{\mathrm{Hess}}}

\title{Differentiable Equations}
\author{Kexing Ying}
\date{January 11, 2021}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\newpage

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

While we have seen differential equations in year one, we have mostly
focused on the different methods of solving specific differential
equations. This cannot be expected for general differential equations
and in this year, we will focus on existence and uniqueness of solutions
to differential equations and develop qualitative tools to help us
understand these solutions.

We recall that an algebraic equation is an equation of the form
\(f(x) = 0\) while a differential equation is an equation of the form
\(\dot x = f(x)\) for some function \(f : \mathbb{R} \to \mathbb{R}\).
That is, an algebraic equation has real numbers as solutions while an
differential equation has functions as its solution.

As an example, let us consider the simple differentiable equation
\begin{equation}\label{simple}
  \dot x = a x,
\end{equation} for some \(a \in \mathbb{R}\). Then, a function
\(\lambda : I \to \mathbb{R}\) solves \ref{simple} if
\(\dot \lambda = a \lambda\) for all \(t \in I\) where
\(I \subseteq \mathbb{R}\) is a interval. These types of differentiable
equations occurs often in relation in growth and decay and one can
easily see that the family of functions
\[\lambda_b : \mathbb{R} \to \mathbb{R} = t \mapsto b e^{at}, \hspace{2mm} b \in \mathbb{R},\]
are solutions to \ref{simple}. Of course, we know this already, so an
more interesting question would be whether or not this family contains
all the solutions to \ref{simple}. It turns out to be true, and to show
this we will assume \(\mu : I \to \mathbb{R}\) is a solution to
\(\dot x = a x\). Then,
\[\dv{t}\left(\mu e^{-at}\right) = \dot \mu e^{-at} - a\mu e^{-at} = 0,\]
since \(\dot \mu = a \mu\) and so, \(\mu e^{-at}\) is constant,
i.e.~there exists \(b \in \mathbb{R}\) such that \(\mu e^{-at} = b\) and
hence, \[\mu = b e^{at}.\] This demonstrates that all solutions to
\ref{simple} are members of the aforementioned solution family and
hence, we have found \textbf{all} of the solutions to \ref{simple}.

With the above example, we see that rather than working with solutions
that are in finite-dimensional vector spaces, our solution are in
function spaces which are typically infinite-dimensional. This is
studied in more detail in the next year's \emph{functional analysis}
course, and in general, infinite-dimensional spaces are more difficult
to grasp. However, for the vast majority of materials in this course, a
finite-dimensional thinking suffices while we will also cover some
material from functional analysis to understand the differentiable
equations as well.

\hypertarget{ordinary-differential-equations-and-initial-value-problems}{%
\subsection{Ordinary Differential Equations and Initial Value
Problems}\label{ordinary-differential-equations-and-initial-value-problems}}

There are two types of differential equations -- \emph{autonomous
differential equations} and \emph{nonautonomous differentiable
equations}. Autonomous differential equations are differentiable
equations of the form \(\dot x = f(x)\) such as equation \ref{simple}
while nonautonomous differential equations are equations of the form
\(\dot x = f(t, x)\).

We note that this does not cover higher-order differential equations,
but from last year, we recall that one may reduce a higher-order
differential equations into a first-order differential equation in
vector form and thus, the theories we develop within this course will
also apply to higher-order differential equations.

\begin{definition}[Ordinary Differential Equation]
  Let \(d \in \mathbb{N}\), \(D \subseteq \mathbb{R} \times \mathbb{R}^d\) be 
  open, and a function \(f \to D \to \mathbb{R}^d\). Then, an equation of the form 
  \[\dot x = f(t, x)\]
  is called a \(d\)-dimensional (first-order) ordinary differential equation. 

  A differentiable function \(\lambda : I \to \mathbb{R}^d\) on some interval 
  \(I \subseteq \mathbb{R}\) is called a solution of the differential equation 
  if and only if for all \((t, \lambda(t) \in D\), if \(t \in I\) then, 
  \[\dot \lambda(t) = f(t, \lambda(t)).\]
  We say that an ordinary differential equation is autonomous if \(f\) is 
  independent of \(t\) and nonautonomous otherwise.
\end{definition}

We will only consider ordinary differential equations (ODE) in this
course while partial differential equations, that is differential
equations which solutions are functions which depends on multiple
variables are covered in the second year course \textbf{Partial
Differential Equations in Action}.

\begin{prop}[Constant solutions to autonomous differential equations]\label{const_sol}
  Let \(D \subseteq \mathbb{R}^d\) be an open set and \(f : D \to \mathbb{R}^d\) 
  be a function where \(d \in \mathbb{N}\). Then, there exists a constant 
  solution \(\lambda : \mathbb{R} \to \mathbb{R}^d : x \mapsto a\) to the 
  autonomous differential
  \[\dot x = f(x)\]
  for some \(a \in \mathbb{R}^d\) if and only if \(f(a) = 0\).
\end{prop}
\proof

(\(\implies\)) Suppose that
\(\lambda : I \to \mathbb{R}^d : x \mapsto a\) is a solution the
\(\dot x = f(x)\). Then
\[0 = \dot \lambda (t) = f(\lambda (t)) = f(a).\] (\(\impliedby\))
Suppose there exists some \(a \in \mathbb{R}^d\) such that \(f(a) = 0\),
then verifying, we find
\(\lambda : \mathbb{R} \to \mathbb{R}^d : x \mapsto a\) is a solution to
the differential equation. \qed

This proposition allows us to find solutions to many autonomous ODEs as,
indeed, if \(f : D \to \mathbb{R}^d\) has a root \(a \in \mathbb{R}^d\),
the above proposition guarantees that
\(\lambda : \mathbb{R} \to \mathbb{R}^d : x \mapsto a\) is a solution to
\(\dot x = f(x)\).

\begin{definition}[Initial Value Problem]
  Let \(d \in \mathbb{N}\), \(D \subseteq \mathbb{R} \times \mathbb{R}^d\) be an 
  open set, and \(f : D \to \mathbb{R}^d\) be a function. The system of equations 
  from combining the differential equation 
  \[\dot x = f(t, x),\]
  with the initial condition
  \[x(t_0) = x_0\] 
  where \((t_0, x_0) \in D\) is called an initial value problem.

  A solution to the above initial value problem is a function 
  \(\lambda : I \to \mathbb{R}^d\) that is a solution to the differential equation 
  \(\dot x = f(t, x)\) and \(\lambda(t_0) = x_0\).
\end{definition}

While, previously, we have seen a differential equation which always has
a solution. This, however, is not always the case.

\begin{example}
  Consider the differential equation \(\dot x = f(x)\), where 
  \[f(x) = \begin{cases}
    1, & x < 0;\\
    -1, & x \ge 0,
  \end{cases}\]
  with the boundary condition \(x(0) = 0\). As we will see on the problem sheet, 
  this differential equation indeed does not have a solution.
\end{example}

\begin{example}
  Consider the initial value problem \(\dot x = f(x) = \sqrt{|x|}\) with the 
  boundary condition \(x(0) = 0\).

  Since \(f(0) = 0\) we have \(x(t) = 0\) is a constant solution by 
  proposition \ref{const_sol}. Furthermore, by consider the function 
  \[\lambda_b(t) = 
  \begin{cases}
    0, & t \le b;\\
    \frac{1}{4}(t - b)^2, & t > b,
  \end{cases}\]
  we find \(\lambda_b\) to also be a solution for any \(b \in \mathbb{R}^+_0\).
  However, as for all \(t \le b\), \(\lambda_b(t) = 0\), we see that \(x\) is 
  not unique given values of \(x\) at \(t\).
\end{example}

Before moving on, let us quickly recall the \emph{separation of
variables} procedure for solving differential equations. Suppose we are
to solve a differential equation of the form, \[\dot x = g(t) h(x),\]
with the boundary condition \(x(t_0) = x_0\) where
\(g : I \to \mathbb{R}\) and \(h : J \to \mathbb{R}\) are continuous
functions. Then, we have
\[\int_{x_0} ^x\frac{\dd y}{h(y)} = \int_{t_0}^t g(s) \dd s.\] Thus, by
evaluating the integral, we can express \(x\) in \(t\). We see that this
procedure does indeed provide us with a correct solution by simply
applying FTC on both sides of the equation.

\begin{example}
  Consider the initial value problem \(\dot x = t x^2\) with the boundary condition 
  \(x(t_0) = x_0\) where \(x_0 \neq 0\). By the separation of variables, we find 
  \[x = \frac{2x_0}{2 + x_0(t_0^2 - t^2)}.\]
  However, we see that for this solution, \(x\) does not necessarily exist for 
  all \(t\). Indeed, if \(t_0 = 0, x_0 = 1\), we find 
  \[x = \frac{2}{2 - t^2},\]
  which does not have a solution for \(t = \pm\sqrt{2}\), and so, the solution 
  does not exist globally.
\end{example}

\hypertarget{visualisations}{%
\subsection{Visualisations}\label{visualisations}}

Visualisations are very important for differential equations as it
provides us a mental image of how to think about differential equations.
In general, there are two main methods to visualise differential
equations:

\begin{itemize}
  \item nonautonomous differential equations \(\dot x = f(t, x)\) via the 
    \textit{solution portrait} in the extended phase space;
  \item autonomous differential equations \(\dot x = f(x)\) via the 
    \textit{phase portrait} in the phase space.
\end{itemize}

Suppose we have the nonautonomous differential equations
\(\dot x = f(t, x)\) where
\(f : D \subseteq \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d\),
have a solution \(\lambda : I \to \mathbb{R}^d\), i.e.~for all
\(t \in I\), \(\dot \lambda(t) = f(t, \lambda(t))\). Thus, we see that
the vector \((1, f(t_0, \lambda(t_0))\) for some \(t_0 \in I\) is
tangential to the solution which passes through \((t_0, \lambda(t_0))\).
This can be done for all \(p \in \mathbb{R} \times \mathbb{R}^d\) and
so, by drawing these vectors at a sufficient number of points, we can
have a mental image of what the solution looks like. A plot of these
vectors is refereed to as a \textbf{vector field}.

A \textbf{solution portrait} is given by a visualisation of several
solution curves in the \((t, x)\)-space, the so called \emph{extended
phase space}. This is called as such since the \(x\)-space is normally
refereed as the phase space, and so, we are extending it by the time
axis.

On the other hand, suppose we have the autonomous differential equation
\(\dot x = f(x)\) where
\(f : D \subseteq \mathbb{R}^d \to \mathbb{R}^d\) has a solution
\(\lambda : I \to \mathbb{R}^d\). By considering that \(f\) is
independent of \(t\), we see that translating \(\lambda\) along \(t\) is
also a valid solution. Indeed, at \(x = \lambda(t_0)\), if \(\lambda'\)
is another solution such that \(\lambda'(t_1) = \lambda(t_0)\), then
\(\dot \lambda'(t_1) = \dot \lambda(t_0)\). This property is refereed to
as \emph{translation invariance} and all solutions of autonomous
differential equations are translation invariant.

\begin{prop}
  let \(\dot x = f(x)\) be an autonomous differential equation. Then, if 
  \(\lambda : I \to \mathbb{R}^d\) is a solution to this differential equation, 
  so is
  \[\mu : \bar{I} \to \mathbb{R}^d : t \mapsto \lambda(t + \tau),\]
  where \(\tau \in \mathbb{R}\), \(\bar{I} := I + \tau\).
\end{prop}
\proof

Follows straight away by chain rule. \qed

\begin{example}
  Consider the harmonic oscillator \(\ddot x = -x\). By converting it into 
  a first order differential equation, we have 
  \[\dot{\mathbf{x}} = 
  \begin{bmatrix}
    \dot x\\ \dot y
  \end{bmatrix} = 
  \begin{bmatrix}
    0 & 1 \\
    -1 & 0
  \end{bmatrix}\mathbf{x}.\]
  By solving the system, we find 
  \[\lambda(t) = \begin{bmatrix}
    \cos(t)\sin(t)\\ -\sin(t)\cos(t)
  \end{bmatrix}
  \begin{bmatrix}
    x_0\\ y_0
  \end{bmatrix}\]
  is a solution. Indeed, this describes a oscillatory motion in the phase space 
  where the position is circular.
\end{example}

With consideration with the example above, we see that we may project a
solution portrait \((t, x) \mapsto (1, f(x))\) onto \(x \mapsto f(x)\)
resulting in a \textbf{phase portrait}.

\hypertarget{existence-and-uniqueness}{%
\section{Existence and Uniqueness}\label{existence-and-uniqueness}}

As we have seen, differential equations need not have unique solutions
given an initial value. Indeed, we have also seen that it is not
guaranteed to have a solution at all. We will in this chapter resolve
the question on whether or not a solution exists by presenting a theory
that guarantees existence and uniqueness for solutions to initial value
problems.

\hypertarget{picard-iterates}{%
\subsection{Picard iterates}\label{picard-iterates}}

We observe that often times, to solve an differential equation, we need
to reformulate it as an integral equation.

\begin{prop}\label{alt_repr}
  Consider the initial value problem 
  \[\dot x = f(t, x); \hspace{2mm} x(t_0) = x_0,\]
  where \(f : D \subseteq \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d\) is 
  continuous. Let \(\lambda : I \to \mathbb{R}^d\), \(\lambda\) solves the IVP 
  if and only if \(\lambda\) is continuous and solves the integral 
  equation\footnote{We recall that the integral of a vector valued function is 
  simply the integral of the components.} 
  \[\lambda(t) = x_0 + \int_{t_0}^t f(s, \lambda(s)) \dd s.\]
\end{prop}
\proof

Follows from FTC. \qed

However, as \(f\) depends on \(\lambda\), we in general cannot compute
the integral on the right hand side. Nonetheless, this brings us closer
to the formulation of the Picard iterates.

\begin{prop}
  Let \(f\) be a continuous function. Then by defining \(a_0\) for some value and 
  \(a_{n + 1} = f(a_n)\), if \((a_n)_{n = 1}^\infty\) converges to some value 
  \(a\), then \(a = f(a)\).
\end{prop}
\proof

We see that if \((a_n)_{n = 1}^\infty\) converges to some value \(a\),
\(f(a_n) \to f(a)\) by sequential continuity. However, as
\(\lim_{n \to \infty} f(a_n) = \lim_{n \to \infty} a_{n + 1} = a\), by
uniqueness of limits on Hausdorff spaces, we have \(a = f(a)\). \qed

With this proposition in mind, we may apply a similar iteration on the
integral equation resulting in the Picard iterates.

\begin{definition}[Picard Iterates]
  Given an IVP, the Picard iterates is the sequence of functions 
  \((\lambda_n : J \to \mathbb{R}^d)_{n = 1}^\infty\) be defined such that 
  \(\lambda_0(t) = x_0\), and 
  \[\lambda_{n + 1}(t) = x_0 + \int_{t_0}^t f(s, \lambda_n(s)) \dd s.\]
\end{definition}

With connotation to the motivating proposition, we hope that
\((\lambda_n)\) converges to the solution \(\lambda_\infty\) in some
notion. As we shall see, the Picard iterates needs to converge to
\(\lambda_\infty\) uniformly in order for \(\lambda_\infty\) to be a
solution.

\begin{prop}
  Given an IVP, if the Picard iterates \((\lambda_n)_{n = 0}^\infty\) converges 
  uniformly to \(\lambda_\infty\), then \(\lambda_\infty\) is a solution to the 
  IVP.
\end{prop}
\proof

Consider the following chain of equalities, \[\begin{split}
    \lambda_\infty(t) = \lim_{n \to \infty} \lambda_{n + 1}(t) & = 
    \lim_{n \to \infty} x_0 + \int_{t_0}^t f(s, \lambda_n(s)) \dd s\\ &= 
    x_0 + \int_{t_0}^t f(s, \lim_{n \to \infty} \lambda_n(s)) \dd s = 
    x_0 + \int_{t_0}^t f(s, \lambda_\infty(s)) \dd s,
  \end{split}\] where the third equality is true as
\(\lambda_n \to \lambda_\infty\) uniformly (see first year analysis for
proof). Thus, by proposition \ref{alt_repr}, \(\lambda_\infty\) solves
the IVP. \qed

With this proposition in mind, it is \emph{sometimes} possible to show
that a particular Picard iterates converges uniformly towards some
function resulting in a solution to the corresponding IVP.

\hypertarget{lipschitz-continuity}{%
\subsection{Lipschitz Continuity}\label{lipschitz-continuity}}

We recall from first year analysis the definition of Lipschitz
continuity. We are interested in Lipschitz continuity since it is very
helpful when showing the existence and uniqueness of solutions of IVPs.
Indeed, by reformulating the Picard iterates as an operator \(P\) on the
Banach space \(\mathcal{C}^\circ(J, \mathbb{R}^d)\), we find that \(P\)
is a contraction (i.e.~\(P\) has Lipschitz constant \(< 1\)) if \(f\)
satisfies certain Lipschitz condition, and hence, by Banach's fixed
point theorem, has a fixed point.

We recall that a normed vector space is a vector space \(V\) equipped
with a norm \(\| \cdot \|\) such that the norm satisfies positive
definiteness, absolute homogeneity and the triangle inequality. Indeed,
we recall the chain of induced structure: Inner product space
\(\implies\) Normed vector space \(\implies\) (Metric space \(\implies\)
Topological space) \(\vee\) Vector space.

\begin{definition}[Lipschitz Continuity]
  Let \((V, \| \cdot \|_V)\), \((W, \| \cdot \|_W)\) be two normed vector spaces 
  and suppose \(X \subset V\) and \(Y \subseteq W\), then \(f : X \to Y\) is 
  Lipschitz continuous if there exists some \(K > 0\) (a Lipschitz constant) 
  such that for all \(x, y \in X\),
  \[\| f(x) - f(y) \|_W \le K \|x - y\|_V.\]
\end{definition}

We recall the sufficient conditions for which a function is Lipschitz
continuous.

\begin{prop}
  Let \(f : I \to \mathbb{R}\) be differentiable with bounded derivative on some 
  interval \(I\), then \(f\) is Lipschitz continuous.
\end{prop}
\proof

By MVT, for all \(x, y \in I\), there exists some \(c \in (x, y)\) such
that \[f(x) - f(y) = f'(c) (x - y).\] So by taking absolute value on
both sides, and by choosing the Lipschitz constant as the supremum of
\(|f'|\) we have found some \(K := \sup_{c \in I} \|f'(c)\|\), so
\[|f(x) - f(y)| \le K \|x - y\|.\] \qed

We see that, if \(f\) is continuously differentiable, \(f'\) is
continuous on the compact set \(I\), and so \(f'\) is uniformly
continuous, and so is bounded. Thus, if \(f : I \to \mathbb{R}\) is
continuously differentiable, then it is Lipschitz continuous.

For higher dimensions, we require the mean value inequality for higher
dimensions and so an similar result is achieved. To look at the mean
value theorem for higher dimensions, let us first introduce the operator
norm for linear maps.

\begin{definition}[Operator Norm]
  Let \(f : \mathbb{R}^n \to \mathbb{R}^m\) be a linear map, then the operator 
  norm of \(f\) is 
  \[\| f\|_\text{op} := \sup_{x \in \mathbb{R}^n \setminus \{0\}} \frac{\| f(x) \|}{\|x\|}
  \equiv \sup_{x \in \mathbb{R}^n, \|x\| = 1} \|f(x)\|,\]
  where \(\| \cdot \|\) are the Euclidean norms.
\end{definition}

As we saw in linear algebra last term, the operator norm form norms on
the space of linear maps as the name suggests. By considering that the
set \(S_1 := \{x \in \mathbb{R}^n, \|x\| = 1\}\) is closed and bounded,
and hence compact by Heine-Borel, as linear maps are continuous, \(f\)
attains its maximum as on \(S_1\), so we may in fact write the operator
norm as
\[\| f\|_\text{op} := \max_{x \in \mathbb{R}^n, \|x\| = 1} \|f(x)\| < \infty.\]

\begin{prop}
  If \(f : \mathbb{R}^n \to \mathbb{R}^m\) is a linear map and \(x \in \mathbb{R}^n\),
  then, 
  \[\|f(x)\|  \le \|f\|_\text{op} \|x\|.\]
\end{prop}
\proof

If \(x = 0\) then both sides are zero so suppose otherwise. If
\(x \neq 0\), then \(\|x\| > 0\) and so it suffices to show
\[\frac{\|f(x)\|}{\|x\|} \le \|f\|_\text{op}.\] But, this follows
directly from the definition so we are done. \qed

\begin{prop}
  If \(f : \mathbb{R}^n \to \mathbb{R}^m\) be a linear map, then \(f\) is Lipschitz 
  continuous with the Lipschitz constant \(\|f\|_\text{op}\).
\end{prop}
\proof

This follows as, for all \(x, y \in \mathbb{R}^n\)
\[\|f(x) - f(y)\| = \| f(x - y) \| \le \|f\|_\text{op} \|x - y\|.\] \qed

\begin{definition}[Closed Line Segment]
  For convenience, for all \(x, y \in \mathbb{R}^n\), we define the closed line 
  segment 
  \[[x, y] := \{\alpha x + (1 - \alpha)y \mid \alpha \in [0, 1]\}.\]
\end{definition}

\begin{theorem}[Mean Value Inequality]
  Let \(D \subseteq \mathbb{R}^n\) be open and \(f : D \to \mathbb{R}^m\) be a 
  function which is continuously differentiable. Then, for all \(x, y \in D\) 
  with \([x, y] \subseteq D\), there exists some \(\xi \in [x, y]\) such that 
  \[\| f(x) - f(y)\| \le \|f'(\xi)\|_\text{op} \|x - y\|.\]
\end{theorem}
\proof

Let
\(g : [0, 1] \to \mathbb{R}^m : \alpha \mapsto f(\alpha x + (1 - \alpha) y)\)
and we see that \(g\) is continuously differentiable as it is the
composition of two continuous differentiable functions. Consider,
\[\| f(x) - f(y)\| = \|g(1) - g(0)\| = \left\| \int_0^1 g'(\alpha) \dd \alpha\right\|.\]
By the chain rule, we have
\(g'(\alpha) = (x - y) f'(\alpha x +(1 - \alpha)y)\) and so,
\[\begin{split}
    \left\| \int_0^1 g'(\alpha) \dd \alpha\right\| 
      & = \left\| \int_0^1 f'(\alpha x +(1 - \alpha)y) (x - y) \dd \alpha \right\|\\
      & = \int_0^1 \| f'(\alpha x +(1 - \alpha)y) (x - y) \| \dd \alpha\\
      & \le \|x - y\| \int_0^1 \| f'(\alpha x +(1 - \alpha)y) \|_\text{op} \dd \alpha\\
      & \le \|x - y\| \max_{\alpha \in [0, 1]} \| f'(\alpha x +(1 - \alpha)y) \|_\text{op}.
  \end{split}\] So, by defining \(\xi = \alpha x +(1 - \alpha)y\) where
\(\alpha\) maximises \(\| f'(\alpha x +(1 - \alpha)y) \|_\text{op}\), we
have \[\| f(x) - f(y)\| \le \|f'(\xi)\|_\text{op} \|x - y\|.\] \qed

We note that in the proof above, we assumed
\(\|\int f\| \le \int \|f\|\). We shall prove this claim now.

\begin{prop}
  Let \(I \subseteq \mathbb{R}\) be a interval and let \(f : I \to \mathbb{R}^m\) 
  be continuous. Then, 
  \[\left\|\int_{t_0}^{t_1} f(s) \dd s\right\| \le \left|\int_{t_0}^{t_1} \|f(s)\| \dd s\right|,\]
  for all \(t_0, t_1 \in I\).
\end{prop}
\proof

We use Riemann rather than Darboux sums for this occasion. Since, as we
have shown in first year analysis that Riemann and Darboux sums are
equivalent, this does not matter.

Wlog. assume \(t_0 < t_1\), and let us consider the \(n\)-th Riemann sum
of \(\left\|\int_{t_0}^{t_1} f(s) \dd s\right\|\),
\[\left\| \frac{t_1 - t_0}{n} 
    \sum_{i = 0}^{n - 1}f\left(t_0 + \frac{i}{n}(t_1 - t_0)\right)\right\| 
    \le \frac{t_1 - t_0}{n} 
    \left\|\sum_{i = 0}^{n - 1}f\left(t_0 + \frac{i}{n}(t_1 - t_0)\right)\right\|.\]
Thus, by taking \(n \to \infty\), the inequality is achieved. \qed

Now, with the mean value inequality under our belt, we may generalise
the result about Lipschitz continuity for higher dimensions.

\begin{corollary}
  Let \(U \subseteq \mathbb{R}^n\) be open, \(f : U \to \mathbb{R}^m\) be 
  continuously differentiable and \(C \subseteq U\) be compact and convex. 
  Then, \(f \mid_C : C \to \mathbb{R}^m\) is Lipschitz continuous.
\end{corollary}
\proof

Follows straight away from the mean value inequality. \qed

We note the restriction to a convex subset \(C\) since, if otherwise,
for all \(x, y \in \mathbb{R}^n\), it is not necessarily true that
\([x, y] \subseteq U\), and so, the mean value inequality does not
apply.

\hypertarget{picard-lindeluxf6f-theorem}{%
\subsection{Picard-Lindelöf Theorem}\label{picard-lindeluxf6f-theorem}}

We have now come to a very important theorem in this course -- the
Picard-Lindelöf theorem. The Picard-Lindelöf theorem is a strong
statement providing us the existence and uniqueness of solutions to
particular IVPs.

As we have seen previously, we shall approach this using the Banach's
fixed point theorem. By considering the Banach space
\(C^\circ(J, \mathbb{R}^d\)) equipped with the supremum norm, and by
showing a particular mapping is a contraction, we may apply Banach's
fixed point theorem resulting in the existence of a solution to the IVP.

\begin{theorem}[Picard-Lindelöf Theorem (global version)]
  Consider a nonautonomous differential equation 
  \[\dot x = f(t, x),\]
  where \(f : \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d\) is continuous 
  and satisfy the global Lipschitz condition, that is, there exists some 
  \(K \in \mathbb{R}^+\), such that,
  \[\| f(t, x) - f(t, y)\| \le K \|x - y\|,\]
  for all \(t \in \mathbb{R}\), \(x, y \in \mathbb{R}^d\). Then, the IVP 
  \[\dot x = f(t, x); \hspace{2mm} x(t_0) = x_0,\]
  has a unique solution on \([t_0 - h, t_0 + h]\), given by 
  \(\lambda : [t_0 - h, t_0 + h] \to \mathbb{R}^d\), where \(h := 1 / 2K\).
\end{theorem}

To prove this theorem, let us first prove the following lemma in which
we shall simply use the same notations as established for convenience.

\begin{lemma}
  Let \(X\) be the Banach space \(C^\circ([t_0 - h, t_0 + h], \mathbb{R}^d)\) 
  equipped with the supremum norm \(\|\cdot\|_\infty\). Then the function,
  \[P : X \to X : \lambda \mapsto \left(t \mapsto x_0 + \int_{t_0}^t f(s, \lambda(s)) \dd s\right)\]
  is a contraction on \(X\).
\end{lemma}
\proof

To show \(P\) is a contraction, it suffices to show that \(P\) is
Lipschitz continuous with some Lipschitz constant
\(K \in \mathbb{R}_{< 1}^+\); in this case, we shall show \(P\) has
Lipschitz constant \(1 / 2\).

Let \(u_1, u_2 \in X\), by the definition of the suprmum norm, it
suffices to show that for all \(t \in [t_0 - h, t_0 + h]\),
\[\|P(u_1)(t) - P(u_2)(t)\| \le \frac{1}{2} \|u_1 - u_2\|_\infty.\] This
follows as, \[\begin{split}
    \|P(u_1)(t) - P(u_2)(t)\| 
    & = \left\|\int_{t_0}^t f(s, u_1(s)) \dd s - \int_{t_0}^t f(s, u_2(s)) \dd s\right\|\\
    & = \left\|\int_{t_0}^t f(s, u_1(s)) - f(s, u_2(s)) \dd s \right\|\\
    & \le \left|\int_{t_0}^t \| f(s, u_1(s)) - f(s, u_2(s)) \| \dd s\right|
  \end{split}\] Since \(f\) satisfy the Lipschitz condition,
\(\|f(s, u_1(s)) - f(s, u_2(s))\| \le K \|u_1(s) - u_2(s)\| \le K\|u_1 - u_2\|_\infty\)
and so, \[\begin{split}
     \left|\int_{t_0}^t \| f(s, u_1(s)) - f(s, u_2(s)) \| \dd s\right|
      & \le K\|u_1 - u_2\|_\infty \left|\int_{t_0}^t \dd s\right|\\
      & \le K h \|u_1 - u_2\|_\infty = \frac{1}{2} \|u_1 - u_2\|_\infty.
  \end{split}\] \qed

With that, we can apply the Banach fixed point theorem.

\proof (Picard-Lindelöf theorem). Let \(P\) to be the function as
before, then by the Banach fixed point theorem, there exists a unique
\(\lambda \in X\) such that \(P(\lambda) = \lambda\). So, \(\lambda\) is
the unique local solution to our IVP. \qed

While the theorem only provides us with a local solution, the solution
can be easily extended on the whole space by reapplying the theorem at
the end points of the local unique solution (see problem sheet).

We remark that we have proven the global version of the Picard-Lindelöf
theorem in contrast to the local version. The global version requires
\(f\) is globally defined and is also the Lipschitz condition is held
globally; these conditions will be relaxed for the local version.
Indeed, it is not difficult to see that the global Lipschitz condition
is too strong by considering the differential equations such as
\(\dot x = tx^2\).

Let us now differentiate between the functions that are globally and
locally Lipschitz continuous.

\begin{definition}[Globally Lipschitz Continous]
  \(f : D \subseteq \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d\) is said 
  to be globally Lipschitz continuous with respect to \(x\) if there exists some 
  \(K > 0\) such that for all \(t \in \mathbb{R}\), \(x, y \in \mathbb{R}^d\),
  \[\|f(t, x) - f(t, y)\| \le K\|x - y\|.\]
\end{definition}

\begin{definition}[Locally Lipschitz continuous]
  \(f : D \subseteq \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d\) is said 
  to be locally Lipschitz continuous with respect to \(x\) if for all 
  \((t_0, x_0) \in D\), there exists some neighbourhood \(U\) of \((t_0, x_0)\)
  and some \(K > 0\) such that 
  \[\|f(t, x) - f(t, y)\| \le K\|x - y\|,\]
  for all \((t, x), (t, y) \in U\).
\end{definition}

\begin{theorem}[Picard-Lindelöf Theorem (local version)]
  Let \(D \subseteq \mathbb{R} \times \mathbb{R}^d\) be open and 
  \(f : D \to \mathbb{R}^d\) be continuous and locally Lipschitz continuous. 
  Then, for all \((t_0, x_0) \in D\), the IVP 
  \[\dot x = f(t, x); \hspace{2mm} x(t_0) = x_0\]
  has a unique solution on an interval of the form \([t_0 - h, t_0 + h]\) where 
  \(h = h(t_0, x_0)\) (Qualitative version). Furthermore, if we denote \(U\) for 
  the neighbourhood of \((t_0, x_0)\) on which the Lipschitz condition is held 
  with the Lipschitz constant \(K > 0\), by defining 
  \[W^{\tau, \delta}(t_0, x_0) := [t_0 - \tau, t_0 + \tau] \times 
    \overline{B_\delta(x_0)} \subseteq U,\]
  for some sufficent \(\tau, \delta > 0\), and \(M :\ge \|f(t, x)\|\) on 
  \(W^{\tau, \delta}(t_0, x_0)\), there exists a unique solution on the 
  interval \([t_0 - h, t_0 + h]\), where \(h := \min\{\tau, 1 / 2K, \delta / M\}\) 
  (Quantitative version). 
\end{theorem}
\proof

See extra meterials. \qed

\begin{prop}
  Let \(\Omega \subseteq \mathbb{R}^d\) be open and \(f : \Omega \to \mathbb{R}^d\) 
  be continuously differentiable. Then \(f\) is locally Lipschitz continuous.
\end{prop}
\proof

For al \(x \in \Omega\), let \(U \subseteq \Omega\) be a compact set
containing \(x\) (\(U\) exists as \(\Omega\) is open, there exists some
\(\delta > 0\) such that \(B_\delta(x) \subseteq \Omega\), and so, we
can simply take \(U = \overline{B_{\delta / 2}(x)}\)). Then, since
\(f'\) is continuous, it is bounded by some \(M > 0\). Thus, for all
\(x, y \in U\), by the mean value inequality, there exists some
\(\xi \in [x, y]\) such that
\[\|f(x) -f(y)\| \le \|f'(\xi)\|_{\text{op}}\|x - y\| \le M \|x - y\|.\]
Thus, \(f\) is locally Lipschitz at \(x\) on \(U\) with the Lipschitz
constant \(M\). \qed

\begin{lemma}
  Let \(f : D \subseteq \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d\) be continuous 
  and locally Lipschitz where \(D\) is open. Then, given two solutions 
  \(\lambda : I \to \mathbb{R}^d\) and \(\mu : J \to \mathbb{R}^d\) of 
  the differential equation \(\dot x = f(t, x)\), either \(\lambda(t) = \mu(t)\) 
  or \(\lambda(t) \neq \mu(t)\) for all \(t \in I \cap J\).
\end{lemma}
\proof

Suppose there exists some \(t_0, t_1 \in I \cap J\) such that
\(\lambda(t_0) = \mu(t_0)\) and \(\lambda(t_1) \neq \mu(t_1)\), and
furthermore, Wlog. assume \(t_1 > t_0\). Then, by defining
\[\tilde t := \sup \{t > t_0 \mid \lambda(t') = \mu(t'), 
    \hspace{1mm} \forall t' \in [t_0, t]\},\] by the continuity of
\(\lambda\) and \(\mu\), \(\lambda - \mu\) is continuous, and so, by
considering
\(0 = \lim_{n \to \infty} (\lambda - \mu)(\tilde t - 1 / n) =  (\lambda - \mu)(\tilde t)\),
we have \(\lambda(\tilde t) = \mu(\tilde t)\). Then, by applying the
local version of the Picard-Lindelöf theorem with the initial value
\((\tilde t, \lambda(\tilde t) = \mu(\tilde t))\), we see that there
exists two different solutions \#. \qed

As we have seen, the local version of Picard-Lindelöf provides us with a
unique solution on some interval around the initial value. We would now
like to maximise the size of this interval around the initial value and
such solutions are called maximal solutions.

\begin{definition}[Maximal Existence Interval]
  Given a initial value problem \(\dot x = f(t, x)\), \(x(t_0) = x_0\), 
  we define 
  \[I_+(t_0, x_0) := \sup \{t \ge t_0 \mid \exists \text{ solution on } [t_0, t]\},\]
  \[I_-(t_0, x_0) := \inf \{t \le t_0 \mid \exists \text{ solution on } [t, t_0]\},\]
  and the maximal existence interval,
  \[I_{\max} (t_0, x_0) := (I_-(t_0, x_0), I_+(t_0, x_0)).\]
\end{definition}

\begin{theorem}
  There exists a maximal solution\footnote{Any solution is defined on a subset of
  \(I_{\max}(t_0, x_0)\).} 
  \[\lambda_{\max} : I_{\max} (t_0, x_0) \to \mathbb{R}^d\]
  of the IVP \(\dot x = f(t, x)\), \(x(t_0) = x_0\) where \(f\) is continuous 
  and locally Lipschitz. Furthermore, if \(I_+(t_0, x_0) < \infty\), then 
  either the maximal solution is unbounded for \(t \supseteq t_0\), that is,
  \[\sup_{t \ge t_0}\|\lambda_{\max}(t)\| = \infty,\]
  or \(\partial D \neq \varnothing\), and 
  \[\lim_{t \uparrow I_+(t_0, x_0)} \text{dist}((t, \lambda_{\max}(t)), \partial D) = 0,\]
  where \(\text{dist}(y, A) := \inf\{\|x - y\| \mid x \in A\}\).
  Analogous case for \(I_- (t_0, x_0) > -\infty\).
\end{theorem}
\proof

Let \(\bar{t} \in I_{\max}(t_0, x_0)\), then, by the definition of the
maximal existence interval, there exists some solution
\(\mu : I \to \mathbb{R}^d\) such that \(\bar{t}, t_0 \in I\). Now, by
considering that the solutions cannot cross each other, we see that all
solutions of the IVP on \([t_0, \bar{t}]\) must coincide as they crosses
at \((t_0, x_0)\). Thus, the definition
\[\lambda_{\max}(\bar{t}) := \mu(\bar{t})\] is well-defined. Indeed,
\(\lambda_{\max}\) is a solution at \(\bar{t}\) since
\[\dot \lambda_{\max}(\bar{t}) = \dot \mu(\bar{t}) = f(\bar{t}, \mu(\bar{t})) = 
    f(\bar{t}, \lambda_{\max}(\bar{t})).\] Now, by the construction of
\(I_{\max}\), the only possible case on which the the theorem does not
hold is that \(I_+(t_0, x_0) \in I_{\max}(t_0, x_0)\). So, for
contradiction, assume that there exists some solution
\(\mu : [t, I_{\max}(t_0, x_0)] \to \mathbb{R}^d\) to the IVP. However,
by the local version of Picard-Lindelöf, there exists some open \(U\)
containing \(I_{\max}(t_0, x_0)\) such that there exists a unique
solution \(\eta\) of the IVP on the interval
\((I_{\max}(t_0, x_0) - h, I_{\max}(t_0, x_0) + h)\), and hence, we may
extend \(\mu\) to be a solution on \([t, I_{\max}(t_0, x_0) + h]\)
contradicting the maximum condition \#.

For the second part of the theorem, assume \(I_+(t_0, x_0) < \infty\).
Then, we may find some sequence \((t_n)_{n = 1}^\infty\subseteq D\) that
converges to \(I_-(t_0, x_0)\) and assume, for contradiction, there
exists some \(M > 0\), such that \(\|\lambda_{\max}(t_n)\| \le M\) and
\(\text{dist}((t_n, \lambda_{\max}(t_n)),  \partial D) \ge 1 / M\) for
all \(n \in \mathbb{N}\).

By assumption, we have \((t_n, \lambda_{\max}(t_n))\) is a bounded
sequence, and so, by Bolzano-Weierstrass, it has a convergent
subsequence \((t_{n_i}, \lambda_{\max}(t_{n_i}))\) which converges to
\((t^*, x^*) \in D\). Now, on the problem sheet, we shall see that, the
solution is uniform on a neighbourhood of \((t^*, x^*)\). But, this
contradicts the maximal condition, so we are done! \qed

\hypertarget{general-solution-and-flows}{%
\subsection{General Solution and
Flows}\label{general-solution-and-flows}}

By the existence of the maximal solution, we find there to be a general
solution for all initial pairs.

\begin{prop}
  Let \(f : D \subseteq \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d\) be 
  continuous and locally Lipschitz. Then there exists a maximal solution \(\lambda_{\max}\) 
  to the IVP
  \[\dot x = f(t, x), \hspace{2mm} x(t_0) = x_0\]
  for every initial pair \((t_0, x_0) \in D\). 
\end{prop}
\proof

Follows straight away from the existence of maximal solutions. \qed

In the case as described in the proposition, we can define
\(\lambda : D \times \mathbb{R} \to \mathbb{R}^d\) such that
\(\lambda((t_0, x_0), t)\) is the maximal solution to
\(\dot x = f(t, x)\) and \(x(t_0) = x_0\). This \(\lambda\) is called
the general solution. Commonly, we shall flatten the arguments of
\(\lambda\) such that \(\lambda = \lambda(t, t_0, x_0)\).

\begin{prop}
  Let \(\dot x = f(t, x)\) be a differential equation with the general solution 
  \(\lambda\). Then, given \((t_0, x_0) \in D\),
  \begin{itemize}
    \item \(I_{\max}(s, \lambda(s, t_0, x_0)) = I_{\max}(t_0, x_0)\);
    \item \(\lambda(t_0, t_0, x_0) = x_0\);
    \item \(\lambda(t, s, \lambda(s, t_0, x_0)) = \lambda(t, t_0, x_0)\).
  \end{itemize}
\end{prop}
\proof

The second statement is immediate since the general solution satisfies
the IVP so let us consider the two other claims.

Consider the initial pair \((s. \lambda(s, t_0, x_0))\). Then, we see
that both \(\lambda(\cdot, t_0, x_0)\) and
\(\lambda(\cdot, s, \lambda(s, t_0, x_0))\) coincides at
\((s, \lambda(s, t_0, x_0))\). Now, since the maximal solution is
unique, the results follow. \qed

In the case that the differential equation is autonomous, we find the
the general solution contains redundancy due to translational invariant.
So, this leads us to the concept of \emph{flow}.

Let \(f : D \subseteq \mathbb{R}^d \to \mathbb{R}^d\) be continuous and
locally Lipschitz and suppose \(\lambda\) is a general solution to the
differential equation \(\dot x = f(x)\). Then, if \(t_0 \in \mathbb{R}\)
and \(x_0 \in D\) is fixed, then \(\lambda(\cdot, t_0, x_0)\) solves the
IVP \[\dot x = f(x), \hspace{2mm} x(t_0) = x_0.\] However, consider
\(\lambda(\cdot, 0, x_0)\) is a solution to the IVP is initial condition
\(x(0) = x_0\), we may translate the solution so that
\[\lambda^*(t) = \lambda(t - t_0, 0, x_0),\] which is also a solution to
the IVP with initial condition \(x(t_0) = x_0\). So, since the solution
is unique, we see that
\[\lambda(t - t_0, 0, x_0) = \lambda(t, t_0, x_0).\] So, since the left
hand equation contains only two variable, we conclude that the general
solution has redundant information.

\begin{definition}[Flow]
  Let \(\dot x = f(x)\) with \(f : D \subseteq \mathbb{R}^d \to \mathbb{R}^d\).
  For all \(x_0 \in D\) let us define 
  \[J_{\max}(x_0) := I_{\max}(0, x_0),\]
  and 
  \[\phi(t, x_0) := \lambda(t, 0, x_0),\]
  then \(\phi\) is called the flow of the autonomous differential equation 
  \(\cdot x = f(x)\).
\end{definition}

\begin{prop}
  Let \(\phi\) be a flow of \(\dot x = f(x)\) where \(f : D \subseteq 
  \mathbb{R}^d \to \mathbb{R}^d\) is continuous and locally Lipschitz. Then 
  for \(x \in D\), we have 
  \begin{itemize}
    \item \(J_{\max}(\phi(t, x)) = J_{\max}(x) - t\);
    \item \(\phi(0, x) = x\);
    \item \(\phi(t, \phi(s, x)) = \phi(t + s, x)\);
    \item \(\phi(-t, \phi(t, x)) = x\).
  \end{itemize}
\end{prop}
\proof

Easy as before. \qed

\begin{definition}[Orbit]
  Let \(\phi\) be a flow of \(\dot x = f(x)\) where \(f : D \subseteq \mathbb{R}^d 
  \to \mathbb{R}^d\) is continuous and locally Lipschitz. Then, given \(x \in D\), 
  the orbit (or trajectory) through \(x\) is 
  \[\sigma(x) := \{\phi(t, x) \in D \mid t \in J_{\max}(x)\}.\]
  Furthermore, the positive half-orbit and the negative half-orbit through \(x\) 
  are 
  \[\sigma^+(x) := \{\phi(t, x) \in D \mid t \in J_{\max}(x) \cap \mathbb{R}^+\},\]
  \[\sigma^-(x) := \{\phi(t, x) \in D \mid t \in J_{\max}(x) \cap \mathbb{R}^-\},\]
  respectively.
\end{definition}

We would like to classify the different possible orbits and it turns out
there are three possible orbits.

\begin{itemize}
  \item \(\sigma(x)\) is a singleton. This occurs when \(f(x) = 0\) and 
    \(J_{\max}(x) = \mathbb{R}\) and we call \(x\) an equilibrium of the equation;
  \item \(\sigma(x)\) is a closed curve. That is there exists \(t > 0\) such that 
    \(\phi(t, x) = x\) and \(f(x) \neq 0\). In this case \(J_{\max}(x) = \mathbb{R}\) 
    and we call \(x\) is called a periodic orbit;
  \item \(\sigma(x)\) is not a closed curve nor a singleton. In this case, the 
    solution does not necessarily exist for all time, however, we see that the 
    map \(t \mapsto \phi(t, x)\) is injective.
\end{itemize}

Lastly, let us consider the continuity and differentiability of general
solutions and flows. Indeed, by construction, the general solution and
flow must be differentiable and so, is continuous, however, we would
like to also consider continuity with respect to the other arguments. In
this course, we will state without proof that both the general solution
and flow are continuous functions under the standard assumptions while
this will be further investigated in more advanced modules.

\newpage

\hypertarget{linear-systems}{%
\section{Linear Systems}\label{linear-systems}}

Linear differential equations are important models for certain
applications, but the theory of linear differential equations is of
utmost importance also for non-linear systems.

Consider the differential equation \[\dot x = f(t, x),\] and assume
\(\mu : I \to \mathbb{R}^d\) is a solution. As in perturbation theory,
or the real world, there might be some small deviations from the
theoretical solution and we would like to examine what happens if the
initial condition is a small step away. This can be studied through the
variational equation
\[\dot y = \pdv{f}{x}\hspace{0mm}(t, \mu(t))y = A(t)y,\] which is a
nonautonomous linear system (see problem sheet). In general, the
nonautonomous case if difficult to solve exactly while the situation is
more simple if

\begin{itemize}
  \item \(f\) is autonomous;
  \item \(\mu\) is constant, i.e. \(\mu = x_0\) and \(f(x_0) = 0\).
\end{itemize}

Indeed, if the two conditions above are satisfied, then the variational
equation becomes \(\dot y = Ay\) where \(A = f'(x_0)\).

\hypertarget{autonomous-linear-differential-equations}{%
\subsection{Autonomous Linear Differential
Equations}\label{autonomous-linear-differential-equations}}

For a matrix \(A \in \mathbb{R}^{d \times d}\), consider
\[\dot x = A x.\] Indeed, by considering, for all
\(x, y \in \mathbb{R}^d\),
\[\|Ax - Ay\| = \|A(x - y)\| \le \|A\|_\text{op}\|x - y\|,\] we have
\(x \mapsto A x\) is globally Lipschitz, and so, we may apply the global
version of Picard-Lindelöf resulting in the unique existence of a
globally defined flow
\(\phi : \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d\).

To find this solution, we will again use the Picard iterates. For all
\(t \in [-h, h]\) where \(h = 1 / 2\|A\|_\text{op}\), we define
\(\lambda_0(t) = x_0\) and for \(n \in \mathbb{N}\),
\[\lambda_{n + 1}(t) = x_0 + \int_0^t A \lambda_n(s) \dd s.\]
Inductively, one finds that the closed form of the Picard iterates can
be written as \[\lambda_n(t) = \sum_{k = 0}^n \frac{t^k A^k}{k!} x_0.\]
So, by taking \(n \to \infty\), by the Picard-Lindelöf theorem, we have
for all \(t \in [-h, h]\), the flow \(\phi\) of the differential
equation is
\[\phi(t, x_0) = \sum_{k = 0}^\infty\frac{t^k A^k}{k!}x_0 =: e^{At}\]
and we call it the matrix exponential function. However, we note that we
have not justified that \(e^{At}\) exists and hence well-defined. We
shall prove this now.

\begin{lemma}
  For all \(B, C \in \mathbb{R}^{d \times d}\), \(\|BC\|_\text{op} \le 
  \|B\|_\text{op}\|C\|_\text{op}\).
\end{lemma}
\proof

Let \(x \in \mathbb{R}^d\setminus\{0\}\), then
\[\|BC x \| \le \|B\|_\text{op}\|Cx\| \le \|B\|_\text{op}\|C\|_\text{op} \|x\|,\]
and so, we have
\[\frac{\|BCx\|}{\|x\|} \le \|B\|_\text{op}\|C\|_\text{op}.\] Thus, by
taking the supremum on the left hand side, the inequality is
established. \qed

\begin{prop}
  For \(B \in \mathbb{R}^{d \times d}\), the matrix exponential defined as 
  \[e^{B} := \sum_{k = 0}^\infty \frac{1}{k!} B^l\]
  exists.
\end{prop}
\proof

By the above lemma, through induction, we have
\(\|B^k\|_\text{op} \le \|B\|_\text{op}^k\) for all
\(k \in \mathbb{N}\). Then, consider
\[e^{\|B\|\text{op}} = \sum_{k = 0}^\infty \frac{\|B\|_\text{op}^k}{k!} \ge 
    \sum_{k = 0}^\infty \frac{\|B^k\|_\text{op}}{k!},\] where the right
hand side is a increasing bounded sequence. So,
\[\sum_{k = 0}^\infty \frac{\|B^k\|_\text{op}}{k!}\] exists. Now, by
considering for all \(n > m\), we have
\[\left\|\sum_{k = 0}^n \frac{B^k}{k!} - \sum_{k = 0}^m \frac{B^k}{k!}\right\|_\text{op} 
    = \left\|\sum_{k = m + 1}^n \frac{B^k}{k!}\right\|_\text{op}
    \le \sum_{k = m + 1}^n \frac{\|B^k\|_\text{op}}{k!} \to 0\] by the
trivial test. Thus, \(\sum_{k = 0}^n \frac{B^k}{k!}\) converges as
required. \qed

\begin{theorem}
  Consider the linear differential equation 
  \[\dot x = A x.\]
  Then, the flow of this differential equation is give by 
  \[\phi(t, x) = e^{At} x,\]
  for all \(t \in \mathbb{R}, x \in \mathbb{R}^d\).
\end{theorem}
\proof

As shown above, there exists some \(h > 0\) such that
\(\phi(t, x) = e^{At} x\) for all \(x \in \mathbb{R}^d\) and
\(t \in [-h, h]\). Then, for all \(t, s \in [-h/2, h/2]\),
\(e^{A(t + s)}x = \phi(t + s, x) = \phi(t, \phi(s, x))  = e^{At}e^{As}x\).
Indeed, by algebraic manipulation, we see that this property holds for
all \(t, s \in \mathbb{R}\).

Now, let \(t \in \mathbb{R}\) and some \(N \in \mathbb{N}\) such that
\(t / N \in [-h, h]\). Furthermore, define
\[\phi_{t / N} : \mathbb{R}^d \to \mathbb{R}^d\ : x \mapsto \phi\left(\frac{t}{N}, x\right),\]
and so,
\[\phi(t, x) = (\phi_{t / N} \circ \cdots \circ \phi_{t / N})(x) = 
    \prod_{i = 1}^N e^{\frac{t}{N}A} x = e^{tA}x\] which is exactly what
we wanted to show. \qed

\begin{prop}
  Let \(B, C, T \in \mathbb{R}^{d \times d}\) and suppose \(T\) is invertible. 
  Then, 
  \begin{itemize}
    \item \(C = T^{-1}B T \implies e^C = T^{-1}e^B T\);
    \item \(e^{-B} = (e^B)^{-1}\);
    \item \(BC = CB \implies e^{B + C} = e^B e^C\);
    \item \(B = \bigoplus B_i \implies e^B = \bigoplus e^{B_i}\).
  \end{itemize}
\end{prop}
\proof

See problem sheet. \qed

\hypertarget{planar-linear-systems}{%
\subsection{Planar Linear Systems}\label{planar-linear-systems}}

Consider the linear system \(\dot x = A x\). By decomposing \(A\)
through Jordan normal decomposition, we have \(A = T^{-1} J T\) where
\(J\) is in Jordan normal form. Furthermore, we see that
\(e^{At} = T e^{Jt} T^{-1}\) and so, if we have a explicit form for the
matrix exponential of matrices in Jordan normal form, we would also
obtain a method for calculating \(e^A\) explicitly.

We shall in particular consider the case where
\(A \in \mathbb{R}^{2 \times 2}\). In this case, there are 4 possible
Jordan forms which are

\begin{itemize}
  \item \(J = \begin{pmatrix}
      a & 0 \\ 0 & b
    \end{pmatrix}\) if \(A\) has two distinct eigenvalue \(a, b \in \mathbb{R}\);
  \item \(J = \begin{pmatrix}
      a & 0 \\ 0 & a
    \end{pmatrix}\) if \(A\) has one eigenvalue \(a \in \mathbb{R}\) and \(\dim E_a = 2\);
  \item \(J = \begin{pmatrix}
      a & 1 \\ 0 & a
    \end{pmatrix}\) if \(A\) has one eigenvalue \(a \in \mathbb{R}\) and \(\dim E_a = 1\);
  \item \(J = \begin{pmatrix}
      a & b \\ -b & a
    \end{pmatrix}\) if \(A\) has one eigenvalue \(a + ib \in \mathbb{C}\) and \(b \neq 0\).
\end{itemize}

We remark the last case is the real-version of the Jordan form for
complex eigenvalues. We shall justify this later on.

Suppose \(A\) is not singular, i.e.~0 is not an eigenvalue of \(A\) and
we shall consider the four cases individually.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  \(J = \begin{pmatrix}  a & 0 \\ 0 & b \end{pmatrix}\) for some
  distinct \(a, b \in \mathbb{R}\setminus \{0\}\). In this case, since
  \(J\) is diagonal, we see straight away that
  \[e^{Jt} = \begin{pmatrix}
    e^{at} & 0 \\ 0 & e^{bt}
  \end{pmatrix}.\] By considering the phase portrait (union of all
  orbits) of this system, we have
  \[\gamma(x_0, y_0) := \{\phi(t, (x_0, y_0)) \mid t \in \mathbb{R}\} 
  = e^{Jt}(x_0, y_0)^T = \{(x_0 e^{at}, y_0 e^{bt} \mid t \in \mathbb{R}\}.\]
  By defining \(x := x_0e^{at}\) assuming \(x_0 \neq 0\), we see that
  \(t = \frac{\log(x / x_0)}{a}\) and so, the trajectory becomes,
  \[\gamma(x_0, y_0) = \left\{\left(x, y_0\left(\frac{x}{x_0}\right)^{\frac{b}{a}}
  \right) \mid \frac{x}{x_0} > 0 \right\}.\]
\item
  \(J = \begin{pmatrix}  a & 0 \\ 0 & a \end{pmatrix}\) for some
  \(a \in \mathbb{R}\setminus \{0\}\). This case follows from the first
  resulting in \[e^{Jt} = \begin{pmatrix}
    e^{at} & 0 \\ 0 & e^{at}
  \end{pmatrix},\] and so the phase portrait is, \[\gamma(x_0, y_0) 
  = \left\{\left(x, x\frac{y_0}{x_0}\right) \mid \frac{x}{x_0} > 0\right\}.\]
\item
  \(J = \begin{pmatrix}  a & 1 \\ 0 & a \end{pmatrix}\) for some
  \(a \in \mathbb{R} \setminus\{0\}\). One can check that in this case
  \[e^{Jt} = \begin{pmatrix}
    e^{at} & te^{at} \\ 0 & e^{at}
  \end{pmatrix},\] (see problem sheet). In this case, we have the orbit
  \[\sigma(x_0, y_0) = \left\{
    \left(\frac{x_0}{y_0}y + \frac{y}{a}\log\frac{y}{y_0}, y\right)
    \mid \frac{y}{y_0} > 0 \right\}.\]
\item
  \(J = \begin{pmatrix}  a & b \\ -b & a \end{pmatrix}\) where
  \(a, b \in \mathbb{R}\setminus\{0\}\). In this case, one see that
  \[e^{Jt} = e^{at}\begin{pmatrix}
    \cos(bt) & \sin(bt) \\ -\sin(bt) & \cos(bt)
  \end{pmatrix},\] (see problem sheet). In this case, the motion is
  circular and more details is provided on the lecture slides.
\end{enumerate}

In the case that \(A\) is singular, we have
\(J = \begin{pmatrix}  a & 0 \\ 0 & 0 \end{pmatrix}\) and so,
\[e^{Jt} = \begin{pmatrix}
  e^{at} & 0 \\ 0 & 1
\end{pmatrix},\] resulting in no movements in the \(y\) direction.

By investigating the trajectories qualitatively, we see that for all
linear system \(\dot x = A x\) where \(\rho = a + ib\) is an eigenvalue
of \(A\), we have \(b\) characterises rotation and \(a\) characterises
exponential growth with the growth rate obtained by
\[a = \lim_{t \to \infty} \frac{1}{t} \log \mu(t).\] In this case where
\(\mu(t) = e^{at}\) we have what is known as a pure exponential growth.
There are also non-pure exponential growths such as
\(\mu(t) = t^n e^{at}\), and in this case, we also have the growth rate
\(a\).

\begin{definition}[Lyapunov Exponent]
  Let \(\lambda : \mathbb{R} \to \mathbb{R}^d\) be a non-zero solution to 
  \(\dot x = A x\) where \(A \in \mathbb{R}^{d \times d}\). Then the Lyapunov 
  exponent of \(\lambda\) is given by 
  \[\sigma_\text{Lyap}(\lambda) := \lim_{t \to \infty} \frac{1}{t} \log \|\lambda(t)\|,\]
  if the limit exists. Indeed, one can show that, for a linear system the limit 
  will always exist and equal the real part of one of the eigenvalues.
\end{definition}

We say that \(\lambda\) is exponentially decaying if
\(\sigma_\text{Lyap}(\lambda) < 0\) and exponentially growing if
\(\sigma_\text{Lyap}(\lambda) > 0\).

\end{document}
