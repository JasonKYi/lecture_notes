---
title: Probability for Statistics
author: Kexing Ying
date: May 15, 2020
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
geometry: margin = 1.5in
urlcolor: red
header-includes:
  - \usepackage{tikz}
  - \usepackage{amsthm}
  - \usepackage{mathtools}
  - \usepackage{lipsum}
  - \usepackage[ruled,vlined]{algorithm2e}
  - \newtheorem{theorem}{Theorem}
  - \newtheorem{prop}{Proposition}[theorem]
  - \newtheorem{corollary}{Corollary}[theorem]
  - \newtheorem*{remark}{Remark}
  - \theoremstyle{definition}
  - \newtheorem{definition}{Definition}[section]
  - \newcommand{\cov}{\mathop{\mathrm{Cov}}}
  - \newcommand{\var}{\mathop{\mathrm{Var}}}
---

\newpage
# Measures

## Probability Measures

Last year we saw briefly constructions and definitions relevant to working with 
probabilities such as \(\sigma\)-algebras, random variables and more. We will 
revisit them here with a more general (and more technical) approach.

\begin{definition}[\(\sigma\)-algebra]
  Let \(X\) be a set. A \(\sigma\)-algebra on \(X\), \(\mathcal{A}\) is a 
  collection of subsets of \(X\) such that 
  \begin{itemize}
    \item \(\varnothing \in \mathcal{A}\) 
    \item for all \(A \in \mathcal{A}\), \(A^C \in \mathcal{A}\)
    \item for all \((A_n)_{n = 1}^\infty \subseteq \mathcal{A}\), 
      \(\bigcup_n A_n \in \mathcal{A}\).
  \end{itemize}
\end{definition}

\begin{prop}
  Let \(X\) be a set and \(I\) a non-empty collection of \(\sigma\)-algebras on 
  \(X\). Then \(\bigcap I\) is also a \(\sigma\)-algebra on \(X\).
\end{prop}

This proposition is easy to check and thus, it makes sense to consider the 
\(\sigma\)-algebra generated by some set.

\begin{definition}[Generator of \(\sigma\)-algebra]
  Let \(X\) be a set and \(S \subseteq \mathcal{P}(X)\) a collection of subsets 
  of \(X\). Then the \(\sigma\)-algebra generated by \(S\) is 
  \[
    \sigma(S) := \bigcap \{\mathcal{A} \supseteq S \mid \mathcal{A} 
      \text{ is a \(\sigma\)-algebra on \(X\)} \}
  \]
\end{definition}

By the fact that the power set of \(X\) is a \(\sigma\)-algebra containing 
\(S\), we see that \(\{\mathcal{A} \supseteq S \mid \mathcal{A} 
\text{ is a \(\sigma\)-algebra on \(X\)} \}\) is non-empty and so for all 
\(S \subseteq \mathcal{P}(X)\), \(\sigma(S)\) a (and the smallest) 
\(\sigma\)-algebra on \(X\).

With this, we can construct a commonly seen \(\sigma\)-algebra, the 
Borel \(\sigma\)-algebra. Given some topological space \(X\), the Borel 
\(\sigma\)-algebra on \(X\) is the \(\sigma\)-algebra generated by 
\(\mathcal{T}_X\), i.e. \(\mathcal{B}(X) = \sigma(\mathcal{T}_X)\). 
We will most commonly work with the Borel \(\sigma\)-algebra on the real 
numbers \(\mathcal{B}(\mathbb{R})\).

We call the ordered pair \((X, \mathcal{A})\) where \(\mathcal{A}\) is a 
\(\sigma\)-algebra n \(X\) a *measurable space*.

\begin{definition}[Measure]
  Given a measurable space \((X, \mathcal{A})\), a measure on this measurable 
  space \(\mu : \mathcal{A} \to [0, \infty]\) is a function such that 
  \begin{itemize}
    \item \(\mu(\varnothing) = 0\)
    \item for all disjoint sequence 
      \((A_n)_{n = 1}^\infty \subseteq \mathcal{A}\),
      \(\mu\left(\bigsqcup_n A_n\right) = \sum_n \mu(A_n)\)
  \end{itemize}
\end{definition}

With measures defined, we can add an additional restriction to create a 
*probability space*.

\begin{definition}[Probability Measure]
  Let \(\mu\) be a measure on the measurable space \((X, \mathcal{A})\), then 
  \(\mu\) is a probability measure if and only if \(\mu(X) = 1\). We then call 
  the order triplet \((X, \mathcal{A}, \mu)\) a probability space.
\end{definition}

To distinguish probability space from normal measure spaces, we will often write 
\((\Omega, \mathcal{F}, \mathbb{P})\) to denote a probability space. We will 
call \(\Omega\) the *sample space*, \(\mathcal{F}\) the *events* and for all 
\(A \in \mathcal{F}\), \(\mathbb{P}(A)\) the *probability* of the event \(A\).

### Some Properties of the Probability Measure

\begin{theorem}\label{cont_pmeasure}
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space with 
  \((A_i)_{i = 1}^\infty\) an increasing sequence in \(\mathcal{F}\), then 
  \[\mathbb{P}\left(\bigcup_i A_i\right) = \lim_{i \to \infty} \mathbb{P}(A_i).\]
\end{theorem}
\proof
  Follows from additivity of the probability measure by writing \(\bigcup_i A_i\) 
  as the disjoint union \(A_1 \sqcup \bigsqcup_i (A_{i + 1} \setminus A_i)\).
\qed

A corollary of the above is immediately deduced by considering the complement 
of a decreasing function.

\begin{corollary}
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space with 
  \((A_i)_{i = 1}^\infty\) a decreasing sequence in \(\mathcal{F}\), then 
  \[\mathbb{P}\left(\bigcap_i A_i\right) = \lim_{i \to \infty} \mathbb{P}(A_i).\]
\end{corollary}

In fact the two above propositions apply to general measures with identical 
proofs.

\begin{theorem}
  Suppose \((\Omega, \mathcal{F})\) is a measurable space with the finitely 
  additive function \(\mathbb{P} : \mathcal{F} \to [0, 1]\) such that theorem 
  \ref{cont_pmeasure} holds, then \(\mathbb{P}\) is a probability measure.
\end{theorem}
\proof
  Let \((A_i)_{i = 1}^\infty\) be a sequence of disjoint sequence in 
  \(\mathcal{F}\), then, let us define \(B_n = \bigcup_{i = 1}^n A_i\). As 
  \(\sigma\)-algebras are closed under unions, \(B_n \in \mathcal{F}\) for all 
  \(n\). Now, by assumption, as \((B_n)\) is increasing, 
  \(\mathbb{P}\left(\bigsqcup_i A_i \right)  
  = \mathbb{P}\left(\bigcup_n B_n\right) = \lim_{n \to \infty} \mathbb{P}(B_n)
  = \lim_{n \to \infty} \mathbb{P}\left(\bigcup_{i = 1}^n A_i \right)
  = \lim_{n \to \infty} \sum_{i = 1}^n \mathbb{P}(A_i) 
  = \sum_{i = 1}^\infty \mathbb{P}(A_i)\) where the second to last equality if 
  true by finite additivity.
\qed

## The Lebesgue Measure

As the point of measures in general is to assign sets (in the relevant 
\(\sigma\)-algebra) to some number, it might be useful to take a look at the 
most famous measure of them all -- the Lebesgue measure.

In the easiest terms, the Lebesgue measure is a measure, that maps the interval 
\([a, b] \subseteq \mathbb{R}\) to the real number \(b - a\). In probability, we 
can think of this as \(\mathbb{P}([a, b])\),
or the probability of \(X \in [a, b]\) where \(X\) is a random 
variable with uniform distribution, (we will talk more about what this means in 
the next section).

In this course, we will assume the Lebesgue measure exists (and in fact, is 
unique which we shall prove from first principle in next term's measure theory 
course).

It turns out that a lot of sets are Lebesgue measurable, in fact, the set of 
sets that are Lebesgue measurable is greater than the Borel \(\sigma\)-algebra. 
However, unfortunately, not all sets a Lebesgue measurable. We will give an 
example of a non-Lebesgue measurable set here called the Vitali set. 

\begin{definition}[The Vitali Set]\label{vitali}
  Let \(\Omega := [0, 2\pi)\), then we can some probability measure \(\mathbb{P}\) 
  such that \(\mathbb{P}(s) = \frac{\beta - \alpha}{2\pi}\) corresponding to 
  the Lebesgue measure. Now, let \(\sim\) be the equivalence relation such that 
  \(x \sim y\) if and only if \(x - y\) is a rational multiple of \(2\pi\). 
  As \(\sim\), 
  is an equivalence relation, it partitions \(\Omega\), so there is a set of 
  equivalence classes \(\Omega / \sim\). Now, by using the axiom of choice,
  the Vitali set is defined to be the set \(A\) choosing one element from 
  each equivalence classes in \(\Omega / \sim\).
\end{definition}

\begin{theorem}
  The Vitali Set is not measurable with respect to the measure in 
  theorem \ref{vitali}.
\end{theorem}
\proof
  We suppose for contradiction that the Vitali set is measurable.
  As \(\mathbb{Q}\) is countable, let \(x_1, x_2, \cdots\) be the enumeration of 
  all rational multiples of \(2\pi\) in \([0, 2\pi)\). Now, define 
  \(A_i := A + x_i = \{a + x_i \mid a \in A\}\). We see that \(A_i, A_j\) are 
  disjoint for all \(i \neq j\) since if there exists some 
  \(a \in A + x_i \cap A + x_j\), so there exists \(\alpha, \beta \in A\),
  \(\alpha + x_i = a = \beta + x_j\), so \(\alpha \sim \beta\) implying 
  \(\alpha = \beta\) by the construction of \(A\) and hence, \(x_i = x_j\).
  Now, as \(\Omega = \bigsqcup_{i = 1}^\infty A_i\), we have \(1 
  = \mathbb{P}(\Omega) = \mathbb{P}(\bigsqcup_{i = 1}^\infty A_i) 
  = \sum \mathbb{P}(A_i)\). However, as the Lebesgue measure is transitional 
  invariant, for all \(i, j\), \(\mathbb{P}(A_i) = \mathbb{P}(A_j)\), so 
  \(1 = \sum \mathbb{P}(A_i) = \lim_{i \to \infty} i \mathbb{P}(A_1)\) which 
  results in a contradiction by applying excluded middle on 
  \(\mathbb{P}(A_1) = 0\).
\qed

\newpage
# Random variables

Now that we have the basic notion of a probability space, we would like to play 
around with it using *random variables*. In the most general sense, random 
variables are simply functions from the probability space to another measurable 
space, most commonly the real numbers equipped with \(\mathcal{B}(\mathbb{R})\). 

\begin{definition}[Measurable Functions]
  Let \((X, \mathcal{A})\) and \((Y, \mathcal{B})\) be two measurable spaces and 
  \(f : X \to Y\) a mapping between the two. We call \(f\) measurable if and 
  only if for all \(A \in \mathcal{B}\), \(f^{-1}(A) \in \mathcal{A}\).
\end{definition}

\begin{definition}[Random Variables]
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and 
  \((E, \mathcal{A})\) be a measurable space. Then an \(E\)-valued random variable 
  is a measurable function \(X : \Omega \to E\).
\end{definition}

In general, we will only be working with real valued random variables, so the 
image measurable space is \((\mathbb{R}, \mathcal{B}(\mathbb{R}))\).

Often, when we have a random variable \(X : \Omega \to \mathbb{R}\), we might 
ask questions such as "what is the probability that \(X \in A\)" for some 
\(A \subseteq \text{Im} X\). We now see that this question is asking for exactly 
\(\mathbb{P}(X \in A) = \mathbb{P}(X^{-1}(A))\) (this makes sense as \(X\) is 
measurable).

\begin{theorem}
  If \((\Omega, \mathcal{F}, \mathbb{P})\) is a probability space and 
  \(X : \Omega \to \mathbb{R}\) is a function. Then \(X\) is a 
  \(\mathbb{R}\)-valued random variable if and only if for all 
  \(x \in \mathbb{R}\),
  \[\{\omega \in \Omega \mid X(\omega) \le x\} \in \mathcal{F}.\]
\end{theorem}
\proof
  The forward direction is trivial so let us consider the reverse. 
  Suppose for all \(x \in \mathbb{R}\), 
  \(\{\omega \in \Omega \mid X(\omega) \le x\} = X^{-1}((-\infty, x]) \in \mathcal{F}\). 
  Then, for all \(a, b \in \mathbb{R}\), \(a < b\), 
  \(X^{-1}((-\infty, a]), X^{-1}((-\infty, b]) \in \mathcal{F}\), so 
  \(X^{-1}((-\infty, a])^c = X^{-1}((a, \infty)) \in \mathcal{F}\), and thus, 
  \(X^{-1}((a, \infty)) \cap X^{-1}((-\infty, b]) = X^{-1}((a, b]) \in \mathcal{F}\).
\qed

Let us now consider some properties we can but on these random variables. 

\begin{definition}[Identically Distributed Random Variables]
  Let \(X, Y\) be two real valued random variables. We say \(X\) and \(Y\) are 
  identically distributed if for all \(S \in \mathcal{B}(\mathbb{R})\), 
  \[\mathbb{P}(X \in S) = \mathbb{P}(Y \in S).\]
\end{definition}

We note that two random variables are identically distributed does not imply 
they are equal, that is they are not necessarily the same function. An easy 
example of this if to let \(X, Y\) be the number of heads and tails of \(n\) coin 
flips. We see that \(X, Y\) are identically distributed by symmetry but 
definitely not equal.

Another property that is useful for random variables is the notion of 
independence. 

\begin{definition}[Independence of Events]
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and 
  \((A_n) \subseteq \mathcal{F}\) a sequence of events. Then \((A_n)\) is said 
  to be independent if and only if for all \textit{finite} index set \(I\),
  \[\mathbb{P}\left(\bigcap_{n \in I} A_n \right) 
    = \prod_{n \in I} \mathbb{P}(A_n).\] 
\end{definition}

\begin{definition}[Independence of \(\sigma\)-algebras]
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and 
  \((\mathcal{A}_n)\) be a sequence of sub-\(\sigma\)-algebras of 
  \(\mathcal{F}\). Then \((\mathcal{A}_n)\) is said to be independent if and 
  only if for all \((A_n) \subseteq \mathcal{F}\) a sequence of events such that 
  \(A_i \in \mathcal{A}_i\), \((A_n)\) is independent.
\end{definition}

Equipped with these two notions of independence, it makes sense to create a 
notion of some \(\sigma\)-algebra induced by arbitrary measurable functions and 
with that the notion of independence of random variables is also induced.

\begin{definition}[\(\sigma\)-algebra Generated by Functions]
  Let \(E\) be a set and \(\{f_i : E \to \mathbb{R} \mid i \in I\}\) be an 
  indexed family of real-valued functions. Then the \(\sigma\)-algebra on \(E\) 
  generated by these functions is 
  \[
    \sigma(\{f_i \mid i \in I\}) := 
    \sigma(\{f_i^{-1}(A) \mid A \in \mathcal{B}(\mathbb{R}), i \in I\}).  
  \]
\end{definition}

Note that with this definition, we created the smallest \(\sigma\)-algebra on 
\(E\) such that all \(f_i\) are measurable and for a single function \(f\),
\(\sigma(\{f \mid i \in I\}) = \{f^{-1}(A) \mid A \in \mathcal{B}(\mathbb{R})\}\).

\begin{definition}[Independence of Random Variables]
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and \((X_n)\) 
  be a sequence of real-valued random variables. Then \((X_n)\) is said to be 
  independent if and only if the family of \(\sigma\)-algebras \(\sigma(X_n)\) 
  is independent.
\end{definition}

We will check that this definition of independence of random variables behave as 
intended, that is 
\(\mathbb{P}(X \in A, Y \in B) = \mathbb{P}(X \in A) \mathbb{P}(Y \in B)\).

\begin{theorem}
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and 
  \(X, Y\) be real-valued random variables. Then \(X, Y\) are independent if 
  and only if for all \(A, B \in \mathcal{B}(\mathbb{R})\), 
  \[\mathbb{P}(X \in A, Y \in B) = \mathbb{P}(X \in A) \mathbb{P}(Y \in B).\]
\end{theorem}
\proof
  Recall that \(\mathbb{P}(X \in A, Y \in B) = 
  \mathbb{P}((X \in A) \cap (Y \in B)) = 
  \mathbb{P}(X^{-1}(A) \cap Y^{-1}(B))\). Now, if \(\sigma(X)\) and 
  \(\sigma(Y)\) are independent, as  \(X^{-1}(A) \in \sigma(X)\) and 
  \(Y^{-1}(B) \in \sigma(Y)\), by definition, we have 
  \(\mathbb{P}(X \in A, Y \in B) = \mathbb{P}(X \in A)\mathbb{P}(Y \in B)\).

  Similarly, if the equality in question is true for all 
  \(A, B \in \mathcal{B}(\mathbb{R})\), then the \(\sigma\)-algebras are 
  independent by definition, and thus, so are the random variables.
\qed

## Cumulative Distribution Function

We would now like to take a look at the cumulative distribution function of a 
random variable \(X\). 

\begin{definition}[Cumulative Distribution Function]
  Given a random variable \(X\), the cumulative distribution function, of simply 
  the CDF of \(X\) is 
  \[F_x(x) = \mathbb{P}(X \le x).\]
  This function is well defined as by our previous assertion, \(X\) is measurable 
  on \(\mathcal{B}(\mathbb{R})\) if and only if 
  \(\{\omega \in \Omega \mid X(\omega) \le x\}\) is measurable for all \(x\).
\end{definition}

The CDF of a random variable is important as it characterised the random variable. 
Formally it can be stated as,

\begin{theorem}
  Let \(X, Y\) be real valued random variables. Then \(X, Y\) are identically 
  distributed if and only if \(F_X = F_Y\).
\end{theorem}
\proof
  The forward direction is trivial while the backwards direction follows from the 
  fact that every open real set can be constructed using sets of the form 
  \(\{x \le a | x \in \mathbb{R}\}\) from some \(a \in \mathbb{R}\).
\qed

The CDF of a random variable have some nice properties which we have used 
throughout our first year probability course. 

\begin{prop}
  Given a random variable \(X\), its CDF, \(F_X\), is non-decreasing.
\end{prop}
\proof
  This follows from the fact for all \(x, y \in \mathbb{R}\), if \(x < y\), 
  then we can write \(\{\omega \in \Omega \mid X(\omega) \le y\}
  = \{\omega \in \Omega \mid X(\omega) \le x\} \sqcup 
    \{\omega \in \Omega \mid x < X(\omega) \le y\}\), and so 
  \[F_X(y) = F_X(x) + \mathbb{P}(x < X \le y) \ge F_X(x).\]
\qed

\begin{prop}
  Given a random variable \(X\), \(\lim_{x \to -\infty} F_X(x) = 0\) and 
  \(\lim_{x \to \infty} F_X(x) = 1\).
\end{prop}
\proof
  We recall that the axiom that \(\mathbb{P}(\Omega) = 1\), so it suffices to 
  prove that \(X^{-1}(\lim_{x \to \infty} (-\infty, x])) = \Omega\). But this is 
  trivial as every element of \(\Omega\) is mapped to a real number so we are done.
  (This first claim is true by similar argument.)\footnote{Note that this proof 
  is not technically true since we can't say 
  \(\lim_{x \to \infty} (-\infty, x] = \mathbb{R}\). But this can be fixed by 
  considering any sequence \((x_n)\) that it increasing to \(\infty\).}
\qed

\begin{prop}
  Let \(X\) be a random variable, then 
  \(\lim_{x \downarrow x_0} F_X(x) = F_X(x_0)\).
\end{prop}
\proof
  Similar proof to the previous proposition.
\qed

## Types of Random Variables

The most simple random variable we have is the point mass random variable. 
\begin{definition}
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space, then the 
  point mass random variable \(X_a\) at \(a\) is the function 
  \(X_a : \Omega \to \mathbb{R} : \omega \mapsto a\).
\end{definition}
We can easily see that the point mass random variable has the CDF 
\(\delta_a(x) = \text{if } (x < a) \text{ then } 0, \text{ else } 1\). While 
the point mass random variable in itself is not very interesting, it is the 
building blocks for discrete random variables.

\begin{definition}[Discrete Random Variable]
  A random variable \(X\) is a discrete random variable if and only if there 
  exist sequences \((a_n)_{n \ge 1}\) and \((b_n)_{n \ge 1}\) such that 
  \(\sum b_i = 1\) and \(F_X(x) = \sum b_i \delta_{a_i}(x)\).
\end{definition}

\begin{definition}[Continuous Random Variable]
  A random variable \(X\) is a continuous random variable if and only if \(F_X\) 
  is continuous on \(\mathbb{R}\).
\end{definition}

\begin{definition}[Absolutely Continuous Random Variable]
  A random variable \(X\) is absolutely continuous if and only if there exists 
  some \(f_X : \mathbb{R} \to \mathbb{R}\) such that 
  \(F_X(x) = \int_{-\infty}^x f(t) \text{d}t\).
\end{definition}

We note that continuous random variables need not be absolutely continuous (see 
the Cantor distribution), however, for most purposes, we can assume interchangeability.

\begin{prop}
  Let \(X\) be any random variable and let \(x_n \uparrow x \in \mathbb{R}\), 
  then \(\mathbb{P}(X < x) = \lim_{x_n \uparrow x} \mathbb{P}(X \le x_n)\).
\end{prop}
\proof
  We define \(A_n := \{\omega \in \Omega \mid X(\omega) \le x_n\}\), then 
  \(A_n \uparrow A := \{\omega \in \Omega \mid X(\omega) < x\}\). So, by taking 
  the probability of the limit of \(A_n\), we have 
  \(\lim_{n \to \infty} \mathbb{P}(A_n) = \mathbb{P}(A)\).
\qed

\begin{prop}
  Let \(X\) be a continuous random variable, then \(\mathbb{P}(X = x) = 0\) for 
  all \(x \in \mathbb{R}\).
\end{prop}
\proof
  This follows as the probability measure is continuous.
\qed

While these are the some nicely behaving random variables, often times, random 
variables appears to be neither discrete or continuous. An example of this is to 
consider the random variable \(X\) representing the units of beer an individual 
within the population had consumed today. 

## Transformations of Random Variables

Often times, we might want to work with transformed random variables. This can 
be done in many ways, but the most obvious way is to work with the transformed 
random variable straight away. While this can work in simple cases, we might 
find it is normally easier to work with the transformed CDF instead. But before 
we can discuss the consequences of transforming random variables, we should 
first consider when is a transformed random variable still a random variable.

Recall, by definition, a random variable is a measurable function from the 
measurable set \(\Omega\) to some other measurable set, most often the reals. 
So, for a transformed random variable to also be a random variable, we require 
it to be measurable as well.

\begin{theorem}
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and \(X\) 
  be a real random variable. Then, for all \(g : \mathbb{R} \to \mathbb{R}\) 
  where \(g\) is measurable with respect to \(\mathcal{B}(\mathbb{R})\), 
  \(g(X) := g \circ X\) is a random variable.
\end{theorem}
\proof
  Follows directly from definitions.
\qed

It is in general very hard to construct a non-\(\mathcal{B}(\mathbb{R})\)-measurable 
function (but one example of this is the indicator function of the Vitali set), 
we can regard most transformations of random variables to also be a random 
variable\footnote{For one, continuity implies \(\mathcal{B}(\mathbb{R})\)-measurable.}.

Working with transformed random variables is very simple. Say \(X\) is a real 
random variable and \(g\) is \(\mathcal{B}(\mathbb{R})\)-measurable. Then 
to get the CDF of \(g(X)\) (recall that the CDF characterises the random variable) 
we simply consider \(F_{g(X)}(x) = \mathbb{P}(g(X) \in (-\infty, x]) = 
\mathbb{P}(X \in g^{-1}(-\infty, x])\) which we can obtain using the CDF of \(X\).

\newpage
# Multivariate Random Variables

Recall the definition regarding multivariate distributions from year one and 
we shall in this section consider some of their properties.

\begin{theorem}
  Let \(X_1, \cdots, X_n\) be independent random variables and \(f_1, \cdots, f_n\) 
  are Borel measurable real-valued functions, then \(f_1(X_1), \cdots, f_n(X_n)\) 
  are also independent.  
\end{theorem}
\proof
  Suppose \(B_1, \cdots, B_n \in \mathcal{B}(\mathbb{R})\), then we need to show 
  that \(\mathbb{P}(\bigcap_{i = 1}^n\{f_i(X_i) \in B_i\}) 
  = \prod_{i = 1}^n \mathbb{P}(f_i(X_i) \in B_i)\). By considering by definition 
  \(\{f_i(X_i) \in B_i\} = f_i(X_i)^{-1}(B_i) = X_i^{-1}(f_i^{-1}(B_i))\), we 
  have \(\mathbb{P}(\bigcap_{i = 1}^n\{f_i(X_i) \in B_i\}) = 
  \mathbb{P}(\bigcap_{i = 1}^n X_i^{-1}(f_i^{-1}(B_i))) = 
  \prod_{i = 1}^n \mathbb{P}(X_i^{-1}(f_i^{-1}(B_i))) = 
  \prod_{i = 1}^n \mathbb{P}(f_i(X_i) \in B_i)\) where the third equality is due 
  to the independence of \(X_i\).
\qed

## Covariance and Correlation

While this is a nice theorem on independence of transformed random variables, it 
is also useful to develop some tools to help us determine whether or not two 
random variables are independent. Recall the definition of covariance.

\begin{definition}[Covariance]
  For random variables \(X, Y\), with finite expectations \(\mu_X, \mu_Y\) 
  respectively, the covariance between \(X\) and \(Y\) is defined to be 
  \[\cov(X, Y) = E((X - \mu_X) (Y - \mu_Y)).\]
\end{definition}

By expanding, we see that the covariance between \(X\) and \(Y\) is equivalently 
\[\cov(X, Y) = E(XY) - E(X)E(Y).\]
Furthermore, we see that the covariance is zero for independent random variables, 
however, the reverse is not necessarily true. Indeed, not much can be interpreted 
from this value as \(\cov(X, Y)\) has the same dimension as \(XY\), thus, it does 
not make sense the refer to the covariance as big or small, this is instead the 
role of the correlation.

\begin{definition}[Correlation]
  The correlation of the random variables \(X, Y\) is
  \[\frac{\cov(X, Y)}{\sqrt{\var(X)} \sqrt{\var(Y)}}.\]
\end{definition}

We recall from last year that the correlation is always between \(-1\) and 1, so 
it does make sense to consider the size of the correlation.

From analysis, we recall the definition of a inner product space -- that is a 
vector space equipped with an inner product. By checking the axioms, we find that 
the covariance of random variables forms an inner product over the space of 
random variables.

\begin{remark}
  The previous statement is not necessarily true since \(\cov(X, X) = 0\) for 
  \(X = c\) for some \(c \in \mathbb{R}\); that is the covariance does not 
  satisfy positive definiteness. To fix this, we quotient on the set of random 
  variables with the equivalence relation \(X \sim Y\) if and only if there exists 
  \(c \in \mathbb{R}\), \(\mathbb{P}(X = Y + c) = 1\).
\end{remark}

## Working with Multivariate Random Variables

### Transforming Multivariate Random Variables

Let \(D \subseteq \mathbb{R}^2\) and \(T : D \to \mathbb{R}^2\) be a function 
with range \(R \subseteq \mathbb{R}^2\).
Suppose the partial derivatives of \(T\) exist and are continuous. 
We define the Jacobian of \(T\) is 
\[J(u, v) = \det\begin{bmatrix}
  \frac{\partial{x}}{\partial{u}} & \frac{\partial{x}}{\partial{v}}\\
  \frac{\partial{y}}{\partial{u}} & \frac{\partial{y}}{\partial{v}}
\end{bmatrix}.\]
Then, if \((U, V) = T(X, Y)\)is a function of the pair of random variables 
\((X, Y)\) with joint probability density function \(f_{XY}\), the joint 
pdf of \((U, V)\) is 
\[f_{UV}(u, v) = f_{XY}(x(u, v), y(u, v))\left|J(u, v)\right|.\]

### Conditioning on Multivariate Random Variables

We recall several definitions from last year. The Bayes' theorem for conditioning 
on events states
\[\mathbb{P}(B \mid A) = \frac{\mathbb{P}(B \cap A)}{\mathbb{P}A)}.\]
We can extend this to condition on any random variables with 
\[F_{X \mid A}(x) =\frac{\mathbb{P}(\{X \le x\} \cap A)}{\mathbb{P}(A)},\]
where \(X\) is a random variable. and \(A \in \mathcal{F}\). 
If \(X\) is discrete, we see that 
\[f_{X \mid A}(x) = \mathbb{P}(X = x \mid A),\]
by Bayes', while if \(X\) is absolutely continuous, we define 
\[f_{X \mid A}(x) = \frac{\text{d}}{\text{d}x}F_{X \mid A}(x),\]
resulting 
\[\mathbb{P}(X \in C \mid A) = \int_{C} f_{X \mid A}(x) \text{d}x,\]
for any \(C \in \mathcal{B}(\mathbb{R})\).

We need to be careful when working with conditioned probabilities such as 
\(Y \mid X = x\) as \(\mathbb{P}(X = x) = 0\) for absolutely continuous \(X\) 
and \(Y\). To deal with this, we instead conditioning on 
\(X \in (x, x + \delta)\) for some \(\delta > 0\) and consider the limit 
of \(Y \mid X \in (x, x + \delta)\) as \(\delta \to 0\).
\[\mathbb{P}(Y \le y \mid X = x)
  = \lim_{\delta \to 0}\mathbb{P}(Y \le y \mid X \in (x, x + \delta)) 
  = \lim_{\delta \to 0}
    \frac{\int_x^{x + \delta} \int_{-\infty}^y f_{X, Y}(u, v) \text{d}v \text{d}u}
    {\int_x^{x + \delta} f_X(u)\text{d}u},
\]
then by using l'Hopital's rule, we find 
\[\mathbb{P}(Y \le y \mid X = x) = 
  \frac{\int_{-\infty}^y f_{X, Y}(x, v) \text{d}v}{f_X(x)},\]
and so, by differentiating,
\[f_{Y \mid X}(y \mid x) = \frac{f_{X, Y}(x, y)}{f_X(x)}.\]

When interpreting this probability, we need to be careful as we should never 
by conditioning on events with probability 0 (see the Borel-Kolmogorov paradox in 
the problem sheet).

## Multivariate Normal Distribution 

We shall examine one of the most useful distributions -- the multivariate normal 
distribution.

### Bivariate Normal Distribution

We recall from last year the bivariate normal distribution with PDF given by 
\[
  f(x, y \mid \rho) = \frac{1}{2\pi \sqrt{1 - \rho ^2}}
  \exp\left(-\frac{1}{2(1 - \rho ^2)}(x^2 - 2\rho x y + y^2) \right),
\]
where \((x, y) \in \mathbb{R}^2\) and the parameter \(-1 < \rho < 1\).

Straight away, we see that whenever \(\rho = 0\), the PDF, can be factored into 
two univariate functions. Therefore, by the factorisation theorem, if \(\rho = 0\)
we have \(X, Y\) are independent (this is **not** true in general). 

By completing the square in the exponential, we can compute the marginal density 
of \(X\) and \(Y\). 
\begin{align*}
  f_Y(y) & = \int_{x \in \mathbb{R}} \frac{1}{2\pi \sqrt{1 - \rho ^2}}
      \exp\left(-\frac{1}{2(1 - \rho ^2)}(x^2 - 2\rho x y + y^2) \right) \text{d}x\\
    & = \int_{x \in \mathbb{R}} \frac{1}{2\pi \sqrt{1 - \rho ^2}}
    \exp\left(-\frac{1}{2(1 - \rho ^2)}((x - \rho y)^2 + (1 - \rho ^2)y^2) \right) \text{d}x\\
    & = \frac{1}{\sqrt{2\pi}}\exp(-y^2/ 2) \int_{x \in \mathbb{R}}
    \frac{1}{\sqrt{2\pi}\sqrt{1 - \rho^2}}
    \exp\left(-\frac{(x - \rho y)^2}{2(1 - \rho ^2)} \right) \text{d}x.
\end{align*}
By inspection, we see that the integral within the equation evaluates to 1 as 
its the integral over the support of a univariate normal random variable 
with mean \(\rho y\) and variance \(1 - \rho ^2\). Thus, the marginal density 
of \(Y\) is simply, 
\[f_Y(y) = \frac{1}{\sqrt{2\pi}}\exp(-y^2/ 2).\]
By, symmetry, we also obtain the marginal distribution of \(X\),
\[f_X(x) = \frac{1}{\sqrt{2\pi}}\exp(-x^2/ 2).\]
That is, \(X, Y \sim N(0, 1)\).

While, the initial step of completing the square might seem like a cheap trick, 
it is equivalent to writing \(f(x, y) = f(x \mid y)f(y)\). Thus, we find also 
that \(X \mid Y = y \sim N(\rho y, 1 - \rho ^2)\).

Let us now look at the covariance of the bivariate normal distribution.
\begin{align*}
  E(XY) & = \iint_{\mathbb{R}^2} x y f_{X \mid Y}(x \mid y) f_Y(y)\text{d}x\text{d}y\\
        & = \int_\mathbb{R} y f_Y(y) \int_\mathbb{R} x f_{X \mid Y} (x \mid y)\text{d}x\text{d}y\\
        & = \int_\mathbb{R} y f_Y(y) \rho y \text{d}y\\
        & = \rho E(Y^2) = \rho,
\end{align*} 
where the third equality because \(X \mid Y = y\sim N(\rho y, 1 - \rho ^2)\) 
as previously mentioned, and thus, has an expectation \(\rho y\). Thus, the 
covariance between \(X\) and \(Y\) is 
\[
  \cov(X, Y) = E(XY) - E(X)E(Y) = \rho - 0 = \rho,   
\]
so, in fact, the covariance between \(X\) and \(Y\) is 0 if and only if 
they are independent (**not** true in general). 

In fact, we see this is simply the *law of iterated expectation*,
\[E(XY) = E_Y(E(X \mid Y = y)) = E_Y(\rho YY) = \rho E(Y^2) = \rho.\]

### Multivariate Normal Distribution

We notice that what we have discussed above is the standard bivariate normal 
distribution -- that is, the marginal distributions are standard normal 
distributions. Of course, in the real world, we rarely see just standard normal 
distributions but scaled normal distributions. 
We deal with this similar to how we deal with the 1 dimensional case -- 
transforming it by some affine transformation such that we end up back with 
a standard bivariate normal distribution.

\begin{definition}[Multivariate Normal Distribution]
  The multivariate normal distribution is the probability density function of 
  a vector of normal random variables \(\mathbf{X} = (X_1, X_2, \cdots, X_d)\) 
  where \(X_i \sim N(\mu_i, \sigma_i^2)\) for all \(i = 1,\cdots, d\). Writen 
  out explicitly, we have 
  \[
    f_\mathbf{X}(\mathbf{x} \mid \mathbf{\mu}, \mathbf{\Sigma}) = 
    \frac{1}{\sqrt{(2\pi)^d\left| \mathbf{\Sigma} \right| }}  
    \exp\left(-\frac{1}{2}(\mathbf{x} - \mathbf{\mu})^T \mathbf{\Sigma}^{-1}
    (\mathbf{x} - \mathbf{\mu})\right),
  \]
  where \(\mathbf{\Sigma}\) is the covariance matrix with 
  \(\mathbf{\Sigma}_{i, j} = \cov (X_i, X_j)\) and \(\mathbf{\mu} = 
  (\mu_1, \mu_2, \cdots, \mu_d)\) is the vector of means.
\end{definition}

In the two dimension case, we have \(\mathbf{\mu} = (\mu_X, \mu_Y)\) and 
\[
  \mathbf{\Sigma} = 
  \begin{bmatrix}
    \sigma_X^2 & \rho \sigma_X \sigma_Y \\
    \rho \sigma_X \sigma_Y & \sigma_Y^2
  \end{bmatrix}.
\]

\begin{remark}
  We see that 
  \[
    f_\mathbf{X}(\mathbf{x} \mid \mathbf{\mu}, \mathbf{\Sigma}) \propto 
    \exp\left(-\frac{1}{2}(\mathbf{x} - \mathbf{\mu})^T \mathbf{\Sigma}^{-1}
    (\mathbf{x} - \mathbf{\mu})\right)
  \]
  where the inner term \((\mathbf{x} - \mathbf{\mu})^T \mathbf{\Sigma}^{-1}
  (\mathbf{x} - \mathbf{\mu})\), is in positive definite quadratic form. 
  In fact, we see that this quantity is the inner product 
  \(\langle \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu}), 
  (\mathbf{x} - \mathbf{\mu}) \rangle\) and thus, represents some distance 
  between \(\mathbf{x}\) and \(\mathbf{\mu}\). This is refereed to as 
  Mahalanobis distance.
\end{remark}

\begin{prop}
  The covariance matrix is symmetric, positive definite and has diagonal 
  \(\mathbf{\Sigma}_{i, i} = \sigma_{X_i}\).
\end{prop}
\proof
  The first and last property follows directly from the properties of covariance.

  To show that the covariance matrix is positive definite, we consider that 
  \[\var(a^T X) = \var\left(\sum_{i = 1}^d a_i X_i\right) = 
    \sum_{i = 1}^d a_i^2 \var(X_i) + 2 \sum_{i < j} a_i a_j \cov(X_i, X_j)
    = a^T \mathbf{\Sigma}a \]
for arbitrary \(a \in \mathbb{R}^d\). So, as the variance is non-negative, we have 
\(\mathbf{\Sigma}\) is positive definite.
\qed

Let us now consider the linear transformations of a multivariate normal 
distribution. Let \(\mathbf{X} \sim MVN_d(\mu, \mathbf{\Sigma})\) and 
\(\mathbf{Y} = A \mathbf{X}\) for some \(A \in GL_d(\mathbb{R})\). So, 
\(\mathbf{X} = A^{-1}\mathbf{Y}\) and \(\frac{\partial{X_i}}{\partial{Y_i}} = (A^{-1})_{i, j}\) 
and thus, the Jacobian is simply \(A^{-1}\). Hence, 
\begin{align*}
  f_\mathbf{Y}(\mathbf{y}) & = f_\mathbf{X}(A^{-1}\mathbf{y})\left|A^{-1}\right|\\
  & \propto \exp\left(-\frac{1}{2}(A^{-1}\mathbf{y} - \mu)^T 
    \mathbf{\Sigma}^{-1}(A^{-1}\mathbf{y} - \mu)\right)\\
  & = \exp\left(-\frac{1}{2}(\mathbf{y} - A\mu)^T (A^{-1})^T
    \mathbf{\Sigma}^{-1} A^{-1}(\mathbf{y} - A\mu)\right),
\end{align*}
and thus \(\mathbf{Y} \sim MVN_d(A\mu, A \mathbf{\Sigma}A^T)\).

## Order Statistic

Given a random sample, we often would like to consider the ordering of the sample. 
This suggests we should investigate the joint distribution of ordered samples 
from a distribution. We call the random variables of such statistics the 
*order statistics*. A version of the *order statistic* is the random variable 
\(Y := \max\{X_1, \cdots, X_n\}\) where \(X_1, \cdots, X_n\) is a random sample 
(i.e. independence and identically distributed) from an absolutely continuous 
distribution with CDF \(F_X\) and PDF \(f_X\). 

We see that \(Y\) is a random variable as, by previous results, the composition 
of measurable functions is measurable, so, as \(\max\{X_1, \cdots, X_n\} = 
g \circ f : \Omega \to \mathbb{R}\) where \(f : \Omega \to \mathbb{R}^n : 
\omega \mapsto (X_1(\omega), \cdots, X_n(\omega))\) and \(g : \mathbb{R}^n \to 
\mathbb{R} = \max\), it suffices to show that \(g\) is measurable. Now, it is 
very easy to show that \(g\) is measurable. Let \(a \in \mathbb{R}\),
then, we see that \(g^{-1}(-\infty, a) = \{(x_i) \mid x_i < a\} = (-\infty, a)^n
\in \mathcal{B}(\mathbb{R})^n\) and we are done.

In general, given a random sample \(X_1, \cdots, X_n\), we denote 
\(X_{(k)}\) or \(Y_k\) the \(k\)-th smallest sampled variable and we call the 
the \(k\)-th order statistic.

To see the PDF of the \(k\)-th order statistic, we consider that 
\(X_{(k)} \le y\) for some \(y \in \mathbb{R}\) if and only if there 
are at least \(k\) \(X_i\) which are less than \(y\) (since if otherwise 
\(X_{(k)} > y\)). So, by independence, we have, 
\[F_{X_{(k)}} = \sum_{i = k}^n \binom{n}{i}F_X(X \le y)^i(1 - F_X(X \le y))^{n - i}.\]

Alternatively, by symmetry and independence, we notice that the joint density of 
the order statistics is 
\[f_{X_{(i)}}(y_i) = n! \prod_i f_X(y_i).\]

\newpage
# Convergence of Random Variables

We would sometimes like to consider a sequence of random variables and 
how they behave with respect to some parameter. As random variables are functions, 
one might think to use the same notion of convergence we had learnt during last 
years analysis, that is pointwise convergence and uniform convergence 
of functions. Nonetheless, we shall look at a different notion of convergence 
-- the convergence of probabilities (or more generally -- the convergence of 
functions in measures). 

Suppose we would like to estimate some disease' prevalence within some population. 
Say, we sample some individuals at random, then a simple model can describe this as 
\[X_1, X_2, \cdots, X_n \sim \text{Ber}(p)\]
where \(X_i\) are i.i.d. for all \(i\). From last year, we recall that the maximum 
likelihood estimator for the parameter \(p\) is \(\hat{p} = \bar{\mathbf{x}}\) 
with \(E(\hat{p}) = E\left(\frac{1}{n}\sum X_i \right) = p\) and 
\(\var(\hat{p}) = \var\left(\frac{1}{n} \sum X_i\right) = \frac{1}{n^2} \sum \var(X_i)
= \frac{p(1 - p)}{n}\). Thus \(\hat{p}\) is unbiased and \(\var(\hat{p}) \to 0\) 
as \(n \to \infty\). Heuristically, we can interpret this as the estimator becoming 
more accurate as \(n\) becomes larger and indicates some sort of convergence 
for the estimator. We formalise the above notion with the converge of 
random variables.

## Convergence in Measure

\begin{definition}[Convergence in Measure]
  Let \((X, \mathcal{A}, \mu)\) be a measure space and let
  \((f_i)_{i = 1}^\infty\) be a sequence of measurable functions. Then, 
  \(f_i \to f\) in measure for some measurable function \(f\) if and only if 
  \[\mu(\{x \in X \mid \left| f_n(x) - f(x) \right| \ge \epsilon\}) \to 0,\]
  as \(n \to \infty\) for all \(\epsilon > 0\).
\end{definition}

If \((X, \mathcal{A}, \mu)\) is a probability space then we say \(f_i\) 
converges in probability. That is, given \(X_i\) is sequence of 
random variables, then \(X_i \to X\) for some random variable \(X\) if and only 
if for all \(\epsilon > 0\), 
\(\lim_{n \to \infty}\mathbb{P}(\left|X_n - X\right| \ge \epsilon) = 0\).

This is a weaker notion than pointwise convergence and in fact, if the sequence 
of function converges pointwise almost everywhere (i.e. \(\mu(\{x \in X \mid f_n(x) 
\not\to f(x)\}) = 0\)), then it converges in measure. The proof of this 
follows from the property that \(\liminf \mu(A_n) \ge \mu(\liminf A_n)\) which 
definitions we shall encounter later.

### Weak Law of Large Numbers

We recall the weak law of large numbers from last year. Now that we are equipped 
with the formal definition of what it means to converge in probability, we can 
finally prove it (semi-)formally.

We first recall Markov's and Chebychev's inequality.
\begin{theorem}[Markov's Inequality]
  Let \(X\) be a random variable with \(X(\Omega) \subseteq [0, \infty)\). Then 
  for all \(a \in \mathbb{R}\),
  \[\mathbb{P}(X \ge a) \le \frac{E(X)}{a}.\]
\end{theorem}
\proof
  Let \(A = [a, \infty)\), then for all \(\omega \in \Omega\), 
  \(X(\omega) \ge a\mathbb{I}_A(X(\omega))\) where \(\mathbb{I}_A\) is the indicator 
  of \(A\); this is because if \(X(\omega) < a\), the the right hand side is zero 
  so we are done; on the other hand, if \(X(\omega) \ge 0\), then the right 
  hand side is \(a\) and the inequality is also true. So, by taking the expectation 
  of this inequality on both sides, we have 
  \[E(X) \ge a E(\mathbb{I}_A(X)) = a \mathbb{P}(X \ge a).\]
\qed

\begin{theorem}[Chebychev's Inequality]
  Let \(X\) be a random variable such that \(E(X) = \mu, \var(X) = \sigma ^2 < \infty\).
  Then for any \(\epsilon > 0\),
  \[\mathbb{P}(\left|X - \mu \right| \ge \epsilon) \le \frac{\sigma ^2}{\epsilon^2}.\]
\end{theorem}
\proof
  Let \(Y := (X - \mu)^2\) which is non-negative. So by Markov's inequality, 
  by letting \(a = \epsilon ^2\), we are done!
\qed

With that, the weak law of large number follows straight away.

\begin{theorem}[Weak Law of Large Numbers]
  Let \((X_i)_{i = 1}^\infty\) be a sequence of i.i.d. random variable such that 
  \(E(X_i) = \mu, \var(X_i) = \sigma ^2 < \infty\) for all \(i\). 
  Then \(\bar{X}_n \to \mu\) is probability where \(\bar{X}_n := 
  \frac{1}{n} \sum_{i = 1}^n X_i\).
\end{theorem}
\proof
  By Chebychev's inequality, for all \(\epsilon > 0\)
  \[\mathbb{P}(\left|\bar{X}_n - E(\bar{X}_n) \right| \ge \epsilon) \le 
  \frac{\var(\bar{X}_n)}{\epsilon^2}.\]
  By independence, we have \(E(\bar{X}_n) = \mu\) and 
  \(\var(E(\bar{X}_n)) = \frac{1}{n}\sigma ^2\), so 
  \[\mathbb{P}(\left|\bar{X}_n - \mu \right| \ge \epsilon) \le 
  \frac{\sigma^2}{n\epsilon^2}.\]
  Thus, by taking the limit as \(n \to \infty\), we have \(\bar{X}_n \to \mu\).
\qed

## Convergence in Distribution

Another notion of convergence of random variables is the convergence of random 
variables in distribution.

\begin{definition}[Convergence in Distribution]
  Let \((X_i)_{i = 1}^\infty\) be a sequence of random variables with CDFs 
  \((F_i)_{i = 1}^\infty\) and let \(X\) be a random variable with CDF \(F\). 
  Then, we say \(X_n\) converges to \(X\) in distribution if and only if 
  \[\lim_{n \to \infty}F_n(x) = F_X(x),\]
  at all points \(x \in \mathbb{R}\) where \(F_x\) is continuous. We denote this 
  by \(X_n \xrightarrow[]{\mathcal{D}} X\).
\end{definition}

This notion of convergence is weaker than convergence in probability.

\begin{theorem}
  Let \((X_i)_{i = 1}^\infty\) be a sequence of random variables with CDFs 
  \((F_i)_{i = 1}^\infty\). Suppose \(X_i\) converges to \(X\) in 
  probability where \(X\) is a random variable with CDF \(F_X\), then 
  \(X_i \xrightarrow[]{\mathcal{D}} X\).
\end{theorem}
\proof
  Let \(x \in \mathbb{R}\) such that \(F_X\) is continuous at \(x\) and let 
  \(\epsilon > 0\). By considering
  \[\{X_n \le x\} \subseteq \{X \le x + \epsilon\} 
    \cup \{\left| X_n - X \right| > \epsilon\},\]
  by sub-additivity
  \[\mathbb{P}(X_n \le x) \le \mathbb{P}(X \le x + \epsilon) + 
    \mathbb{P}(\left| X_n - X \right| > \epsilon).\]
  Similarly 
  \[\mathbb{P}(X \le x - \epsilon) \le \mathbb{P}(X_n \le x) + 
    \mathbb{P}(\left| X_n - X \right| > \epsilon).\]
  and so, 
  \[\mathbb{P}(X \le x - \epsilon) - \mathbb{P}(\left| X_n - X \right| > \epsilon)
    \le \mathbb{P}(X_n \le x) \le 
    \mathbb{P}(X \le x - \epsilon) + \mathbb{P}(\left| X_n - X \right| > \epsilon).\]
  Thus, as \(\epsilon \to 0\), we have by squeeze, 
  \(\mathbb{P}(X \le x) = \mathbb{P}(X_n \le x)\), and so 
  \(X_i \xrightarrow[]{\mathcal{D}} X\).
\qed

## Limiting Events

Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and let 
\((A_n)_{n = 1}^\infty\) be a sequence of events in \(\mathcal{F}\). We are often 
interested in considering whether or not \(A_n\) occurs *infinitely often*, 
denoted by \(\{A_n \text{ i.o.}\}\), that is 
\[\omega \in \{A_n \text{ i.o.}\} \iff \forall N \in \mathbb{N}, 
  \exists n \ge N, \omega \in A_n.\]
So, by definition, for all \(n \in \mathbb{N}\), \(\omega \in \{A_n \text{ i.o.}\}\) 
if and only if \(\omega \in \bigcup_{n = N}^\infty A_n\) and so
\[\{A_n \text{ i.o.}\} = \bigcap_{N = 1}^\infty \bigcup_{n = N}^\infty A_n.\]
A closely related is the notion that \(A_n\) occurs *almost always*, 
that is all but finitely many of the \(A_n\) occur, we denote this by 
\(\{A_n \text{ a.a}\}\). Formally, we define this as 
\[\omega \in \{A_n \text{ a.a.}\} \iff \exists N \in \mathbb{N}, 
  \forall n \ge N, \omega \in A_n.\]
Similarly, we can write this as 
\[\{A_n \text{ a.a.}\} = \bigcup_{N = 1}^\infty \bigcap_{n = N}^\infty A_n.\]
Clearly, \(\{A_n \text{ a.a.}\} \subseteq \{A_n \text{ i.o.}\}\), and furthermore, 
by De Morgan's, we see that \(\{A_n \text{ i.o.}\}^c = \{A_n^c \text{ a.a.}\}\).

By recalling the definition of \(\limsup\) and \(\liminf\) of real sequences from 
last year, we see that this is an analogous construction of \(\limsup\) and 
\(\liminf\) of set with respect to the partial order \(\subseteq\). That is, we 
define 
\[\{A_n \text{ i.o.}\} = \limsup_{n \to \infty} A_n = 
  \bigcap_{N = 1}^\infty \bigcup_{n = N}^\infty A_n;\]
\[\{A_n \text{ a.a.}\} = \liminf_{n \to \infty} A_n =
  \bigcup_{N = 1}^\infty \bigcap_{n = N}^\infty A_n.\]
Straight away, we see that \(\limsup_{n \to \infty} A_n\) and 
\(\liminf_{n \to \infty} A_n\) are in \(\mathcal{F}\) since \(\sigma\)-algebras 
are closed under countable union and intersections.

\begin{theorem}[Borel-Cantelli Lemmas]
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and suppose 
  \((A_n)_{n = 1}^\infty\) is a sequence of events in \(\mathcal{F}\). Then, 
  \begin{itemize}
    \item if \(\sum_{n = 1}^\infty \mathbb{P}(A_n) < \infty\), then 
      \(\mathbb{P}(\limsup_{n \to \infty} A_n) = 0\);
    \item if \(\sum_{n = 1}^\infty \mathbb{P}(A_n) = \infty\), and \((A_n)\) is 
      a independent sequence, then \(\mathbb{P}(\limsup_{n \to \infty} A_n) = 1\).
  \end{itemize}
\end{theorem}
\proof
  We provide a proof for the first part while the second part is left as exercise.

  Suppose \(\sum \mathbb{P}(A_n) < \infty\), then 
  \(\sum_{N = n}^\infty \mathbb{P}(A_N) \to 0\) as \(n \to \infty\). Now, as 
  \[\mathbb{P}(\limsup_{n \to \infty} A_n) = \mathbb{P}(\bigcap_{N = 1}^\infty 
    \bigcup_{n = N}^\infty A_n) \le \mathbb{P}(\bigcup_{N = k}^\infty A_n) 
    \le \sum_{n = k}^\infty \mathbb{P}(A_n),\]
  for all \(k \in \mathbb{N}\), we have 
  \[0 \le \mathbb{P}(\limsup_{n \to \infty} A_n) \le \lim_{k \to \infty} 
    \sum_{n = k}^\infty \mathbb{P}(A_n) = 0,\]
  implying the first part of the theorem by the squeeze theorem.
\qed

## Convergence Almost Everywhere

Lastly, we have an even stronger notion of convergence than convergence in 
probability -- convergence almost everywhere. But before, we can define 
this notion, we have to make sure that the event 
\[\{X_n \to X\} := \{\omega \in \Omega \mid X_n(\omega) \to X(\omega)\}\]
is measurable.

\begin{prop}
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and suppose 
  \((X_n)\) is a sequence of random variables. Furthermore, let \(X\) also be 
  a random variable. Then 
  \[\{X_n \to X\} := \{\omega \in \Omega \mid X_n(\omega) \to X(\omega)\} \in \mathcal{F}.\]
\end{prop}
\proof
  Consider for all \(\omega \in \Omega\), \(\omega \in \{X_n \to X\}\) if and only 
  if, for all \(m \in \mathbb{N}\), there exists some \(N(m) \in \mathbb{N}\) such 
  that for all \(n \ge N(m)\),
  \[\left| X_n(\omega) - X(\omega) \right| < \frac{1}{m}.\]
  That is, 
  \[\omega \in \bigcap_{m = 1}^\infty \bigcup_{N = 1}^\infty \bigcap_{n = N}^\infty
    \{\omega \mid \left| X_n(\omega) - X(\omega) \right| < 1 / m \},\]
  which is an event in \(\mathcal{F}\) by previous arguments.
\qed

With that, we can define the notion of convergence almost everywhere.

\begin{definition}[Convergence Almost Everywhere]
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and suppose 
  \((X_n)\) is a sequence of random variables. Furthermore, let \(X\) also be 
  a random variable. Then we say \(X_n \to X\) almost everywhere (or almost 
  surely) if and only if \(\mathbb{P}(\{X_n \to X\}) = 1\). We denote this by 
  \(X_n \xrightarrow[]{a.s.} X\).
\end{definition}

\begin{prop}
  \(X_n \xrightarrow[]{a.s.} X\) implies 
  \(X_n \xrightarrow[]{\mathcal{P}} X\), that is, convergence 
  almost everywhere implies convergence in probability.
\end{prop}
\proof
  For all \(\epsilon > 0\), define the sequence of events, 
  \[A_N := \{\omega \in \Omega \mid \left| X_n(\omega) - X(\omega) \right| < \epsilon, n \ge N\}.\]
  We see that \(A_N\) is increasing and \(\{X_n \to X\} \subseteq \bigcup A_N\), 
  so, 
  \[1 = \mathbb{P}(\{X_n \to X\}) \le \mathbb{P}\left(\bigcup A_N\right) \le 1.\]
  Hence, by continuity, \(\lim_{N \to \infty} \mathbb{P}(A_N) = 
  \mathbb{P}(\bigcup A_N) = 1\). Now as \(A_N \subseteq \{\left| X_N - X \right| < \epsilon\}\),
  we have 
  \[\lim_{n \to \infty} \mathbb{P}(\left| X_n - X \right| < \epsilon) = 1,\]
  which is equivalent to \(X_n \xrightarrow[]{\mathcal{P}} X\).
\qed

### Strong Law of Large Numbers

\begin{theorem}[Strong Law of Large Numbers]
  Let \((X_n)_{n = 1}^\infty\) be a sequence of independent and identically 
  distributed random variables such that \(E(X_n^4) < \infty\) and \(E(X_n) = \mu\) 
  for all \(n\). Then
  \[\mathbb{P}(\bar{X}_n \to \mu) = 1.\]
\end{theorem}
\proof
  Suppose we define \(Z_n := X_n - \mu\), and \(S_n := \sum_{i = 1}^n X_i\) and 
  consider 
  \[E((S_n - n\mu)^4) = E((\sum_{i = 1}^n Z_i)^4) = n E(Z_1^4) + 3n(n - 1)E(Z_1^2Z_2^2),\]
  where the second equality is true as \(E(Z_i) = 0\) for all \(i\) and the all terms 
  but the above two of the sum expansion contains it. Now, by 
  defining \(C := 4 \max \{E(Z_1^4), E(Z_1^2)^2\}\), 
  \[n E(Z_1^4) + 3n(n - 1)E(Z_1^2Z_2^2) = 
  n^2 \left(\frac{C}{4n} + \frac{3C(n - 1)}{4n} \right) \le Cn^2,\]
  and so,
  \[E((\bar{X}_n - \mu)^4) = \frac{1}{n^4}E((S_n - n\mu)^4) \le \frac{C}{n^2}.\]
  Now, let \(A_n := \{\left| \bar{X}_n - \mu \right| \ge n^{-1 / 8}\}\), then by 
  Markov's inequality, 
  \[\mathbb{P}(\left| \bar{X}_n - \mu \right| \ge n^{- 1 / 8}) \le 
    \frac{E(\bar{X}_n - \mu)^4}{n^{-2}} \le Cn^{- 3 / 2}.\]
  Lastly, by taking the sum on both sides, we have \(\sum \mathbb{P}(A_n) < \infty\), 
  and so, by the Borel-Cantelli lemma, \(\mathbb{P}(\{A_n^c \text{ a.a.}\}) = 1\),
  which implies straight away \(\mathbb{P}(\bar{X}_n \to \mu) = 1\).
\qed

\newpage
# Central Limit Theorem

## Moment Generating Functions

We recall the definition of the moment generating functions of a random variable 
from last year.
\begin{definition}[Moment Generating Function]
  Let \(X\) be a random variable, then the moment generating function of \(X\), 
  is the function \(M_X(t) = E(\exp(tX))\). 
\end{definition}
Also, recall some basic properties of the moment generating function.
\begin{prop}
  Let \(X\) be a random variable and suppose there exists some 
  \(a, b \in \mathbb{R}\) such that \(Y = aX + b\). Then \(M_Y(t) = \exp(bt)M_X(at)\)
\end{prop}
\begin{prop}
  Let \(X, Y\) be independent random variables and let \(Z = X + Y\). Then 
  \(M_Z = M_X M_Y\).
\end{prop}
\begin{prop}
  Let \(X\) be a random variable and suppose there exists \(t_0 > 0\) such that 
  for all \(t \in \mathbb{R}, \left| t \right| < t_0\), \(M_X(t) < \infty\), 
  then 
  \[M_X(t) = \sum_{k = 0}^\infty E(X^k) \frac{t^k}{k!},\]
  and for all \(k \ge 0\), 
  \[\frac{d^k}{dt^k}M_X(t)\mid_{t = 0} = E(X^k).\]
\end{prop}
Furthermore, we can show that this moment generating function is unique and 
continuous under certain conditions.
\begin{prop}
  Suppose \(X\) and \(Y\) are random variables with common moment generating 
  function \(M\) which is finite for all \(t \in \mathbb{R}\), 
  \(\left| t \right| < t_0\) for some \(t_0 > 0\). Then \(X\) and \(Y\) are 
  identically distributed.
\end{prop}

\begin{prop}\label{contin}
  Suppose \(X\) is a random variable with moment generating function \(M_X\) and 
  \((X_n)_{n \ge 1}\) is a sequence of random variables, with respective moment 
  generating function \(M_{X_i}(t)\). If \(M_{X_i}(t) \to M_X(t) < \infty\) as 
  \(n \to \infty\) for all \(t \in \mathbb{R}\), \(\left| t \right| < t_0\) for 
  some \(t_0 > 0\), then \(X_n \xrightarrow[]{\mathcal{D}} X\).
\end{prop}

The above proposition provides us with another proof of the weak law of large 
numbers.

\proof (Weak Law of Large Numbers again).
  Let \((X_n)_{n = 1}^\infty\) be a sequence of independent and identically 
  distributed random variables with moment generating function \(M(t)\). Consider 
  the moment generating function of \(\bar{X}_n\), 
  \[M_{\bar{X}_n}(t) = E\left(\exp\left(\frac{t}{n}\sum_{i = 1}^n (X_i)\right)\right) = 
    \prod_{i = 1}^n E\left(\exp\left(\frac{tX_i}{n}\right)\right) 
    = M\left(\frac{t}{n}\right)^n.\]
  Now, by considering the Taylor expansion of \(M(t) = 1 + \mu t + o(t)\), 
  we have 
  \[M_{\bar{X}_n}(t) = \left(1 + \frac{\mu t}{n} + o\left(\frac{t}{n}\right)\right)^n 
    \to \exp(\mu t),\]
  as \(n \to \infty\). We see that this is the moment generating function of a 
  constant random variable, and thus, by proposition \ref{contin}, 
  \(\bar{X}_n \to \mu\) in distribution. Now, as \(\mu\) is constant, we 
  have \(\bar{X}_n \to \mu\) in probability.
\qed

We shall use a similar method when proving the central limit theorem in the next 
section.

## Central Limit Theorem

Let \((X_n)_{n = 1}^\infty\) be a sequence of independent and identically distributed 
random variables with mean \(\mu\) and variance \(\sigma^2\). We would like to 
consider the behaviour of \(\bar{X}_n\) as 
\(n\) becomes large. As we have previously seen, the weak law of large numbers 
dictates that \(\bar{X}_n \to \mu\) in probability as \(n \to \infty\) but this 
is not sufficient for our analysis; instead, we would like to consider the 
behaviour of the region around \(\mu\) as \(n\) becomes large. To achieve this, 
we shall consider the following transformation, 
\[Z_n = \frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}},\]
in which we standardise both the mean and the variance. By empirical experimentation 
we find that as \(n\) becomes large, \(Z_n\) seems to become standard normal. 
This is not a coincidence and we shall formalise this observation with the 
central limit theorem.

\begin{theorem}[The Central Limit Theorem]
  Let \((X_n)_{n = 1}^\infty\) be a sequence of independent and identically distributed 
  random variables with mean \(\mu\) and variance \(\sigma^2 < \infty\). Then, 
  by defining 
  \[Z_n = \frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma},\]
  then, \(Z_n \xrightarrow[]{\mathcal{D}} Z \sim N(0, 1)\).
\end{theorem}
\proof
  Let \(M(t)\) be the moment generating function of \(X_i - \mu\) and furthermore, 
  let \(M_n(t)\) be the moment generating function of \(Z_n\). By expanding the 
  definitions, we have 
  \[M_n(t) = E \left(\exp\left( \frac{t}{\sigma\sqrt{n}} \sum(X_i - \mu) \right)\right) 
    = \prod E \left(\exp\left(\frac{t}{\sigma\sqrt{n}}(X_i - \mu)\right)\right) = 
    M\left(\frac{t}{\sigma\sqrt{n}}\right)^n.\]
  Now, by considering the Taylor expansion of \(M(t)\) at zero, that is 
  \[M(t) = 1 + t M'(0) + \frac{t^2}{2} M''(0) + o(t^2) 
    = 1 + \frac{t^2}{2} \sigma^2 + o(t^2),\]
  we have,
  \[M_n(t) = M\left(\frac{t}{\sigma\sqrt{n}}\right)^n = 
    \left(1 + \frac{t^2}{2n} + o\left(\frac{t^2}{n}\right)\right)^n \to 
    \exp\left(\frac{t^2}{2}\right),\]
  as \(n \to \infty\) resulting in the moment generating function of standard 
  normal random variable. Hence, by proposition \ref{contin}, we have 
  \(Z_n \xrightarrow[]{\mathcal{D}} Z \sim N(0, 1)\).
\qed

\begin{remark}
  We assumed within the proof that the moment generating function of \(X_i\) 
  exists in some open interval around 0. This is in general not true, but the 
  central limit theorem can be proved in the general case using characteristic 
  functions.
\end{remark}

\newpage
# Stochastic Processes

## Markov Chains

\begin{definition}[Stochastic Process]
  A stochastic process on the state space \(\mathcal{E}\) is a collection of 
  \(\mathcal{E}\)-valued random variables \((X_t)_{t \in \mathcal{T}}\) indexed 
  by some set \(\mathcal{T}\).
\end{definition}
We note that we are no longer necessarily using real valued random variables 
and instead, considering random variables on some arbitrary measurable set 
\(\mathcal{E}\). In general, we shall consider stochastic processes on 
discrete time, that is, the index set \(\mathcal{T} = \mathbb{N}\).

\begin{definition}[Markov Chain]
  Let \((X_n)_{n \in \mathbb{N}}\) be a discrete time stochastic process on 
  the state space \(\mathcal{E}\). Then, we say \((X_n)\) is a Markov chain if 
  and only if 
  \[\mathbb{P}\left(X_n = x_n \mid \bigcap_{i \le n} \{X_i = x_i\}\right) = 
    \mathbb{P}(X_n = x_n \mid X_{n - 1} = x_{n - 1}).\]
  That is, the stochastic process is memoryless.
\end{definition}

A consequence of the Markov assumption results in 
\[\mathbb{P}(X_n = z, X_{n - 2} = x \mid X_{n - 1} = y) = 
  \mathbb{P}(X_n = z \mid X_{n - 1 = y})\mathbb{P}(X_{n - 2} = x \mid X_{n - 1} = y).\]
That is, \(X_n\) is conditionally independent of \(X_{n - 2}\) given \(X_{n - 1}\).

\begin{definition}[Time Homegenoeous]
  Let \((X_n)_{n \in \mathbb{N}}\) be a Markov chain on some state space 
  \(\mathcal{E}\). Then, we say \((X_n)\) is time homogeneous if and only if 
  for all \(n \in \mathbb{N}\), \(i, j \in \mathcal{E}\), 
  \[\mathbb{P}(X_{n + 1} = j \mid X_n = i) = \mathbb{P}(X_1 = j \mid X_0 = i).\]
\end{definition}

A time homogeneous Markov chain results in the probability of a particular state 
to be independent of the indexing \(\mathcal{T}\). We shall mostly look at 
time homogeneous Markov chains in this course.

Time homogeneous Markov chains can be represented nicely in matrix called the 
transition matrix.

\begin{definition}[Transition Matrix]
  Let \((X_n)_{n \in \mathbb{N}}\) be a time homogeneous Markov chain and suppose 
  \(p_{ij} = \mathbb{P}(X_1 = j \mid X_0 = i)\) for all \(i, j \in \mathcal{E}\). 
  Then the transition matrix of \((X_n)\) is the matrix \(P\) where 
  \([P]_{ij} = p_{ij}\).
\end{definition}

Since the entries of transition matrix are probabilities, we have \(0 \le p_{ij} \le 1\).
Furthermore, by total probability, we have the sum of each row 
\(\sum_{j \in \mathcal{E}} p_{ij} = 
  \sum_{j \in \mathcal{E}} \mathbb{P}(X_1 = j \mid X_o = i) = 1\).

The transition matrix tells us that, given a state, how the system will change 
in the next time frame. However, this is not sufficient to characterise a whole 
Markov chain as we still do not know how the system began. Thus, to fully specify 
the stochastic process, we will also provide the initial distribution, that is 
\(\lambda = (\lambda_j)_{j \in \mathcal{E}} \in \mathbb{R}^n\) where 
\(\lambda_j = \mathbb{P}(X_0 = j)\) and \(n = \left| \mathcal{E} \right|\). 

Sometimes, however, we are interested in the probability of reaching some 
state given some previous state. 

\begin{definition}
  Let \((X_n)_{n \in \mathbb{N}}\) be a Markov chain with state space \(\mathcal{E}\).
  Then, the \(n\)-step transition matrix \(P(n)\) is the matrix with entries 
  \[p_{ij}(n) = \mathbb{P}(X_n = j \mid X_0 = i).\]
\end{definition}

Clearly, we have \(P(1) = P\) the transition matrix and \(P(0) = I\). In general 
however, the Chapman-Kolmogorov equations provides us explicitly the 
\(n\)-step transition probability. 

\begin{prop}[The Chapman-Kolmogorov Equations]
  Let \((X_n)_{n \in \mathbb{N}}\) be a Markov chain with state space \(\mathcal{E}\). 
  Suppose now that, \(m \ge 0\) and \(n \ge 1\), then 
  \[p_{ij}(m + n) = \sum_{l \in \mathcal{E}}p_{il}(m)p_{lj}(n).\]
  As matrices,
  \[P(m + n) = P(m) P(n).\] 
\end{prop}
From the second part of the proposition, we can deduce that \(P(m) = P^m\).

\proof
  By considering the law of total probability and the properties of time homogeneous 
  Markov chains,
  \begin{align*}
    p_{ij}(n) = \mathbb{P}(X_{m + n} = k \mid X_0 = i) & = 
      \sum_{l \in \mathcal{E}} \mathbb{P}(X_{m + n} = j, X_m = l \mid X_0 = i)\\
    & = \sum_{l \in \mathcal{E}} \mathbb{P}(X_{m + n} = j \mid X_n = l, X_0 = i)
      \mathbb{P}(X_m = l \mid X_0 = i)\\
    & = \sum_{l \in \mathcal{E}} \mathbb{P}(X_{m + n} = j \mid X_n = l)
      \mathbb{P}(X_m = l \mid X_0 = i)\\
    & = \sum_{l \in \mathcal{E}}\mathbb{P}(X_ n = j \mid X_0 = l) 
      \mathbb{P}(X_m = l \mid X_0 = i) = \sum_{l \in \mathcal{E}}p_{il}(m)p_{lj}(n).
  \end{align*}
\qed

Conceptually, the Chapman-Kolmogorov equation is saying that, in order to reach 
\(j\) from \(i\) in \(m + n\) time steps, we must first reach some state \(l\) 
from \(i\) in \(m\) time steps and then reach \(j\) from \(l\) in \(n\) time 
steps. 

## Classification of Chains

Some Markov chains can be predictable in some sense. Consider a random walk 
on the integers \(\mathbb{Z}\), where at state \(i\), there is equal probability 
the state is \(i + 1\) and \(i - 1\) at the next time step. We see that, say if 
we started at \(0\) at time \(0\), then for even \(t\) the state would also be even 
while the state would be odd for odd \(t\). This gives us some information about 
this walk and we need to take this into account when calculating. 

In analysing Markov chains, it is often useful to consider whether or not a state 
is accessible from another, that is, starting at some state \(i\), we would like 
to know whether or not it is possible to reach state \(j\).

\begin{definition}[Accessible]
  Let \(i, j \in \mathcal{E}\) where \(\mathcal{E}\) is some state space. Then 
  we say \(j\) is accessible from \(i\) if and only if there exists \(n \ge 0\) 
  such that \(p_{ij}(n) > 0\). We denote this by \(i \to j\) (and we sometimes 
  call this \(i\) \textit{leads to} \(j\)).
\end{definition}

\begin{definition}[Communicate]
  Let \(i, j \in \mathcal{E}\) where \(\mathcal{E}\) is some state space. Then 
  we say \(i, j\) communicate if and only if \(i \to j\) and \(j \to i\) and 
  we denote this by \(i \leftrightarrow j\).
\end{definition}

\begin{prop}
  Given some state space \(\mathcal{E}\), communication forms an equivalence relation 
  on \(\mathcal{E}\).
\end{prop}
\proof
  Reflexivity are trivial is trivial, so it suffices to prove transitivity. 
  Suppose \(i \leftrightarrow j\) and \(j \leftrightarrow k\) for some 
  \(i, j, k \in \mathcal{E}\), then, there exists some \(n_1, n_2 \ge 0\) such 
  that \(p_{ij}(n_1), p_{jk}(n_2) > 0\). Then, by Chapman-Kolmogorov, 
  \(p_{ik}(n_1 + n_2) = \sum_{l \in \mathcal{E}} p_{il}(n_1) p_{lk}(n_2) \ge 
  p_{ij}(n_1)p_{jk}(n_2) > 0\). So, \(i \to k\) and by a similar argument, 
  we have \(k \to i\) and so \(i \leftrightarrow k\). 
\qed

With that, we see that the state space can be partitioned into *communicating 
classes* such that every element of a class communicates with every other 
element of that class.

\begin{definition}[Closed]
  A communicating class \(C \subseteq \mathcal{E}\) of the state space 
  \(\mathcal{E}\) is closed if and only if \(p_{ij}(n) = 0\) for all \(i \in C\),
  \(j \not\in C\), \(n \in \mathbb{N}\).
\end{definition}

\begin{definition}[Irreducible]
  Let \(\mathcal{E}\) be a state space and let \(S \subseteq \mathcal{E}\). 
  Then, we call \(S\) irreducible if and only if for all \(i, j \in S\), 
  \(i \leftrightarrow j\), that is, there exists a communicating class \(C\) 
  such that \(S \subseteq C\). We call a Markov chain on the state space 
  \(\mathcal{E}\) irreducible if and only if \(\mathcal{E}\) is irreducible 
  (so \(\mathcal{E}\) has only one communicating class).
\end{definition}

Informally, we can think of closed sets of a state space as a set which the 
chain cannot escape from, and furthermore, we can think of a irreducible 
chain as a chain where starting from any position it is possible to reach 
everywhere.

We see that the communicating classes of a state space does not depend on the 
values of \(P\) -- the transition matrix, but only the position of the non-zero 
entries. We call properties with such dependence *structural properties*, so 
communicating classes are a structural property.

\begin{definition}[Period]
  Let \(\mathcal{E}\) be some state space and suppose \(i \in \mathcal{E}\). Then 
  the period of \(i\) is \(d(i) := \gcd \{n > 0 \mid p_{ii}(n) > 0\}\). 
  If \(d(i) = 1\), then we say that \(i\) is aperiodic and periodic if \(d(i) > 1\).
\end{definition}

\begin{prop}
  All states in a communicating class have the same periodicity.
\end{prop}
\proof
  It suffices to show that, given \(i, j \in \mathcal{E}\), 
  \(i \leftrightarrow j\), \(d(i) = d(j)\). Suppose that \(d \mid n\) for all 
  \(n > 0\), \(p_{ii}(n) > 0\), then we will show that whenever \(p_{jj}(m) > 0\),
  \(d \mid m\) for \(m > 0\).

  Since \(i \leftrightarrow j\), there exist \(n_1, n_2 > 0\) such that 
  \(p_{ij}(n_1), p_{ji}(n_2)\). Then, by the Chapman-Kolmogorov equation, 
  \(p_{ii}(n_1 + n_2) = \sum_{l \in \mathcal{E}} p_{il}(n_1)p_{li}(n_2) > 0\), 
  so \(d \mid n_1 + n_2\). Now, let \(m > 0\) such that \(p_{jj}(m) > 0\) following 
  some path \(j \xrightarrow{\gamma} j\). Then, by appending this path, 
  we have \(i \xrightarrow{n_1} j \xrightarrow{\gamma} j \to \xrightarrow{n_2} i\),
  so \(d \mid \gamma + n_1 + n_2\). Therefore, 
  \(d \mid (\gamma + n_1 + n_2) - (n_1 + n_2) = \gamma\) as required.
\qed

A question that is often asked regarding Markov chains is that, starting at some 
position \(i\), what is the probability that we will eventually return to that 
position. To answer this question, let us first introduce some definitions. 

\begin{definition}[Recurrent]
  Let \(\mathcal{E}\) be a state space and let \(i \in \mathcal{E}\). We call 
  \(i\) to be recurrent if and only if 
  \[\mathbb{P}\left(\bigcup_{n = 1}^\infty \{X_n = i\} \mid X_0 = i\right) = 1.\]
\end{definition}

\begin{definition}[Transient]
  Let \(\mathcal{E}\) be a state space and let \(i \in \mathcal{E}\). We call 
  \(i\) to be transient if and only if \(i\) is not recurrent, that is
  \[\mathbb{P}\left(\bigcup_{n = 1}^\infty \{X_n = i\} \mid X_0 = i\right) < 1.\]
\end{definition}

\begin{definition}[First Passage Time]
  The first passage time of some \(j \in \mathcal{E}\) for some state space 
  \(\mathcal{E}\) starting at \(i \in \mathcal{E}\) is 
  \[T_j = \min\{n \ge 1 \mid X_n = j\}.\]
\end{definition}

We note that \(T_j\) is not strictly a random variable (and we call it a 
defective random variable) so whenever we refer to the event 
\(\{T_j = n\}\), we are using it for the shorthand 
\[\{X_n = j\} \cap \bigcap_{m < n} \{X_m \neq j\}. \]
With that in mind, it makes sense to assign a probability to \(T_j\).

\begin{definition}[First Passage Probability]
  The first passage probability of some \(j \in \mathcal{E}\) for some state space 
  \(\mathcal{E}\) starting at \(i \in \mathcal{E}\) is
  \(f_{ij}(n) = \mathbb{P}(T_j = n \mid X_0 = i)\)
  and we denote \(f_{ij} = \mathbb{P}(T_j < \infty \mid X_0 = i)\).
\end{definition}

As for all \(k \neq l\), \(\{T_j = k\}\) and \(\{T_j = l\}\) are disjoint, we find 
that \(f_{ij} = \sum_n f_{ij}(n)\). With this definition in place, we see straight 
away that a state \(i\) is recurrent if and only if \(f_{ii} = 1\) and transient 
if and only if \(f_{ii} < 1\).

\begin{prop}
  Let \(i, j \in \mathcal{E}\) for some state space \(\mathcal{E}\), then 
  for \(n \ge 1\), \(p_{ij}(n) = \sum_{l = 1}^n f_{ij}(l) p_{ii}(n - l)\). 
\end{prop}
\proof
  Consider that \(\{X_n = j\}\) implies that the first passage of time \(T_j \le n\)
  we see that, \(\{X_n = j\} = \bigsqcup_{l = 1}^n\{X_n = j \cap T_j = l\}\). So, 
  by the law of total probability, 
  \begin{align*}
    \mathbb{P}(X_n = j \mid X_0 = i) & = 
      \mathbb{P}\left(\bigsqcup_{l = 1}^n \{X_n = j \cap T_j = l \mid X_0 = i\}\right)
      = \sum_{l = 1}^n \mathbb{P}(X_n = j \cap T_j = l \mid X_0 = i)\\
      & = \sum_{l = 1}^n \mathbb{P}(X_n = j \mid T_j = l \cap X_0 = i) \mathbb{P}(T_j = l \mid X_0 = i).\\
  \end{align*}
  So, by considering that \(\mathbb{P}(X_n = j \mid T_j = l \cap X_0 = i) = 
  \mathbb{P}(X_n = j \mid X_l = j \cap \bigcap_{m < l} X_m \neq j \cap X_0 = i)
  = \mathbb{P}(X_n = j \mid X_l = j)\) by the Markov property, we have
  \[\mathbb{P}(X_n = j \mid X_0 = i) = \sum_{l = 1}^n
    \mathbb{P}(X_n = j \mid X_l = j) 
    \mathbb{P}(T_j = l \mid X_0 = i) = \sum p_{jj}(n - l)f_{ij}(l).\]
\qed

Consider the following notation, \(N_i := \sum_{n = 1}^\infty I_{\{X_n = i\}}\). 
We are interested in finding the expectation of \(N_i\) given the \(X_0 = i\),
that is, the number of times the chain is expected to return back to \(i\). 
By the linearity of expectation, we find that 
\[E(N_i \mid X_0 = i) = \sum_{n = 1}^\infty E(I_{\{X_n = i\}} \mid X_0 = i) 
= \sum p_{ii}(n).\]
Intuitively, if \(i\) is recurrent, we would expect that the chain would keep 
returning to \(i\) resulting in \(E(N_i \mid X_0 = i) = \infty\), while on the 
other hand, if \(i\) is transient, \(N_i \mid X_0 = i\) will be geometrically 
distributed with parameter \(1 - f_{ii}\), and so \(E(N_i \mid X_0 = i) < \infty\).

\begin{theorem}
  Let \(C \subseteq \mathcal{E}\) be a communicating class for some state space 
  \(\mathcal{E}\). Then, for all \(i, j \in C\), either both \(i, j\) are 
  recurrent, or transient.
\end{theorem}
\proof
  Since \(i, j\) are in the same communicating class \(i \leftrightarrow j\) 
  and thus, ther exists \(m, n \ge 0\) such that \(p_{ij}(m), p_{ji}(n) > 0\).
  Now, we see that, for all \(l \ge 0\), the Chapman-Kolmogorov equation 
  gives us 
  \begin{align*}
    p_{jj}(m + l + n) & = \sum_k p_{jl}(m + l) p_{kj}(n)
      \ge p_ji(m + l)p_{ij}(n)\\
      & = p_{ij}(n) \sum_k p_{jk}(m)p_{ki}(l)
      \ge p_{ij}(n) p_{ji}(m) p_{ii}(l).
  \end{align*}
  So, if \(i\) is recurrent, then \(\sum p_{jj}(l) \ge \sum p_{jj}(m + l + n) 
  \ge p_{ij}(n) p_{ji}(m) \sum p_{ii}(l) > \infty\) and so is \(j\) recurrent. 
  Similarly, if \(j\) is transient, the same inequality gives us \(i\) is 
  also transient. Now, as communicating is symmetric, we have the result 
  is therefore true in the reverse direction.
\qed

\begin{prop}
  Let \(C\) be a communicating class such that there exists \(i \in C\) such 
  that \(i\) is recurrent. Then \(C\) is closed. 
\end{prop}
\proof
  Suppose otherwise, then, there exists some \(j \in \mathcal{E} \setminus C\) 
  such that \(p_{ij} > 0\) and so \(i \to j\). Now, as \(j \not\in C\), 
  \(i \not\leftrightarrow j\) and so \(j \not\to i\), and hence, 
  \[\{X_m = j\} \subseteq \bigcap_{n \ge m} \{X_n \neq i\}.\]
  With that, we have \(0 < \mathbb{P}(X_m = j \mid X_0 = i) \le \mathbb{P}
  (\bigcap_{n \ge m} \{X_n \neq i\}) = 0\) \#.
\qed

With that, we can conclude that a state space is uniquely partitioned into 
transient communicating classes and closed irreducible sets.

## Hitting Probabilities

So far, we have classified Markov chains with recurrence and transience which 
indicates whether or not whether or not a state would eventually return. However, 
we would sometimes like to know this information quantitatively, that is, the 
probability of returning to that state and furthermore, the expected duration 
before returning. 

\begin{definition}[Hitting Time]
  If \((X_n)\) is a Markov chain on the state space \(\mathcal{E}\), then the 
  hitting time of a set \(A \subseteq \mathcal{E}\) is the random variable 
  \[H^A = \min \{n \ge 0 \mid X_n \in A\},\]
  that is, the minimum amount of time before we reach \(A\). 
\end{definition}

By convention, we define the minimum of the empty set to by \(\infty\). 

\begin{definition}[Hitting Probability]
  If \((X_n)\) is a Markov chain on the state space \(\mathcal{E}\) and 
  \(i \in \mathcal{E}\), then the hitting probability is defined to be 
  \[h^A_i = \mathbb{P}(H^A < \infty \mid X_0 = i).\]
\end{definition}

Often we will only be interested in the hitting probability of a singular node, 
that is \(h^{\{j\}}_i\) for some \(i, j \in \mathcal{E}\). In this case, for 
shorthand we will simply write \(h^j_i\).

\begin{prop}
  If \(A \subseteq \mathcal{E}\), the vector \(h^A = (h^A_i)_{i \in \mathcal{E}}\) 
  solves the system of linear equations 
  \[h^A_i = \sum_{j \in \mathcal{E}} p_{ij}h_j^A,\]
  for all \(i \not\in A\) (\(h^A_i = 1\) trivially for \(i \in A\)). Moreover, 
  \(h^A\) is the minimal solution, i.e. if \(x = (x_i)_{i \in \mathcal{E}}\) is also 
  a non-negative solution to this system, then \(h^A_i \le x_i\) for all 
  \(i \in \mathcal{E}\).
\end{prop}
\proof
  \(h^A\) satisfies this system follows straight away by the law of total 
  probability.

  To show minimality we will show by induction that 
  \[x_i \ge \mathbb{P}\left(\bigcup_{n = 0}^N \{X_n \in A\} \mid X_0 = i\right),\]
  for all \(N \in \mathbb{N}\). Certainly, this is true for \(n = 0\) so suppose 
  the inequality for \(N = k\). We see that, 
  \begin{align*}
    \mathbb{P}\left(\bigcup_{n = 0}^{k + 1} \{X_n \in A\} \mid X_0 = i \right) 
      & = \sum_{j \in \mathcal{E}} \mathbb{P}\left(\bigcup_{n = 0}^{k + 1} \{X_n \in A\}, X_1 = j \mid X_0 = i \right) \\
      & = \sum_{j \in \mathcal{E}} \mathbb{P}\left(\bigcup_{n = 1}^{k + 1} \{X_n \in A\} \mid X_1 = j\right)p_{ij} \\
      & = \sum_{j \in \mathcal{E}} \mathbb{P}\left(\bigcup_{n = 0}^{k} \{X_n \in A\} \mid X_1 = j\right)p_{ij} 
        \le \sum_{j \in \mathcal{E}}p_{ij} x_j = x_i.
  \end{align*}
  So, the inequality is true for all \(N \in \mathbb{N}\), and hence, by taking 
  the limit, 
  \[x_i \ge \lim_{N \to \infty} \mathbb{P}
    \left(\bigcup_{n = 0}^N \{X_n \in A\} \mid X_0 = i\right) = h^A_i.\]
\qed

## Stationary Distribution

\begin{definition}[Stationary Distribution]
  A vector \(\pi = (\pi_i)_{i \in \mathcal{E}}\) is said to be a stationary 
  distribution for the Markov chain \((X_n)\) if and only if 
  \(\pi_i\) is a probability distribution on \(\mathcal{E}\) (so \(\pi_i \ge 0\) 
  for all \(i \in \mathcal{E}\) and \(\sum_{i \in \mathcal{E}} \pi_i) = 1\)) and 
  \(\pi P = \pi\) where \(P\) is the transition matrix of \((X_n)\).    
\end{definition}

In some literatures, the stationary distribution of a Markov chain is refereed 
as the invariant distribution. The idea of the stationary distribution is that 
it allows us to consider the long run behaviour of a Markov chain. Of course, if 
the distribution of a chain at some time step is the stationary distribution, then 
the distribution of the next time step is also the stationary distribution. 
However, this does not allow us to conclude that the Markov chain in general 
converges to the stationary distribution. 

\begin{definition}[Mean Recurrence Time]
  If \(T_i\) is the random variable corresponding to the first passage time to a 
  state \(i \in \mathcal{E}\), then the mean recurrence time of \(i\) is simply, 
  \[\mu_i = E(T_i \mid X_0 = i) = \sum_{n = 1}^\infty n f_{ii}(n),\]
  if \(i\) is recurrent. The mean recurrence time of a transient node is defined 
  to be \(\infty\).
\end{definition}

\begin{prop}
  An irreducible chain has a stationary distribution if and only if all states 
  are positive recurrent (i.e. the mean recurrence time of all nodes are finite). 
  If that is the case, then \(\pi_i = 1 / \mu_i\) for all 
  \(i \in \mathcal{E}\) where \(\mu_i\) is the mean recurrent time of \(i\).
\end{prop}

With this proposition, we conclude that the stationary distribution of an 
irreducible chain exists and is unique. 

To prove this proposition, we shall use the ergodic theorem which proof is beyond 
the scope of this course. 

\begin{theorem}[The Ergodic Theorem]
  Let \((X_n)\) be a irreducible Markov chain on the state space \(\mathcal{E}\). 
  For all \(i \in \mathcal{E}\), we define 
  \[V_i(n) = \sum_{r = 0}^n I_{\{X_r = i\}},\]
  i.e. the number of visits to \(i\) before time \(n\). Then, for all initial 
  distribution, 
  \[\mathbb{P}\left(\frac{V_i(n)}{n} \to \frac{1}{\mu_i}\right) = 1.\]
  In other words, \(\frac{V_i(n)}{n}\) converges to \(\frac{1}{\mu_i}\) almost 
  surely.
\end{theorem}

Intuitively, the ergodic theorem tells us that the long-rum proportion of time 
in some \(i \in \mathcal{E}\) is \(1 / \mu_i\) which physically makes sense 
since the larger \(\mu_i\) is, the longer period of time it takes before 
returning to \(i\) again, and hence, less time is spend inside \(i\).

With that, given \(i, j \in \mathcal{E}\), consider the proportion of jumps from 
\(i \to j\), that is, the probability of jumping from \(i\) to \(j\) times the 
proportion of time in \(i\), \(p_ij / \mu_i\). Thus, by summing all possible 
\(i \in \mathcal{E}\), the proportion of time in \(j\) is given by 
\(\sum_{i \in \mathcal{E}} \frac{1}{\mu_i} p_{ij} = ((\frac{1}{\mu_i})_i P)_j\) 
and hence, \((\frac{1}{\mu_i})_i\) is a stationary distribution.

\begin{prop}
  If \((X_n)\) is an irreducible, aperiodic Markov chain with stationary distribution 
  \(\pi\), then for any initial distribution \(\lambda\), 
  \[\lim_{n \to \infty} \mathbb{P}(X_n = i) = \pi_i,\]
  for all \(i \in \mathcal{E}\). Furthermore, 
  \[\lim_{n \to \infty} \mathbb{P}(X_n = i \mid X_0 = j) = \pi_i,\]
  and so, the limiting probability is independent of \(j \in \mathcal{E}\).
\end{prop}