% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Probability for Statistics},
  pdfauthor={Kexing Ying},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=red,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin = 1.5in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{lipsum}
\usepackage[ruled,vlined]{algorithm2e}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}[theorem]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\title{Probability for Statistics}
\author{Kexing Ying}
\date{May 15, 2020}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\hypertarget{probability-measures}{%
\subsection{Probability Measures}\label{probability-measures}}

Last year we saw briefly constructions and definitions relevant to
working with probabilities such as \(\sigma\)-algebras, random variables
and more. We will revisit them here with a more general (and more
technical) approach.

\begin{definition}[\(\sigma\)-algebra]
  Let \(X\) be a set. A \(\sigma\)-algebra on \(X\), \(\mathcal{A}\) is a 
  collection of subsets of \(X\) such that 
  \begin{itemize}
    \item \(\varnothing \in \mathcal{A}\) 
    \item for all \(A \in \mathcal{A}\), \(A^C \in \mathcal{A}\)
    \item for all \((A_n)_{n = 1}^\infty \subseteq \mathcal{A}\), 
      \(\bigcup_n A_n \in \mathcal{A}\).
  \end{itemize}
\end{definition}

\begin{prop}
  Let \(X\) be a set and \(I\) a non-empty collection of \(\sigma\)-algebras on 
  \(X\). Then \(\bigcap I\) is also a \(\sigma\)-algebra on \(X\).
\end{prop}

This proposition is easy to check and thus, it makes sense to consider
the \(\sigma\)-algebra generated by some set.

\begin{definition}[Generator of \(\sigma\)-algebra]
  Let \(X\) be a set and \(S \subseteq \mathcal{P}(X)\) a collection of subsets 
  of \(X\). Then the \(\sigma\)-algebra generated by \(S\) is 
  \[
    \sigma(S) := \bigcap \{\mathcal{A} \supseteq S \mid \mathcal{A} 
      \text{ is a \(\sigma\)-algebra on \(X\)} \}
  \]
\end{definition}

By the fact that the power set of \(X\) is a \(\sigma\)-algebra
containing \(S\), we see that
\(\{\mathcal{A} \supseteq S \mid \mathcal{A} \text{ is a \(\sigma\)-algebra on \(X\)} \}\)
is non-empty and so for all \(S \subseteq \mathcal{P}(X)\),
\(\sigma(S)\) a (and the smallest) \(\sigma\)-algebra on \(X\).

With this, we can construct a commonly seen \(\sigma\)-algebra, the
Borel \(\sigma\)-algebra. Given some topological space \(X\), the Borel
\(\sigma\)-algebra on \(X\) is the \(\sigma\)-algebra generated by
\(\mathcal{T}_X\), i.e.~\(\mathcal{B}(X) = \sigma(\mathcal{T}_X)\). We
will most commonly work with the Borel \(\sigma\)-algebra on the real
numbers \(\mathcal{B}(\mathbb{R})\).

We call the ordered pair \((X, \mathcal{A})\) where \(\mathcal{A}\) is a
\(\sigma\)-algebra n \(X\) a \emph{measurable space}.

\begin{definition}[Measure]
  Given a measurable space \((X, \mathcal{A})\), a measure on this measurable 
  space \(\mu : \mathcal{A} \to [0, \infty]\) is a function such that 
  \begin{itemize}
    \item \(\mu(\varnothing) = 0\)
    \item for all disjoint sequence 
      \((A_n)_{n = 1}^\infty \subseteq \mathcal{A}\),
      \(\mu\left(\bigsqcup_n A_n\right) = \sum_n \mu(A_n)\)
  \end{itemize}
\end{definition}

With measures defined, we can add an additional restriction to create a
\emph{probability space}.

\begin{definition}[Probability Measure]
  Let \(\mu\) be a measure on the measurable space \((X, \mathcal{A})\), then 
  \(\mu\) is a probability measure if and only if \(\mu(X) = 1\). We then call 
  the order triplet \((X, \mathcal{A}, \mu)\) a probability space.
\end{definition}

To distinguish probability space from normal measure spaces, we will
often write \((\Omega, \mathcal{F}, \mathbb{P})\) to denote a
probability space. We will call \(\Omega\) the \emph{sample space},
\(\mathcal{F}\) the \emph{events} and for all \(A \in \mathcal{F}\),
\(\mathbb{P}(A)\) the \emph{probability} of the event \(A\).

\hypertarget{some-properties-of-the-probability-measure}{%
\subsubsection{Some Properties of the Probability
Measure}\label{some-properties-of-the-probability-measure}}

\begin{theorem}\label{cont_pmeasure}
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space with 
  \((A_i)_{i = 1}^\infty\) an increasing sequence in \(\mathcal{F}\), then 
  \[\mathbb{P}\left(\bigcup_i A_i\right) = \lim_{i \to \infty} \mathbb{P}(A_i).\]
\end{theorem}
\proof

Follows from additivity of the probability measure by writing
\(\bigcup_i A_i\) as the disjoint union
\(A_1 \sqcup \bigsqcup_i (A_{i + 1} \setminus A_i)\). \qed

A corollary of the above is immediately deduced by considering the
complement of a decreasing function.

\begin{corollary}
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space with 
  \((A_i)_{i = 1}^\infty\) a decreasing sequence in \(\mathcal{F}\), then 
  \[\mathbb{P}\left(\bigcap_i A_i\right) = \lim_{i \to \infty} \mathbb{P}(A_i).\]
\end{corollary}

In fact the two above propositions apply to general measures with
identical proofs.

\begin{theorem}
  Suppose \((\Omega, \mathcal{F})\) is a measurable space with the finitely 
  additive function \(\mathbb{P} : \mathcal{F} \to [0, 1]\) such that theorem 
  \ref{cont_pmeasure} holds, then \(\mathbb{P}\) is a probability measure.
\end{theorem}
\proof

Let \((A_i)_{i = 1}^\infty\) be a sequence of disjoint sequence in
\(\mathcal{F}\), then, let us define \(B_n = \bigcup_{i = 1}^n A_i\). As
\(\sigma\)-algebras are closed under unions, \(B_n \in \mathcal{F}\) for
all \(n\). Now, by assumption, as \((B_n)\) is increasing,
\(\mathbb{P}\left(\bigsqcup_i A_i \right)  = \mathbb{P}\left(\bigcup_n B_n\right) = \lim_{n \to \infty} \mathbb{P}(B_n)  = \lim_{n \to \infty} \mathbb{P}\left(\bigcup_{i = 1}^n A_i \right)  = \lim_{n \to \infty} \sum_{i = 1}^n \mathbb{P}(A_i)  = \sum_{i = 1}^\infty \mathbb{P}(A_i)\)
where the second to last equality if true by finite additivity. \qed

\hypertarget{the-lebesgue-measure}{%
\subsubsection{The Lebesgue Measure}\label{the-lebesgue-measure}}

As the point of measures in general is to assign sets (in the relevant
\(\sigma\)-algebra) to some number, it might be useful to take a look at
the most famous measure of them all -- the Lebesgue measure.

In the easiest terms, the Lebesgue measure is a measure, that maps the
interval \([a, b] \subseteq \mathbb{R}\) to the real number \(b - a\).
In probability, we can think of this as \(\mathbb{P}([a, b])\), or the
probability of \(X \in [a, b]\) where \(X\) is a random variable with
uniform distribution, (we will talk more about what this means in the
next section).

In this course, we will assume the Lebesgue measure exists (and in fact,
is unique which we shall prove from first principle in next term's
measure theory course).

It turns out that a lot of sets are Lebesgue measurable, in fact, the
set of sets that are Lebesgue measurable is greater than the Borel
\(\sigma\)-algebra. However, unfortunately, not all sets a Lebesgue
measurable. We will give an example of a non-Lebesgue measurable set
here called the Vitali set.

\begin{definition}[The Vitali Set]\label{vitali}
  Let \(\Omega := [0, 2\pi)\), then we can some probability measure \(\mathbb{P}\) 
  such that \(\mathbb{P}(s) = \frac{\beta - \alpha}{2\pi}\) corresponding to 
  the Lebesgue measure. Now, let \(\sim\) be the equivalence relation such that 
  \(x \sim y\) if and only if \(x - y\) is a rational multiple of \(2\pi\). 
  As \(\sim\), 
  is an equivalence relation, it partitions \(\Omega\), so there is a set of 
  equivalence classes \(\Omega / \sim\). Now, by using the axiom of choice,
  the Vitali set is defined to be the set \(A\) choosing one element from 
  each equivalence classes in \(\Omega / \sim\).
\end{definition}

\begin{theorem}
  The Vitali Set is not measurable with respect to the measure in 
  theorem \ref{vitali}.
\end{theorem}
\proof

We suppose for contradiction that the Vitali set is measurable. As
\(\mathbb{Q}\) is countable, let \(x_1, x_2, \cdots\) be the enumeration
of all rational multiples of \(2\pi\) in \([0, 2\pi)\). Now, define
\(A_i := A + x_i = \{a + x_i \mid a \in A\}\). We see that \(A_i, A_j\)
are disjoint for all \(i \neq j\) since if there exists some
\(a \in A + x_i \cap A + x_j\), so there exists \(\alpha, \beta \in A\),
\(\alpha + x_i = a = \beta + x_j\), so \(\alpha \sim \beta\) implying
\(\alpha = \beta\) by the construction of \(A\) and hence,
\(x_i = x_j\). Now, as \(\Omega = \bigsqcup_{i = 1}^\infty A_i\), we
have
\(1  = \mathbb{P}(\Omega) = \mathbb{P}(\bigsqcup_{i = 1}^\infty A_i)  = \sum \mathbb{P}(A_i)\).
However, as the Lebesgue measure is transitional invariant, for all
\(i, j\), \(\mathbb{P}(A_i) = \mathbb{P}(A_j)\), so
\(1 = \sum \mathbb{P}(A_i) = \lim_{i \to \infty} i \mathbb{P}(A_1)\)
which results in a contradiction by applying excluded middle on
\(\mathbb{P}(A_1) = 0\). \qed

\hypertarget{random-variables}{%
\subsection{Random variables}\label{random-variables}}

Now that we have the basic notion of a probability space, we would like
to play around with it using \emph{random variables}. In the most
general sense, random variables are simply functions from the
probability space to another measurable space, most commonly the real
numbers equipped with \(\mathcal{B}(\mathbb{R})\).

\begin{definition}[Measurable Functions]
  Let \((X, \mathcal{A})\) and \((Y, \mathcal{B})\) be two measurable spaces and 
  \(f : X \to Y\) a mapping between the two. We call \(f\) measurable if and 
  only if for all \(A \in \mathcal{B}\), \(f^{-1}(A) \in \mathcal{A}\).
\end{definition}

\begin{definition}[Random Variables]
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and 
  \((E, \mathcal{A})\) be a measurable space. Then an \(E\)-valued random variable 
  is a measurable function \(X : \Omega \to E\).
\end{definition}

In general, we will only be working with real valued random variables,
so the image measurable space is
\((\mathbb{R}, \mathcal{B}(\mathbb{R}))\).

Often, when we have a random variable \(X : \Omega \to \mathbb{R}\), we
might ask questions such as ``what is the probability that \(X \in A\)''
for some \(A \subseteq \text{Im} X\). We now see that this question is
asking for exactly \(\mathbb{P}(X \in A) = \mathbb{P}(X^{-1}(A))\) (this
makes sense as \(X\) is measurable).

\begin{theorem}
  If \((\Omega, \mathcal{F}, \mathbb{P})\) is a probability space and 
  \(X : \Omega \to \mathbb{R}\) is a function. Then \(X\) is a 
  \(\mathbb{R}\)-valued random variable if and only if for all 
  \(x \in \mathbb{R}\),
  \[\{\omega \in \Omega \mid X(\omega) \le x\} \in \mathcal{F}.\]
\end{theorem}
\proof

The forward direction is trivial so let us consider the reverse. Suppose
for all \(x \in \mathbb{R}\),
\(\{\omega \in \Omega \mid X(\omega) \le x\} = X^{-1}((-\infty, x]) \in \mathcal{F}\).
Then, for all \(a, b \in \mathbb{R}\), \(a < b\),
\(X^{-1}((-\infty, a]), X^{-1}((-\infty, b]) \in \mathcal{F}\), so
\(X^{-1}((-\infty, a])^c = X^{-1}((a, \infty)) \in \mathcal{F}\), and
thus,
\(X^{-1}((a, \infty)) \cap X^{-1}((-\infty, b]) = X^{-1}((a, b]) \in \mathcal{F}\).
\qed

Let us now consider some properties we can but on these random
variables.

\begin{definition}[Identically Distributed Random Variables]
  Let \(X, Y\) be two real valued random variables. We say \(X\) and \(Y\) are 
  identically distributed if for all \(S \in \mathcal{B}(\mathbb{R})\), 
  \[\mathbb{P}(X \in S) = \mathbb{P}(Y \in S).\]
\end{definition}

We note that two random variables are identically distributed does not
imply they are equal, that is they are not necessarily the same
function. An easy example of this if to let \(X, Y\) be the number of
heads and tails of \(n\) coin flips. We see that \(X, Y\) are
identically distributed by symmetry but definitely not equal.

Another property that is useful for random variables is the notion of
independence.

\begin{definition}[Independence of Events]
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and 
  \((A_n) \subseteq \mathcal{F}\) a sequence of events. Then \((A_n)\) is said 
  to be independent if and only if for all \textit{finite} index set \(I\),
  \[\mathbb{P}\left(\bigcap_{n \in I} A_n \right) 
    = \prod_{n \in I} \mathbb{P}(A_n).\] 
\end{definition}

\begin{definition}[Independence of \(\sigma\)-algebras]
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and 
  \((\mathcal{A}_n)\) be a sequence of sub-\(\sigma\)-algebras of 
  \(\mathcal{F}\). Then \((\mathcal{A}_n)\) is said to be independent if and 
  only if for all \((A_n) \subseteq \mathcal{F}\) a sequence of events such that 
  \(A_i \in \mathcal{A}_i\), \((A_n)\) is independent.
\end{definition}

Equipped with these two notions of independence, it makes sense to
create a notion of some \(\sigma\)-algebra induced by arbitrary
measurable functions and with that the notion of independence of random
variables is also induced.

\begin{definition}[\(\sigma\)-algebra Generated by Functions]
  Let \(E\) be a set and \(\{f_i : E \to \mathbb{R} \mid i \in I\}\) be an 
  indexed family of real-valued functions. Then the \(\sigma\)-algebra on \(E\) 
  generated by these functions is 
  \[
    \sigma(\{f_i \mid i \in I\}) := 
    \sigma(\{f_i^{-1}(A) \mid A \in \mathcal{B}(\mathbb{R}), i \in I\}).  
  \]
\end{definition}

Note that with this definition, we created the smallest
\(\sigma\)-algebra on \(E\) such that all \(f_i\) are measurable and for
a single function \(f\),
\(\sigma(\{f \mid i \in I\}) = \{f^{-1}(A) \mid A \in \mathcal{B}(\mathbb{R})\}\).

\begin{definition}[Independence of Random Variables]
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and \((X_n)\) 
  be a sequence of real-valued random variables. Then \((X_n)\) is said to be 
  independent if and only if the family of \(\sigma\)-algebras \(\sigma(X_n)\) 
  is independent.
\end{definition}

We will check that this definition of independence of random variables
behave as intended, that is
\(\mathbb{P}(X \in A, Y \in B) = \mathbb{P}(X \in A) \mathbb{P}(Y \in B)\).

\begin{theorem}
  Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space and 
  \(X, Y\) be real-valued random variables. THen \(X, Y\) are independent if 
  and only if for all \(A, B \in \mathcal{B}(\mathbb{R})\), 
  \[\mathbb{P}(X \in A, Y \in B) = \mathbb{P}(X \in A) \mathbb{P}(Y \in B).\]
\end{theorem}
\proof

Recall that
\(\mathbb{P}(X \in A, Y \in B) =  \mathbb{P}((X \in A) \cap (Y \in B)) =  \mathbb{P}(X^{-1}(A) \cap Y^{-1}(B))\).
Now, if \(\sigma(X)\) and \(\sigma(Y)\) are independent, as
\(X^{-1}(A) \in \sigma(X)\) and \(Y^{-1}(B) \in \sigma(Y)\), by
definition, we have
\(\mathbb{P}(X \in A, Y \in B) = \mathbb{P}(X \in A)\mathbb{P}(Y \in B)\).

Similarly, if the equality in question is true for all
\(A, B \in \mathcal{B}(\mathbb{R})\), then the \(\sigma\)-algebras are
independent by definition, and thus, so are the random variables. \qed

\end{document}
