---
title: Further Analysis
author: Kexing Ying
date: May 15, 2020
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
geometry: margin = 1.5in
urlcolor: red
header-includes:
  - \usepackage{tikz}
  - \usepackage{amsthm}
  - \usepackage{mathtools}
  - \usepackage{lipsum}
  - \usepackage[ruled,vlined]{algorithm2e}
  - \newtheorem{theorem}{Theorem}
  - \newtheorem{prop}{Proposition}
  - \newtheorem{corollary}{Corollary}[section]
  - \newtheorem*{remark}{Remark}
  - \theoremstyle{definition}
  - \newtheorem{definition}{Definition}[section]
  - \newtheorem{lemma}{Lemma}[section]
  - \newcommand{\diag}{\mathop{\mathrm{diag}}}
  - \newcommand{\hess}{\mathop{\mathrm{Hess}}}
---

# Euclidean Spaces

For \(n \ge 1\), the \(n\)-dimensional *Euclidean space* denoted by 
\(\mathbb{R}^n\), is the set of ordered \(n\)-tuples 
\(\mathbf{x} = (x_1, x_2, \cdots, x_n)\) for \(x_i \in \mathbb{R}\).
Recall that \(\mathbb{R}^n\) is a vector space over \(\mathbb{R}\), we can use 
the usual vector space operations, i.e. vector addition and scalar 
multiplication. Furthermore, \(\mathbb{R}^n\) forms a inner product space with 
the operation, 
\[
  \langle \cdot, \cdot \rangle : \mathbb{R}^n \times \mathbb{R}^n \to 
  \mathbb{R} : (\mathbf{x}, \mathbf{y}) \mapsto \sum_{i = 1}^n x_i y_i.
\]
Thus, as a inner product space induces a normed vector space, we find a natural 
norm defined for \(\mathbb{R}^n\) by,
\[
  \| \cdot \| : \mathbb{R}^n \to \mathbb{R} : \mathbf{x} \mapsto 
  \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle} = \sqrt{\sum_{i = 1}^n x_i^2}.
\]
By manually checking, we find that this norm satisfy the norm axioms, i.e. it 
satisfy the *triangle inequality*, is *absolutely scalable*, and 
*positive definite* (In fact, we do not need the norm to be non-negative as it 
can deduced from the other axioms). 

## Preliminary Concepts in \(\mathbb{R}^n\)

Sequences in \(\mathbb{R}^n\) can be defined similarly to that of \(\mathbb{R}\), 
and we carry over all notations in all suitable places.

\begin{definition}[Convergence in \(\mathbb{R}^n\)]
  We say a sequence \((\mathbf{x}_i)_{i = 1}^\infty \subseteq \mathbb{R}^n\) 
  converges to \(\mathbf{x} \in \mathbb{R}^n\) if and only if for all 
  \(\epsilon > 0\), there exists some \(N \in \mathbb{N}\) such that for all 
  \(i \ge N\), \(\|\mathbf{x}_i - \mathbf{x}\| < \epsilon\).
\end{definition}

\begin{prop}
  A sequence \((\mathbf{x}_i)_{i = 1}^\infty \in \mathbb{R}^n\) converges to 
  \(\mathbf{x} \in \mathbb{R}^n\) if and only if each component of 
  \(\mathbf{x}_i\) converges to the corresponding component of \(\mathbf{x}\).
\end{prop}

In the first dimension, we've considered the topology of \(\mathbb{R}\) 
including the examination of open and closed sets. We extend this idea for 
higher dimensions. The most basic examples we have of an open set 
(or closed set for that matter) in \(\mathbb{R}\) are the open and closed 
intervals respectively. This is extended in \(\mathbb{R}^n\) to be sets of the 
form 
\[
  \prod_{i = 1}^n (a_i, b_i) := \{\mathbf{x} \mid a_i < \mathbf{x}_i < b_i, 
  \forall 1 \le i \le n\},
\]
and similarly for closed intervals. However, while this is nice to look at, it 
is not very useful. So for this reason, we again will extend the notion of open 
and closed sets for \(\mathbb{R}^n\).

\begin{definition} [Open Ball]
  Let \(\mathbf{x} \in \mathbb{R}^n\) and \(r \in \mathbb{R}^+\), we define the 
  open ball of radius \(r\) about \(\mathbf{x}\) as the set 
  \[B_r(\mathbf{x}) := \{\mathbf{y} \in \mathbb{R}^n \mid 
  \| \mathbf{x} - \mathbf{y}\| < r\}\].
\end{definition}
\begin{definition} [Open]
  A set \(U \subseteq \mathbb{  R}^n\) is open in \(\mathbb{R}^n\) if and only if 
  for all \(\mathbf{x} \in U\), there is some \(r \in \mathbb{R}^+\) such that 
  \(B_r(\mathbf{x}) \subseteq U\).
\end{definition}
\begin{definition}[Closed]
  A set \(U \subseteq \mathbb{R}^n\) is closed if and only if its complement is 
  open.
\end{definition}

Straight away from the definition, we can see that every open ball is open (see 
[here](https://github.com/JasonKYi/learn_mspaces/blob/master/src/metric_spaces/basic.lean#L215)).
Furthermore, we find the union and intersection of two open sets is open. In 
fact, the union and any collection of open sets is also open, however, this is 
not necessarily true for closed sets.


\begin{definition} [Continuity at a Point]
  Let \(A \subseteq \mathbb{R}^n\) be an open set, and let 
  \(f : A \to \mathbb{R}^m\). We say \(f\) is continuous at \(p \in A\) if and 
  only if for all \(\epsilon > 0\) there exists \(\delta > 0\) such that for all 
  \(x \in A \cap B_\delta(p)\), \(\| f(x) - f(p) \| < \epsilon\).
\end{definition}

If the function \(f\) is continuous at every point of \(A\), then we say \(f\) 
is continuous on \(A\).

\begin{definition}
  Let \(A \in \mathbb{R}^n\) be open in \(\mathbb{R}^n\) and let 
  \(f : A \to \mathbb{R}^m\). For \(p \in A\), we say that the limit of \(f\) as 
  \(\mathbf{x}\) tends to \(\mathbf{p}\) in \(A\) is equal to 
  \(\mathbf{q} \in \mathbb{R}^m\) if and only if for all \(\epsilon > 0\), there 
  exists \(\delta > 0\) such that for all \(x \in A \cap B_\delta(p), x \neq p\),
  \(\|f(x) - \mathbf{q}\| < \epsilon\).
\end{definition}

This is the same notion we used for continuity in the first dimension to say 
that \(f\) is continuous at \(p\) if and only if \(\lim_{x \to p} f(x) = q\).

\begin{prop}
  Let \(f : \mathbb{R}^n \to \mathbb{R}^m\) be a function. Then \(f\) is 
  continuous if and only if for all open subsets \(U\) of \(\mathbb{R}^m\), 
  \(f^{-1}(U)\) is open in \(\mathbb{R}^n\).
\end{prop}
\proof
  See [here](https://github.com/JasonKYi/learn_mspaces/blob/master/src/metric_spaces/basic.lean#L257) 
  for the proof.
\qed

## Derivative of Maps in Euclidean Spaces

Let \(\Omega\) be a open in \(\mathbb{R}^n\), and \(f : \Omega \to \mathbb{R}^m\) 
be a "nice behaving map". We poses the question on how we should define the 
notion of derivatives for this mapping at some point \(p \in \Omega\). We recall 
that the derivative at a point \(p\) in the first dimension is defined to be 
\[f'(p) = \lim_{x \to p}\frac{f(x) - f(p)}{x - p}.\]
While we see that this equation makes no sense if we simply generalise this 
equation to higher dimensions, we see the following result.

\begin{lemma}
  Let \(f : S \to \mathbb{R}\) where \(S \subseteq \mathbb{R}\), then \(f\) is 
  differentiable at some \(p \in S\) if and only if there exists some 
  \(\lambda \in \mathbb{R}\) such that 
  \[\lim_{x \to p} \left|\frac{f(x) - A_\lambda(x)}{x - p}\right| = 0,\]
  where \(A_\lambda(x) = \lambda(x - p) + f(p)\).
\end{lemma}
\proof
  Follows from algebraic manipulation.
\qed

With this, we can conclude that \(f(x) - A_\lambda(x)\) tends to zero faster 
than \(x - p\). We will generalise this result to higher dimensions.

We may rewrite \(A_\lambda(x) = \lambda(x - p) + f(p) = 
\lambda x + (f(p) + \lambda p)\), so, we see that \(\lambda\) is the translation 
of a linear map \(\lambda x\), i.e. \(A_\lambda = 
(x \mapsto x + (f(p) + \lambda p)) \circ (x \mapsto \lambda x)\). We can easily 
generalise such maps to higher dimensions and we call such maps *affine maps*.

\begin{definition}[Differentiable Functions in \(\mathbb{R}^n\)]
  Recall the definition of linear maps for general vector spaces which we will 
  use in the context of Euclidean spaces. Let \(L(\mathbb{R}^n; \mathbb{R}^m)\) 
  denote the set of linear maps from \(\mathbb{R}^n\) to \(\mathbb{R}^m\), 
  \(\Omega \subseteq \mathbb{R}^n\) be open, and \(f : \Omega \to \mathbb{R}^m\) 
  be a function. Then we say \(f\) is differentiable at some point 
  \(p \in \Omega\) if and only if there exists some 
  \(\Lambda \in L(\mathbb{R}^n; \mathbb{R}^m)\), such that 
  \[ \lim_{x \to p} \frac{\| f(x) - (\Lambda(x - p) + f(p))\|}{\|x - p\|} = 0.
  \]
  If this is true, we write \(Df(p) = \Lambda\) and call \(\Lambda\) the 
  derivative of \(f\) at \(p\).
\end{definition}
\begin{remark}
  Some book refers to \(Df(p)\) as the total derivative of \(f\) at \(p\).
\end{remark}

It is often useful to express the derivative criterion as 
\[\lim_{h \to 0} \frac{\|f(p + h) - f(p) - \Lambda(h)\|}{\| h \|} = 0.\]

\begin{prop}
  Let \(f_i : (a, b) \to \mathbb{R}\) be differentiable for all \(i\), then the 
  function, \(f : (a, b) \to \mathbb{R}^m : x \mapsto (f_i(x))_{i = 1}^m\) is 
  differentiable for all \(p \in (a, b)\).
\end{prop}
\proof
  Let the Jacobian \(\Lambda = \text{diag}(\lambda_i)\) where \(\lambda_i\) is 
  the derivative of \(f_i\) at \(p\). Then I claim, \(Df(p) = \Lambda p\).

  Consider 
  \[\lim_{h \to 0} \frac{\|f(p + h) - f(p) - \Lambda h\|}{\| h \|} = 
  \lim_{h \to 0} \frac{\sqrt{\sum_{i = 0}^m \left|f_i(p + h) - f_i(p) - \lambda_i h\right|}}
  {\| h \|}.\]
  However, for all \(i\), \(\| h \| \ge \left| h_i \right|\), so
  \[ \lim_{h \to 0}
  \frac{\sqrt{\sum_{i = 0}^m \left|f_i(p + h) - f_i(p) - \lambda_i h\right|}}
  {\| h \|} \le \lim_{h \to 0} \sum_{i = 0}^m
  \sqrt{\frac{\left|f_i(p + h) - f_i(p) - \lambda_i h\right|}
  {\left| h_i \right|}} = 0\]
\qed

A lot of results from the first dimension generalises easily to higher dimensions.
Similar to that of the first dimension, the chain rule in general Euclidean spaces states,

\begin{theorem}
  Let \(\Omega \subseteq \mathbb{R}^n\) and \(\Omega' \subseteq \mathbb{R}^m\) 
  be open sets with \(g : \Omega \to \Omega'\) be differentiable at \(p \in \Omega\) 
  and \(f : \Omega' \to \mathbb{R}^l\) be derivatives at \(g(p)\). Then 
  \(h = f \circ g\) is differentiable at \(p\) with derivative 
  \[Dh(p) = Df(g(p)) \circ Dg(p).\] 
\end{theorem}
\proof
  Similar to the proof of the Chain rule in the first dimension using algebraic 
  manipulation.
\qed

Omitted many examples here, check official lecture notes for these examples.

## Directional Derivatives

Although the definition of derivative in dimension one and higher is similar, 
it is different in that we verify whether a linear map is the total derivative 
at a point instead of computing the limit of some equation. This is difficult as 
often times, it is not easy to guess what the derivative of a function is. Thus, 
it is useful to somehow identify candidate linear maps for the derivative.

Assume \(\Omega \subseteq \mathbb{R}^n\) is open and \(f : \Omega \to \mathbb{R}^m\) 
is a differentiable function at some \(p \in \Omega\). Let \(v \in \mathbb{R}^n\) 
be a unit vector. We would like to identify \(Df(p)[v] \in \mathbb{R}^m\).  

By the definition of derivatives in higher dimensions, 
\[\lim_{h \to 0} \frac{\|f(p + h) - f(p) - \Lambda h\|}{\| h \|} = 0. \]
So, by letting \(t \in \mathbb{R}\), we have 
\begin{align*}
  0 & = \lim_{t \to 0} \frac{\|f(p + tv) - f(p) - \Lambda (tv)\|}{\| tv \|}\\
    & = \lim_{t \to 0} \frac{\|f(p + tv) - f(p) - t\Lambda v\|}{\left| t \right|}\\
    & = \lim_{t \to 0} \frac{\|f(p + tv) - f(p)\|}{\left|t\right|} - 
        \Lambda v,\\
\end{align*}
So, \(\lim_{t \to 0} \|f(p + tv) - f(p)\| / \left|t\right| = \Lambda v\). Thus, 
by finding the limits of the above equation for each basis vector \(v \in B\), 
we find the Jacobian \([\Lambda]_B\).
\begin{remark}
  For notation, we denote the limit as 
  \(\lim_{t \to 0} \|f(p + tv) - f(p)\| / \left|t\right|\) as 
  \(\partial f / \partial v \mid_p\) and we call it the directional derivative 
  of \(f\) in the direction of \(v\) at \(p\). We will normally consider the 
  directional derivatives in the direction of the standard basis and we call 
  them the partial derivatives of \(f\) at \(p\).
\end{remark}

\begin{theorem}
  Let \(\Omega \subseteq \mathbb{R}^n\) be open, and 
  \(f : \Omega \to \mathbb{R}^m : x \mapsto [f_i(x)]\) for 
  \(i \in \{1, \cdots m\}\) is differentiable at some \(p \in \Omega\). Then 
  the Jacobian of \(f\) at \(p\) is \([\partial f_i / \partial e_j \mid_p]_{i, j}\).
\end{theorem} 
\proof
  We recall that the Jacobian is simply the matrix form of the linear map that 
  is the derivative. So, for all \(v \in \mathbb{R}^n\), we have 
  \(J v = Df(p)(v)\). As, \(v \in \mathbb{R}^n\), it can be represented as a sum 
  of the standard basis, that is there exists \(v_i \in \mathbb{R}\), 
  \(v = \sum_{i = 1}^n v_i e_i\), so, \(Df(p)(v) = Df(p)(\sum_{i = 1}^n v_i e_i)
  = \sum_{i = 1}^n v_i Df(p)(e_i) 
  = \sum_{i = 1}^n v_i \partial f / \partial e_i \mid_p
  = [\sum_{i = 1}^n v_i \partial f_j / \partial e_i \mid_p]_j
  = [\partial f_i / \partial e_j \mid_p]_{i, j} v\). 
  (We used the fact that \([\partial f / \partial e_i]_j 
  = \partial f_j / \partial e_i\).)
\qed

\begin{remark}
  We note that the reverse is not true, that is the existence of partial 
  derivatives does not imply differentiability. A counter example of this is 
  \(f : \mathbb{R}^2 \to \mathbb{R} : (x, y) \mapsto \text{if } x = y = 0, \text{ then } 0,
  \text{ else}, \frac{xy}{\sqrt{x^2 + y^2}}\).
\end{remark}

\begin{theorem}\label{strong}
  Let \(\Omega \subseteq \mathbb{R}^n\) is open, and \(f : \Omega \to \mathbb{R}\) 
  be a function. Suppose that the partial derivatives of \(f\), \(D_i f(x)\) exists 
  for all \(i = 1 \cdots n\) exists at all \(x \in \Omega\).  Furthermore, 
  if the map \(x \mapsto D_i f(x)\) is continuous for all \(i\) at some point 
  \(p\). Then \(f\) is differentiable at \(p\).
\end{theorem}

## Higher Derivatives

Similar to the first dimension, we would like to think about how to differentiate 
more than once. 

Let \(\Omega \subseteq \mathbb{R}^n\) is open, and \(f : \Omega \to \mathbb{R}^m\) 
be differentiable everywhere on \(\Omega\). Then we may consider the map 
\[Df : \Omega \to \mathcal{L}(\mathbb{R}^n; \mathbb{R}^m) : p \mapsto Df(p).\]
As there is a trivial isomorphism between \(\mathcal{L}(\mathbb{R}^n; \mathbb{R}^m)\) 
and the matrices of dimension \(m \times n\), we can represent every linear map 
from \(\mathbb{R}^n\) to \(\mathbb{R}^m\) as an element of \(\mathbb{R}^{m \times n}\).
Thus, \(Df\) can be represented as a map from \(\Omega\) to \(\mathbb{R}^{m \times n}\).
So, we may ask if \(Df\) is continuous at some \(p\) and furthermore, we can 
ask if \(Df\) is differentiable at some \(p \in \Omega\). If this is the case, 
we have the second derivative
\[DDf : \Omega \to \mathcal{L}(\mathbb{R}^n; \mathbb{R}^{m \times n}).\]

\begin{definition}[Second Derivative]
  Let \(\Omega \subseteq \mathbb{R}^n\) be open and \(f : \Omega \to \mathbb{R}^m\) 
  be differentiable everywhere on \(\Omega\) with derivative 
  \(Df : \Omega \to \mathcal{L}(\mathbb{R}^n; \mathbb{R}^m)\). Then the second 
  derivative of \(f\) at some \(p \in \Omega\) is the linear map 
  \(\Lambda \in \mathcal{L}(\mathbb{R}^n; \mathbb{R}^{m \times n})\) such that 
  \[\lim_{x \to p} \frac{\| Df(x) - Df(p) - \Lambda(x - p) \|}{\| x - p\|} = 0.\]
\end{definition}

Thus, with this definition, we can easily extend the notion of derivatives any 
number of times to get the \(k\)-th derivative of a function. However, this is 
formally difficult and requires the notion of multilinear maps. Luckily, instead 
of working with this difficult definition whenever we would like to work with 
higher derivatives, we can instead look at whether the \(k\)-th derivative 
exists and whether or not it is continuous by theorem \ref{strong}.

Now that we have established the notion of higher derivatives we would like to 
ask how higher partial derivatives interacts. That is, when does 
\(D_i D_j f(p) = D_j D_i f(p)\)? 

\begin{theorem}[Schwartz' Theorem]
  Let \(\Omega \subseteq \mathbb{R}^n\) be open and \(f : \Omega \to \mathbb{R}\) 
  be differentiable at every \(p \in \Omega\). Suppose that for some 
  \(i, j \in \{1, \cdots, n\}\) the second partial derivatives \(D_i D_j f\) and 
  \(D_j D_i f\) exists and is continuous for all \(p \in \Omega\), then 
  \[D_i D_j f(p) = D_j D_i f(p)\]
  for all \(p \in \Omega\).
\end{theorem}

If \(f : \Omega \to \mathbb{R}\), we call the matrix of second partial derivatives 
of \(f\) at some point \(p\) the **Hessian** of \(f\) at \(p\) and we write 
\(\hess f(p) = [D_i D_j f(p)]_{i, j = 1, \cdots, n}\). Given that the hypothesis 
of Schwartz's theorem holds, we find that \([\hess f(p)]_{i, j} = [\hess f(p)]_{j, i}\) 
so the Hessian is symmetric.

## Taylor's Theorem

We recall that the first derivative of a map \(f : \Omega \to \mathbb{R}^m\) allows 
us to find an affine map at some point \(p \in \Omega\) such that the error 
decreases faster than that of \(\| x - p\|\). The existence of higher derivatives 
allows us to obtain better estimates with error decreasing even faster.

Let us first introduce some notations. We define a multi-index \(\alpha\) an 
element of \(\mathbb{N}^n\) and we write \(\left| \alpha \right| = \sum \alpha_i\).
Furthermore, given some function \(f : \Omega \to \mathbb{R}^n\), we write 
\[D^\alpha f := (D_1)^{\alpha_1}\cdots(D_n)^{\alpha_n} f,\]
and given \(h \in \mathbb{R}^n\), we write 
\[h^\alpha := h_1 ^ {\alpha_1} \cdots h_n^{\alpha_n}.\]
Lastly, we write \(\alpha! := \alpha_1! \cdots \alpha_n!\).

With that, we can state the Taylor's theorem.

\begin{theorem}[Taylor's Theorem]
  Suppose \(p \in \mathbb{R}^n\) and \(f : B_r(p) \to \mathbb{R}\) is \(k\)-times
  differentiable on \(B_r(p)\) for some \(r > 0\) and \(k \ge 1\). Then for any 
  \(h \in \mathbb{R}^n\) with \(\|h\| < r\), we have,
  \[
    f(p + h) = \sum_{\substack{\alpha \in \mathbb{N}^n,\\ \left|\alpha\right| \le k - 1}} 
      D^\alpha f(p) \frac{h^\alpha}{\alpha!} + R_k(p, h),
  \]
  where there exists some \(x \in \mathbb{R}^n\) with \(0 < \| x - p\| < \| h \|\)
  such that 
  \[
    R_k(p, h) = \sum_{\substack{\alpha \in \mathbb{N}^n \\ \left| \alpha \right| = k}} 
      D^\alpha f(x)\frac{h^\alpha}{\alpha!}.
  \]
\end{theorem}
\proof
  See one dimensional version from year one. (Essentially boils down to finding 
  the Taylor expansion of the restriction of \(f\) on
  \(\{p + th \mid t \in \mathbb{R}\} \cap B_r(p)\) 
  which is isomorphic to a open set in the real line.)
\qed

## Inverse and Implicit Function Theorem

The inverse and implicit function theorem are two important theorems and we shall 
look at them in this section. 

From last year, we looked at the inverse function theorem in the first dimension.
Let \(f : (a, b) \to \mathbb{R}\) be continuously differentiable, and suppose 
there exists \(p \in (a, b)\) such that \(f'(p) \neq 0\). 
Then there exists a neighbourhood \(I\) around \(p\) such that 
\(f \mid_I : I \to f(I)\) is bijective and thus has a inverse on \(I\), \(f^{-1}\) 
that is differentiable and 
\[(f^{-1})'(y)) = \frac{1}{f'(f^{-1}(y))}.\]

This theorem can be generalised into higher dimensions. 

\begin{theorem}[\(C^1\) Inverse Function Theorem]
  Let \(\Omega \subseteq \mathbb{R}^n\) be open and \(f : \Omega \to \mathbb{R}^m\) 
  be continuously differentiable on \(\Omega\), and there exists some \(q \in \Omega\) 
  such that \(Df(q) \in \mathcal{L}(\mathbb{R}^n; \mathbb{R}^m)\) is invertible.
  Then, there exist open neighbourhoods around \(q\) and \(f(q)\), namely \(U, V\)
  respectively, such that,
  \begin{itemize}
    \item \(f : U \to V\) is a bijection;
    \item \(f^{-1} : V \to U\) is continuously differentiable;
    \item for all \(v \in V\), \(Df^{-1}(v) = (Df(f^{-1}(v)))^{-1}\).
  \end{itemize}
\end{theorem}

\begin{lemma}[Contraction Mapping Theorem]
  Let \(X\) be a complete metric space and let \(\phi : X \to X\) be a contraction 
  of \(X\). Then there exists an unique \(x\) such that \(\phi(x) = x\).
\end{lemma}

\begin{remark}
  We shall examine exactly what this theorem states in the later sections on 
  metric spaces and topology.
\end{remark}

\proof (Part 1 of the \(C^1\) Inverse Function Theorem).
  We denote \(\|A\| = \sup_{\|x\| \le 1} \|Ax\|\) within this proof.
  Suppose \(q \in \Omega\) and \(Df(q) = A\) is invertible, then let us define 
  \(\epsilon := 1 / \|Df(q)\|\). As \(f\) is continuously differentiable, there 
  exists some open neighbourhood of \(q\) -- \(U\) such that for all \(x \in U\)
  \[\|Df(x) - A\| < \epsilon,\]
  (simply choose \(U\) to have diameter smaller than 1). Now, for all 
  \(y \in \mathbb{R}^m\), we define 
  \[\phi_y(x) = x + A^{-1}(y - f(x)).\]
  It is easy to see that \(\phi_y\) is differentiable with derivative 
  \(D\phi_y(x) = I - A^{-1}Df(x) = A^{-1}(A - Df(x)),\) so 
  \(\|D\phi_y(x)\| = \|A^{-1}\| \|(A - Df(x))\| < \|A^{-1}\|\epsilon = 1\). By 
  the mean value theorem, \(\| \phi_y(x_1) - \phi_y(x_2) \| \le \| x_1 - x_2 \|\),
  that is \(phi_y\) is a contraction on \(B\), and hence has a unique fixed point.
  Now as \(f(x) = y\) if and only if \(x\) is a fixed point of \(\phi_y\), we are 
  done. 
\qed

It is in general not easy to find the inverse of a function in the higher dimensions, 
so the inverse function theorem can help us obtain some properties about the 
inverse that is otherwise difficult or unobtainable.

The inverse function theorem can be used to show existence and uniqueness of 
solutions of non-linear system of equations. Given \(f_i(x_1, \cdots, x_n) = y_i\) 
for \(i = 1,\cdots , n\), we can define \(F(\mathbf{x}) = (f_i(\mathbf{x}))^T\). 
Then, by looking at some open neighbourhood containing \(\mathbf{y}\), it might 
be possible to determine \(F^{-1}(\mathbf{y})\).

Let \(\Omega, \Omega' \subseteq \mathbb{R}^n\) be open. Then, we say 
\(f : \Omega \to \Omega'\) is a \(C^1\)-diffeomorphism is 
\begin{itemize}
  \item \(f : \Omega \to \Omega'\) is a bijection;
  \item \(f : \Omega \to \Omega'\) is continuously differentiable;
  \item for all \(x \in \Omega\), \(Df(x)\) is invertible.
\end{itemize}

\begin{remark}
  In fact, the set of all \(C^1\)-diffeomorphisms from some open 
  \(\Omega \subseteq \mathbb{R}^n\) to itself forms a group under composition.
\end{remark}

\begin{theorem}[Implicit Function Theorem -- Simple ver.]
  Let \(\Omega \subseteq \mathbb{R}^2\) be open and \(f : \Omega \to \mathbb{R}\) 
  is continuously differentiable, moreover, suppose there exists 
  \(q = (a, b) \in \Omega\) such that \(f(a, b) = 0\) and \(D_2 f(a, b) \neq 0\). 
  Then, there exists open \(A, B \subseteq \mathbb{R}\) and \(g : A \to B\)
   such that \(a \in A\), \(b \in B\) and \((x, y) \in A \times B\) satisfies 
   \(f(x, y) = 0\) if and only if \(y = g(x)\). Furthermore, \(g\) is 
   continuously differentiable.
\end{theorem}

\proof
Wlog. We assume \(D_2f(p) > 0\), then as \(f\) is continuously differentiable, 
\(D_2f(p)\) is continuous and thus, there exists some open neighbourhoods around 
\(a\) and \(b\) -- \(A, B = (a - \delta_a, a + \delta_a), 
(b - \delta_b, b + \delta_b)\) respectively, such that for all 
\(u \in A \times B\), \(D_2f(u) > 0\) (this can be obtained by drawing the a 
square insider the open ball). Now, suppose we define 
\[h : B \to \mathbb{R} : y \mapsto f(a, y).\]
As \(h'(y) = D_2f(a, y) > 0\), we see that \(h\) is strictly increasing. 
Furthermore, as \(h(b) = f(a, b) = 0\), we have \(h(b - \delta_b / 2) < h(b) = 0\), 
and similarly, \(h(b + \delta_b / 2) > 0\). Thus, there exists some \(\delta_a > \delta' > 0\)
such that \(f(x, b - \delta_b / 2) < 0\) and \(f(x, b + \delta_b / 2) > 0\) for 
all \(x \in (a - \delta', a + \delta')\). Now, by the intermediate value theorem, 
for all \(x \in (a - \delta', a + \delta')\) there exists (uniquely) some 
\(y_x \in (b - \delta_b / 2, b + \delta_b / 2)\) such that \(f(x, y_x) = 0\)
(unique as \(D_2f(x, y) > 0\)). Thus, we can define 
\[g : (a - \delta', a + \delta') \to (b - \delta_b / 2, b + \delta_b / 2) : x \mapsto y_x.\]
We see straight away that \(g\) is continuously differentiable as \(f\) is. So 
we are done.
\qed

There is a more general version of this theorem applying to arbitrary dimensions.

\begin{theorem}[Implicit Function Theorem]
  Let \(\Omega \subseteq \mathbb{R}^n, \Omega' \subseteq \mathbb{R}^m\) be open, 
  and \(f : \Omega \times \Omega' \to \mathbb{R}^m\) be continuously differentiable 
  on \(\Omega \times \Omega'\). Suppose there exists 
  \(p = (a, b) \Omega \times \Omega'\) such that \(f(p) = 0\) and 
  \(D_{n + j} f^i(p)\) is invertible for all \(1 \le i, j \le m\). Then there 
  are \(A \subseteq \Omega, B \subseteq \Omega'\) with \(a \in A, b \in B\) 
  such that there exists a map \(g : A \to B\) in which, 
  \(f(x, y) = 0\) if and only if \(y = g(x)\) for some \(x \in A\).
  Furthermore, \(g\) is continuously differentiable.
\end{theorem}